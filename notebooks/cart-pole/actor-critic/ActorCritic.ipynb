{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "from tfinterface.reinforcement import DeepActorCritic, ExpandedStateEnv\n",
    "from tfinterface.interfaces import EnvironmentInterface\n",
    "from tfinterface.model_base import ModelBase\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "import numbers\n",
    "\n",
    "\n",
    "def get_run():\n",
    "    try:\n",
    "        with open(\"run.txt\") as f:\n",
    "            run = int(f.read().split(\"/n\")[0])\n",
    "    except:\n",
    "        run = -1\n",
    "    \n",
    "    with open(\"run.txt\", 'w+') as f:\n",
    "        run += 1\n",
    "        \n",
    "        f.seek(0)\n",
    "        f.write(str(run))\n",
    "        f.truncate()\n",
    "        \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 08:38:02,318] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Discrete(2)\n",
      "Run: 202\n"
     ]
    }
   ],
   "source": [
    "run = get_run()\n",
    "env = ExpandedStateEnv(\"CartPole-v1\", 3)\n",
    "print(env.action_space)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path = os.getcwd() + \"/actor-critic.model\"\n",
    "logs_path = \"logs/run{}\".format(run)\n",
    "\n",
    "print(\"Run: {}\".format(run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = DeepActorCritic(\n",
    "    n_actions, n_states, y=0.99999, \n",
    "    buffer_length=1000000, pi=0.02,\n",
    "    model_path = model_path,\n",
    "    logs_path = logs_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 0, Length: 24, Reward: 24.0, buffer_len: 24\n",
      "[MAX] Episode: 2, Length: 33, Reward: 33.0, buffer_len: 80\n",
      "[MAX] Episode: 3, Length: 55, Reward: 55.0, buffer_len: 135\n",
      "[MAX] Episode: 6, Length: 55, Reward: 55.0, buffer_len: 265\n",
      "[NOR] Episode: 10, Length: 31, Avg Reward: 36.3, Learning Rate: 0.01, buffer_len: 363\n",
      "Loss: 0.467469424009\n",
      "[MAX] Episode: 16, Length: 58, Reward: 58.0, buffer_len: 578\n",
      "[MAX] Episode: 20, Length: 59, Reward: 59.0, buffer_len: 759\n",
      "[NOR] Episode: 20, Length: 59, Avg Reward: 39.6, Learning Rate: 0.01, buffer_len: 759\n",
      "Loss: 0.493125587702\n",
      "[MAX] Episode: 26, Length: 98, Reward: 98.0, buffer_len: 1029\n",
      "[NOR] Episode: 30, Length: 45, Avg Reward: 38.9, Learning Rate: 0.01, buffer_len: 1148\n",
      "Loss: 0.613241314888\n",
      "[NOR] Episode: 40, Length: 26, Avg Reward: 30.1, Learning Rate: 0.01, buffer_len: 1449\n",
      "Loss: 0.518278598785\n",
      "[NOR] Episode: 50, Length: 16, Avg Reward: 35.3, Learning Rate: 0.01, buffer_len: 1802\n",
      "Loss: 0.616812705994\n",
      "[NOR] Episode: 60, Length: 37, Avg Reward: 30.8, Learning Rate: 0.01, buffer_len: 2110\n",
      "Loss: 0.631880402565\n",
      "[NOR] Episode: 70, Length: 21, Avg Reward: 32.8, Learning Rate: 0.01, buffer_len: 2438\n",
      "Loss: 0.491368293762\n",
      "[NOR] Episode: 80, Length: 14, Avg Reward: 31.6, Learning Rate: 0.01, buffer_len: 2754\n",
      "Loss: 0.558840036392\n",
      "[NOR] Episode: 90, Length: 40, Avg Reward: 31.0, Learning Rate: 0.01, buffer_len: 3064\n",
      "Loss: 0.475624889135\n",
      "[NOR] Episode: 100, Length: 23, Avg Reward: 24.2, Learning Rate: 0.01, buffer_len: 3306\n",
      "Loss: 0.64003443718\n",
      "[NOR] Episode: 110, Length: 14, Avg Reward: 21.3, Learning Rate: 0.01, buffer_len: 3519\n",
      "Loss: 0.513172268867\n",
      "[NOR] Episode: 120, Length: 19, Avg Reward: 24.8, Learning Rate: 0.01, buffer_len: 3767\n",
      "Loss: 0.639866828918\n",
      "[NOR] Episode: 130, Length: 48, Avg Reward: 44.3, Learning Rate: 0.01, buffer_len: 4210\n",
      "Loss: 0.583773493767\n",
      "[NOR] Episode: 140, Length: 31, Avg Reward: 27.5, Learning Rate: 0.01, buffer_len: 4485\n",
      "Loss: 0.624247670174\n",
      "[NOR] Episode: 150, Length: 28, Avg Reward: 22.7, Learning Rate: 0.01, buffer_len: 4712\n",
      "Loss: 0.61384832859\n",
      "[NOR] Episode: 160, Length: 36, Avg Reward: 23.8, Learning Rate: 0.01, buffer_len: 4950\n",
      "Loss: 0.52715575695\n",
      "[NOR] Episode: 170, Length: 54, Avg Reward: 37.3, Learning Rate: 0.01, buffer_len: 5323\n",
      "Loss: 0.498015910387\n",
      "[NOR] Episode: 180, Length: 30, Avg Reward: 27.0, Learning Rate: 0.01, buffer_len: 5593\n",
      "Loss: 0.669825553894\n",
      "[NOR] Episode: 190, Length: 24, Avg Reward: 29.1, Learning Rate: 0.01, buffer_len: 5884\n",
      "Loss: 0.652291178703\n",
      "[NOR] Episode: 200, Length: 12, Avg Reward: 25.4, Learning Rate: 0.01, buffer_len: 6138\n",
      "Loss: 0.642940580845\n",
      "[NOR] Episode: 210, Length: 53, Avg Reward: 27.5, Learning Rate: 0.01, buffer_len: 6413\n",
      "Loss: 0.480633467436\n",
      "[NOR] Episode: 220, Length: 34, Avg Reward: 23.5, Learning Rate: 0.01, buffer_len: 6648\n",
      "Loss: 0.673851668835\n",
      "[NOR] Episode: 230, Length: 34, Avg Reward: 35.9, Learning Rate: 0.01, buffer_len: 7007\n",
      "Loss: 0.690881490707\n",
      "[NOR] Episode: 240, Length: 92, Avg Reward: 38.8, Learning Rate: 0.01, buffer_len: 7395\n",
      "Loss: 0.602312326431\n",
      "[NOR] Episode: 250, Length: 25, Avg Reward: 35.7, Learning Rate: 0.01, buffer_len: 7752\n",
      "Loss: 0.609916567802\n",
      "[NOR] Episode: 260, Length: 16, Avg Reward: 29.0, Learning Rate: 0.01, buffer_len: 8042\n",
      "Loss: 0.567219614983\n",
      "[NOR] Episode: 270, Length: 50, Avg Reward: 39.4, Learning Rate: 0.01, buffer_len: 8436\n",
      "Loss: 0.694430589676\n",
      "[NOR] Episode: 280, Length: 46, Avg Reward: 40.0, Learning Rate: 0.01, buffer_len: 8836\n",
      "Loss: 0.215962231159\n",
      "[MAX] Episode: 284, Length: 149, Reward: 149.0, buffer_len: 9049\n",
      "[NOR] Episode: 290, Length: 38, Avg Reward: 45.9, Learning Rate: 0.01, buffer_len: 9295\n",
      "Loss: 0.47364115715\n",
      "[NOR] Episode: 300, Length: 68, Avg Reward: 42.6, Learning Rate: 0.01, buffer_len: 9721\n",
      "Loss: 0.6573741436\n",
      "[NOR] Episode: 310, Length: 34, Avg Reward: 30.9, Learning Rate: 0.01, buffer_len: 10030\n",
      "Loss: 0.5191385746\n",
      "[NOR] Episode: 320, Length: 32, Avg Reward: 54.4, Learning Rate: 0.01, buffer_len: 10574\n",
      "Loss: 0.520025372505\n",
      "[NOR] Episode: 330, Length: 33, Avg Reward: 41.7, Learning Rate: 0.01, buffer_len: 10991\n",
      "Loss: 0.767980992794\n",
      "[NOR] Episode: 340, Length: 39, Avg Reward: 54.8, Learning Rate: 0.01, buffer_len: 11539\n",
      "Loss: 0.593125343323\n",
      "[MAX] Episode: 347, Length: 149, Reward: 149.0, buffer_len: 11992\n",
      "[MAX] Episode: 348, Length: 158, Reward: 158.0, buffer_len: 12150\n",
      "[NOR] Episode: 350, Length: 18, Avg Reward: 73.4, Learning Rate: 0.01, buffer_len: 12273\n",
      "Loss: 0.321682721376\n",
      "[NOR] Episode: 360, Length: 107, Avg Reward: 51.6, Learning Rate: 0.01, buffer_len: 12789\n",
      "Loss: 0.318740606308\n",
      "[MAX] Episode: 363, Length: 167, Reward: 167.0, buffer_len: 13037\n",
      "[NOR] Episode: 370, Length: 87, Avg Reward: 71.7, Learning Rate: 0.01, buffer_len: 13506\n",
      "Loss: 0.433030188084\n",
      "[MAX] Episode: 378, Length: 362, Reward: 362.0, buffer_len: 14270\n",
      "[NOR] Episode: 380, Length: 142, Avg Reward: 105.8, Learning Rate: 0.01, buffer_len: 14564\n",
      "Loss: 0.120967656374\n",
      "[NOR] Episode: 390, Length: 142, Avg Reward: 123.0, Learning Rate: 0.01, buffer_len: 15794\n",
      "Loss: 0.702686190605\n",
      "[MAX] Episode: 399, Length: 536, Reward: 536.0, buffer_len: 17684\n",
      "[NOR] Episode: 400, Length: 345, Avg Reward: 223.5, Learning Rate: 0.01, buffer_len: 18029\n",
      "Loss: -0.381382107735\n",
      "[MAX] Episode: 401, Length: 607, Reward: 607.0, buffer_len: 18636\n",
      "[MAX] Episode: 406, Length: 684, Reward: 684.0, buffer_len: 20763\n",
      "[NOR] Episode: 410, Length: 302, Avg Reward: 405.5, Learning Rate: 0.01, buffer_len: 22084\n",
      "Loss: -0.515115559101\n",
      "[NOR] Episode: 420, Length: 190, Avg Reward: 243.1, Learning Rate: 0.01, buffer_len: 24515\n",
      "Loss: -1.79723978043\n",
      "[NOR] Episode: 430, Length: 226, Avg Reward: 259.0, Learning Rate: 0.01, buffer_len: 27105\n",
      "Loss: -2.16159963608\n",
      "[NOR] Episode: 440, Length: 269, Avg Reward: 247.1, Learning Rate: 0.01, buffer_len: 29576\n",
      "Loss: -3.03325772285\n",
      "[NOR] Episode: 450, Length: 239, Avg Reward: 232.3, Learning Rate: 0.01, buffer_len: 31899\n",
      "Loss: -2.008664608\n",
      "[NOR] Episode: 460, Length: 140, Avg Reward: 260.8, Learning Rate: 0.01, buffer_len: 34507\n",
      "Loss: -2.14649581909\n",
      "[NOR] Episode: 470, Length: 351, Avg Reward: 279.2, Learning Rate: 0.01, buffer_len: 37299\n",
      "Loss: 1.52481806278\n",
      "[NOR] Episode: 480, Length: 453, Avg Reward: 351.6, Learning Rate: 0.01, buffer_len: 40815\n",
      "Loss: -3.06407260895\n",
      "[MAX] Episode: 486, Length: 707, Reward: 707.0, buffer_len: 43137\n",
      "[NOR] Episode: 490, Length: 316, Avg Reward: 380.6, Learning Rate: 0.01, buffer_len: 44621\n",
      "Loss: -1.65523827076\n",
      "[NOR] Episode: 500, Length: 303, Avg Reward: 380.1, Learning Rate: 0.01, buffer_len: 48422\n",
      "Loss: -2.06640315056\n",
      "[MAX] Episode: 509, Length: 772, Reward: 772.0, buffer_len: 53133\n",
      "[NOR] Episode: 510, Length: 377, Avg Reward: 508.8, Learning Rate: 0.01, buffer_len: 53510\n",
      "Loss: -2.52667427063\n",
      "[MAX] Episode: 516, Length: 930, Reward: 930.0, buffer_len: 56895\n",
      "[NOR] Episode: 520, Length: 440, Avg Reward: 553.3, Learning Rate: 0.01, buffer_len: 59043\n",
      "Loss: -1.7752764225\n",
      "[MAX] Episode: 528, Length: 1619, Reward: 1619.0, buffer_len: 64273\n",
      "[MAX] Episode: 529, Length: 1784, Reward: 1784.0, buffer_len: 66057\n",
      "[NOR] Episode: 530, Length: 644, Avg Reward: 765.8, Learning Rate: 0.01, buffer_len: 66701\n",
      "Loss: -4.25634002686\n",
      "[MAX] Episode: 533, Length: 2064, Reward: 2064.0, buffer_len: 69836\n",
      "[NOR] Episode: 540, Length: 634, Avg Reward: 803.1, Learning Rate: 0.01, buffer_len: 74732\n",
      "Loss: -4.90438556671\n",
      "[NOR] Episode: 550, Length: 769, Avg Reward: 910.1, Learning Rate: 0.01, buffer_len: 83833\n",
      "Loss: -6.1105465889\n",
      "[NOR] Episode: 560, Length: 1162, Avg Reward: 1229.0, Learning Rate: 0.01, buffer_len: 96123\n",
      "Loss: -2.94537115097\n",
      "[MAX] Episode: 565, Length: 3669, Reward: 3669.0, buffer_len: 104622\n",
      "[MAX] Episode: 570, Length: 7318, Reward: 7318.0, buffer_len: 119376\n",
      "[NOR] Episode: 570, Length: 7318, Avg Reward: 2325.3, Learning Rate: 0.01, buffer_len: 119376\n",
      "Loss: -7.73536062241\n",
      "[MAX] Episode: 571, Length: 8144, Reward: 8144.0, buffer_len: 127520\n",
      "[MAX] Episode: 576, Length: 200001, Reward: 200001.0, buffer_len: 336621\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-bb1ff83461af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episode_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200e3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;31m# lambda t: 0.05 * k / (k + t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m/home/cristian/data/cristian/tfinterface/tfinterface/reinforcement/deep_actor_critic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, keep_prob, learning_rate, print_step, update_target, episodes, max_episode_length, batch_size)\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mupdate_target\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 40000.\n",
    "model.fit(\n",
    "    env, keep_prob=0.5,print_step=10, \n",
    "    episodes=int(1e5), max_episode_length=200e3, batch_size=32,\n",
    "    learning_rate = 0.01 # lambda t: 0.05 * k / (k + t)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 08:53:24,278] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <type 'exceptions.TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-675e5e61d077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/cartpole.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/window/xlib/__init__.pyc\u001b[0m in \u001b[0;36mdispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# Check for the events specific to this window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         while xlib.XCheckWindowEvent(_x_display, _window,\n\u001b[0;32m--> 853\u001b[0;31m                                      0x1ffffff, byref(e)):\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;31m# Key events are filtered by the xlib window event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# handler so they get a shot at the prefiltered event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: <type 'exceptions.TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model_run = DeepActorCritic(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path + \".max\",\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "env = ExpandedStateEnv(\"CartPole-v1\", 3)\n",
    "s = env.reset()\n",
    "done = False\n",
    "total = 0\n",
    "while not done:\n",
    "    total += 1\n",
    "    a = model_run.choose_action(s, 1.0, e=0.2)\n",
    "    s, r, done, info = env.step(a)\n",
    "    env.render()\n",
    "    time.sleep(0.01)\n",
    "    \n",
    "print total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
