{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tfinterface.model_base import ModelBase\n",
    "from tfinterface.reinforcement import ExperienceReplay\n",
    "from tfinterface.utils import select_columns, soft_if, get_run\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from tfinterface.reinforcement import ExpandedStateEnv\n",
    "import os\n",
    "\n",
    "\n",
    "name = \"lunar-lander-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Inputs(object):\n",
    "    def __init__(self, n_states, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.episode_length = tf.placeholder(tf.int64, [], name='episode_length')\n",
    "\n",
    "            self.s = tf.placeholder(tf.float32, [None, n_states], name='s')\n",
    "            self.a = tf.placeholder(tf.int32, [None], name='a')\n",
    "            self.r = tf.placeholder(tf.float32, [None], name='r')\n",
    "            self.v1 = tf.placeholder(tf.float32, [None], name='V1')\n",
    "            self.done = tf.placeholder(tf.float32, [None], name='done')\n",
    "            \n",
    "            self.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            self.keep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\n",
    "            self.training = tf.placeholder(tf.bool, [], name='training')\n",
    "            \n",
    "            self.pi = tf.placeholder(tf.float32, [], name='pi')\n",
    "            \n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, base_model, inputs, n_actions, n_states, y, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.V = base_model.define_critic_network(inputs, n_actions, n_states)\n",
    "\n",
    "            self.target = soft_if(inputs.done, inputs.r,  inputs.r + y * inputs.v1)\n",
    "\n",
    "            self.error = self.target - self.V\n",
    "            self.loss = Pipe(self.error, tf.nn.l2_loss, tf.reduce_mean)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "            self.update = tf.train.AdamOptimizer(inputs.learning_rate).minimize(self.loss, var_list=self.variables)\n",
    "\n",
    "            avg_error, std_error = tf.nn.moments(self.error, [0])\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('avg_target', tf.reduce_mean(self.target)),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.scalar('avg_error', avg_error),\n",
    "                tf.summary.scalar('std_error', std_error),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n",
    "class Actor(object):\n",
    "    def __init__(self, base_model, inputs, target_critic, n_actions, n_states, y, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.P = base_model.define_actor_network(inputs, n_actions, n_states)\n",
    "\n",
    "            self.Pa = select_columns(self.P, inputs.a)\n",
    "\n",
    "            self.loss = - tf.log(tf.clip_by_value(self.Pa, 1e-3, 1.0)) * target_critic.error\n",
    "            self.loss = tf.reduce_mean(self.loss)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "            self.update = tf.train.AdamOptimizer(inputs.learning_rate).minimize(self.loss, var_list=self.variables)\n",
    "\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LunarLander(ModelBase):\n",
    "    \n",
    "    def define_model(self, n_actions, n_states, y=0.98, buffer_length=50000, pi=0.1):\n",
    "        self.global_max = float('-inf')\n",
    "\n",
    "        self.replay_buffer = ExperienceReplay(max_length=buffer_length)\n",
    "\n",
    "\n",
    "        with self.graph.as_default(), tf.device(\"cpu:0\"):\n",
    "\n",
    "            self.inputs = Inputs(n_states, \"inputs\")\n",
    "\n",
    "            self.critic = Critic(self, self.inputs, n_actions, n_states, y, \"critic\")\n",
    "            self.target_critic = Critic(self, self.inputs, n_actions, n_states, y, \"target_critic\")\n",
    "            self.actor = Actor(self, self.inputs, self.target_critic, n_actions, n_states, y, \"actor\")\n",
    "\n",
    "            self.update = tf.group(self.critic.update, self.actor.update)\n",
    "\n",
    "            self.episode_length_summary = tf.summary.scalar('episode_length', self.inputs.episode_length)\n",
    "\n",
    "            self.summaries = tf.summary.merge([self.actor.summaries, self.critic.summaries, self.target_critic.summaries])\n",
    "\n",
    "            self.update_target = tf.group(*[\n",
    "                t.assign_add(pi * (a - t)) for t, a in zip(self.target_critic.variables, self.critic.variables)\n",
    "            ])\n",
    "            \n",
    "            \n",
    "    def define_actor_network(self, inputs, n_actions, n_states):\n",
    "        ops = dict(\n",
    "            trainable=True,\n",
    "            kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "            bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01)\n",
    "        )\n",
    "        \n",
    "        net = inputs.s\n",
    "        \n",
    "        net = tf.layers.dense(net, 64, activation=tf.nn.relu, name=\"relu_layer\", use_bias=True, **ops)\n",
    "        net = tf.nn.dropout(net, inputs.keep_prob)\n",
    "        \n",
    "        net = tf.layers.dense(net, n_actions, activation=tf.nn.softmax, name='P', use_bias=False, **ops)\n",
    "        \n",
    "        return net\n",
    "\n",
    "\n",
    "    def define_critic_network(self, inputs, n_actions, n_states):\n",
    "        ops = dict(\n",
    "            trainable=True,\n",
    "            kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "            bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01)\n",
    "        )\n",
    "        \n",
    "        net = inputs.s\n",
    "        \n",
    "        net = tf.layers.dense(net, 64, activation=tf.nn.relu, name=\"relu_layer\", **ops)        \n",
    "        net = tf.layers.dense(net, n_actions, name='V', **ops)[:, 0]\n",
    "        \n",
    "        return net\n",
    "    \n",
    "    \n",
    "    def predict_feed(self, S):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.keep_prob: 1.0,\n",
    "            self.inputs.training: False\n",
    "        }\n",
    "    \n",
    "    def predict(self, state, e = 0.0):\n",
    "        predict_feed = self.predict_feed([state])\n",
    "        actions = self.sess.run(self.actor.P, feed_dict=predict_feed)\n",
    "        actions = actions[0]\n",
    "        n = len(actions)\n",
    "\n",
    "        if random.random() < e:\n",
    "            return random.randint(0, n-1)\n",
    "        else:\n",
    "            return np.random.choice(n, p=actions)\n",
    "    \n",
    "    def fit_feed(self, S, A, R, V1, Done, learning_rate, keep_prob):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.a: A,\n",
    "            self.inputs.r: R,\n",
    "            self.inputs.v1: V1,\n",
    "            self.inputs.done: Done,\n",
    "            self.inputs.learning_rate: learning_rate,\n",
    "            self.inputs.keep_prob: keep_prob,\n",
    "            self.inputs.training: True\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def fit(self, env, keep_prob=0.5, e=0.01, learning_rate=0.01, print_step=10, \n",
    "            update_target_step = 32, episodes=100000, max_episode_length=float('inf'), batch_size=32):\n",
    "        \n",
    "        r_total = 0.\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            ep_step = 0\n",
    "            s = env.reset()\n",
    "            episode_length = 0\n",
    "            ep_reward = 0.\n",
    "            \n",
    "            while not done and ep_step <= max_episode_length:\n",
    "                self.global_step += 1\n",
    "                episode_length += 1\n",
    "                ep_step += 1\n",
    "                \n",
    "                \n",
    "                _learning_rate = learning_rate(self.global_step) if hasattr(learning_rate, '__call__') else learning_rate\n",
    "                _e = e(self.global_step) if hasattr(e, '__call__') else e\n",
    "                \n",
    "                \n",
    "                a = self.predict(s, e = _e)\n",
    "                s1, r, done, info = env.step(a)\n",
    "                r_total += r\n",
    "                ep_reward += r\n",
    "                \n",
    "                \n",
    "                self.replay_buffer.append((s, a, r, s1, float(done)))\n",
    "                \n",
    "                \n",
    "                S, A, R, S1, Done = self.replay_buffer.random_batch(batch_size).unzip()\n",
    "                predict_feed = self.predict_feed(S1)\n",
    "                V1 = self.sess.run(self.target_critic.V, feed_dict=predict_feed)\n",
    "\n",
    "                \n",
    "                fit_feed = self.fit_feed(S, A, R, V1, Done, _learning_rate, keep_prob)\n",
    "                _, summaries = self.sess.run([self.update, self.summaries], feed_dict=fit_feed)\n",
    "                self.writer.add_summary(summaries, self.global_step)\n",
    "                \n",
    "                \n",
    "                if self.global_step % update_target_step == 0:\n",
    "                    self.sess.run(self.update_target)\n",
    "                \n",
    "                \n",
    "                s = s1\n",
    "                \n",
    "            \n",
    "            episode_length_summary = self.sess.run(self.episode_length_summary,\n",
    "                                                   feed_dict={self.inputs.episode_length: episode_length})\n",
    "            self.writer.add_summary(episode_length_summary, self.global_step)\n",
    "\n",
    "\n",
    "            if ep_reward >= self.global_max:\n",
    "                print(\"[MAX] Episode: {}, Length: {}, Reward: {}, buffer_len: {}\".format(episode, episode_length, ep_reward, len(self.replay_buffer)))\n",
    "                self.save(model_path = self.model_path + \".{score}.max\".format(score = ep_reward))\n",
    "                self.global_max = ep_reward\n",
    "\n",
    "\n",
    "            if episode % print_step == 0 and episode > 0:\n",
    "                avg_r = r_total / print_step\n",
    "                actor_loss = self.sess.run(self.actor.loss, feed_dict=fit_feed)\n",
    "                print(\"[NOR] Episode: {}, Length: {}, Avg Reward: {}, e: {}, Learning Rate: {}, buffer_len: {}\".format(episode, episode_length, avg_r, _e, _learning_rate, len(self.replay_buffer)))\n",
    "                print(\"Loss: {}\".format(actor_loss))\n",
    "                self.save()\n",
    "                r_total = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 13:41:55,070] Making new env: LunarLander-v2\n",
      "[2017-03-16 13:41:55,328] Creating monitor directory tmp/monitor57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Run: 57\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = wrappers.Monitor(env, \"tmp/monitor/{}\".format(name))\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/models/{name}.model\".format(path = os.getcwd(), name = name)\n",
    "logs_path = \"{path}/logs/{name}\".format(path = os.getcwd(), name = name)\n",
    "\n",
    "print(\"Run: {}\".format(run))\n",
    "\n",
    "model = LunarLander(\n",
    "    n_actions, n_states, y=0.9999, \n",
    "    buffer_length=500000,\n",
    "    model_path = model_path,\n",
    "    logs_path = logs_path,\n",
    "    restore = False,\n",
    "    pi = 0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 10:44:10,080] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55/openaigym.video.0.27716.video000000.mp4\n",
      "[2017-03-16 10:44:12,361] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55/openaigym.video.0.27716.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 0, Length: 87, Reward: -457.01852629, buffer_len: 87\n",
      "[MAX] Episode: 1, Length: 203, Reward: -390.848072174, buffer_len: 290\n",
      "[MAX] Episode: 2, Length: 126, Reward: -368.230072013, buffer_len: 416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 10:44:16,406] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55/openaigym.video.0.27716.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 8, Length: 63, Reward: -144.60731326, buffer_len: 1151\n",
      "[MAX] Episode: 9, Length: 109, Reward: -73.8010988155, buffer_len: 1260\n",
      "[NOR] Episode: 10, Length: 92, Avg Reward: -428.579284157, e: 0.398423833333, Learning Rate: 0.01, buffer_len: 1352\n",
      "Loss: -32.450428009\n",
      "[MAX] Episode: 13, Length: 1000, Reward: 47.6789619196, buffer_len: 2690\n",
      "[NOR] Episode: 20, Length: 102, Avg Reward: -148.688130463, e: 0.394859666667, Learning Rate: 0.01, buffer_len: 4407\n",
      "Loss: -11.5019416809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 10:44:31,873] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55/openaigym.video.0.27716.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 30, Length: 176, Avg Reward: -72.148762292, e: 0.3917575, Learning Rate: 0.01, buffer_len: 7066\n",
      "Loss: 1.22499370575\n",
      "[NOR] Episode: 40, Length: 1000, Avg Reward: -69.7134553905, e: 0.387684666667, Learning Rate: 0.01, buffer_len: 10557\n",
      "Loss: -10.9945850372\n",
      "[NOR] Episode: 50, Length: 309, Avg Reward: -85.6355955524, e: 0.383800833333, Learning Rate: 0.01, buffer_len: 13886\n",
      "Loss: -7.93014812469\n",
      "[MAX] Episode: 53, Length: 1000, Reward: 79.2486911251, buffer_len: 15311\n",
      "[MAX] Episode: 56, Length: 596, Reward: 109.417531459, buffer_len: 17907\n",
      "[NOR] Episode: 60, Length: 123, Avg Reward: -4.36597404064, e: 0.376503333333, Learning Rate: 0.01, buffer_len: 20141\n",
      "Loss: -8.08912658691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 10:45:20,274] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55/openaigym.video.0.27716.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 70, Length: 1000, Avg Reward: -31.3861294674, e: 0.3706035, Learning Rate: 0.01, buffer_len: 25198\n",
      "Loss: 7.95368099213\n",
      "[NOR] Episode: 80, Length: 1000, Avg Reward: -36.77623516, e: 0.365051333333, Learning Rate: 0.01, buffer_len: 29957\n",
      "Loss: -9.58049297333\n",
      "[NOR] Episode: 90, Length: 225, Avg Reward: -45.5070731288, e: 0.360147833333, Learning Rate: 0.01, buffer_len: 34160\n",
      "Loss: -1.79070329666\n",
      "[NOR] Episode: 100, Length: 134, Avg Reward: -71.2668845376, e: 0.356976833333, Learning Rate: 0.01, buffer_len: 36878\n",
      "Loss: -6.25906276703\n",
      "[NOR] Episode: 110, Length: 177, Avg Reward: -37.6401780045, e: 0.352029, Learning Rate: 0.01, buffer_len: 41119\n",
      "Loss: 3.45888948441\n",
      "[NOR] Episode: 120, Length: 116, Avg Reward: -51.6038872647, e: 0.345135166667, Learning Rate: 0.01, buffer_len: 47028\n",
      "Loss: -3.90584683418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 10:47:02,407] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55/openaigym.video.0.27716.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 126, Length: 523, Reward: 134.833892431, buffer_len: 50359\n",
      "[NOR] Episode: 130, Length: 1000, Avg Reward: -1.1841441287, e: 0.3373605, Learning Rate: 0.01, buffer_len: 53692\n",
      "Loss: 21.6594333649\n",
      "[NOR] Episode: 140, Length: 116, Avg Reward: 12.5260148717, e: 0.328520666667, Learning Rate: 0.01, buffer_len: 61269\n",
      "Loss: -4.17341375351\n",
      "[MAX] Episode: 145, Length: 559, Reward: 182.317735485, buffer_len: 65283\n",
      "[NOR] Episode: 150, Length: 215, Avg Reward: 14.2347012856, e: 0.321661833333, Learning Rate: 0.01, buffer_len: 67148\n",
      "Loss: -3.46813130379\n",
      "[NOR] Episode: 160, Length: 1000, Avg Reward: -14.0461541302, e: 0.314299, Learning Rate: 0.01, buffer_len: 73459\n",
      "Loss: 4.3194975853\n",
      "[MAX] Episode: 164, Length: 408, Reward: 205.010610369, buffer_len: 75242\n",
      "[NOR] Episode: 170, Length: 270, Avg Reward: -11.6110120232, e: 0.3093115, Learning Rate: 0.01, buffer_len: 77734\n",
      "Loss: -10.7033576965\n",
      "[MAX] Episode: 176, Length: 685, Reward: 216.928243653, buffer_len: 81878\n",
      "[NOR] Episode: 180, Length: 589, Avg Reward: 19.1221846854, e: 0.30116, Learning Rate: 0.01, buffer_len: 84721\n",
      "Loss: -3.07187700272\n",
      "[NOR] Episode: 190, Length: 1000, Avg Reward: 48.1895919726, e: 0.293579, Learning Rate: 0.01, buffer_len: 91219\n",
      "Loss: -1.09196960926\n",
      "[NOR] Episode: 200, Length: 1000, Avg Reward: 11.5067661249, e: 0.284655166667, Learning Rate: 0.01, buffer_len: 98868\n",
      "Loss: -1.61309909821\n",
      "[NOR] Episode: 210, Length: 1000, Avg Reward: 57.5495517961, e: 0.275593666667, Learning Rate: 0.01, buffer_len: 106635\n",
      "Loss: -2.66124749184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 10:50:26,663] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55/openaigym.video.0.27716.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 220, Length: 128, Avg Reward: 38.5149973797, e: 0.267988166667, Learning Rate: 0.01, buffer_len: 113154\n",
      "Loss: 2.68323755264\n",
      "[NOR] Episode: 230, Length: 1000, Avg Reward: 47.2137635391, e: 0.2598775, Learning Rate: 0.01, buffer_len: 120106\n",
      "Loss: -3.71074056625\n",
      "[NOR] Episode: 240, Length: 307, Avg Reward: 31.2103931428, e: 0.250754166667, Learning Rate: 0.01, buffer_len: 127926\n",
      "Loss: -1.64285564423\n",
      "[NOR] Episode: 250, Length: 568, Avg Reward: 69.1927049243, e: 0.241725333333, Learning Rate: 0.01, buffer_len: 135665\n",
      "Loss: -1.56709599495\n",
      "[NOR] Episode: 260, Length: 632, Avg Reward: 155.307123289, e: 0.233192333333, Learning Rate: 0.01, buffer_len: 142979\n",
      "Loss: -0.63961738348\n",
      "[MAX] Episode: 263, Length: 374, Reward: 221.578576471, buffer_len: 145196\n",
      "[NOR] Episode: 270, Length: 442, Avg Reward: 155.465071631, e: 0.223983833333, Learning Rate: 0.01, buffer_len: 150872\n",
      "Loss: -6.01575517654\n",
      "[NOR] Episode: 280, Length: 584, Avg Reward: 127.460696161, e: 0.214931666667, Learning Rate: 0.01, buffer_len: 158631\n",
      "Loss: -3.81445217133\n",
      "[NOR] Episode: 290, Length: 507, Avg Reward: 128.672294243, e: 0.205503833333, Learning Rate: 0.01, buffer_len: 166712\n",
      "Loss: -0.30143725872\n",
      "[NOR] Episode: 300, Length: 1000, Avg Reward: 61.417582592, e: 0.194464833333, Learning Rate: 0.01, buffer_len: 176174\n",
      "Loss: -2.65053248405\n",
      "[NOR] Episode: 310, Length: 1000, Avg Reward: 108.255513071, e: 0.185072, Learning Rate: 0.01, buffer_len: 184225\n",
      "Loss: -5.55727767944\n",
      "[NOR] Episode: 320, Length: 1000, Avg Reward: 46.0672403428, e: 0.173652666667, Learning Rate: 0.01, buffer_len: 194013\n",
      "Loss: 6.09690284729\n",
      "[NOR] Episode: 330, Length: 650, Avg Reward: 98.3018061514, e: 0.163891166667, Learning Rate: 0.01, buffer_len: 202380\n",
      "Loss: -1.44747710228\n",
      "[NOR] Episode: 340, Length: 507, Avg Reward: 159.371485778, e: 0.156211, Learning Rate: 0.01, buffer_len: 208963\n",
      "Loss: 0.331924855709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 10:56:29,522] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55/openaigym.video.0.27716.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 350, Length: 844, Avg Reward: 142.625246151, e: 0.146838, Learning Rate: 0.01, buffer_len: 216997\n",
      "Loss: 0.0976938009262\n",
      "[NOR] Episode: 360, Length: 632, Avg Reward: 133.897176573, e: 0.1383575, Learning Rate: 0.01, buffer_len: 224266\n",
      "Loss: -10.5042276382\n",
      "[NOR] Episode: 370, Length: 589, Avg Reward: 153.500765458, e: 0.129966833333, Learning Rate: 0.01, buffer_len: 231458\n",
      "Loss: -4.02126264572\n",
      "[NOR] Episode: 380, Length: 659, Avg Reward: 148.4335002, e: 0.1228875, Learning Rate: 0.01, buffer_len: 237526\n",
      "Loss: 3.96236777306\n",
      "[MAX] Episode: 382, Length: 462, Reward: 222.873168397, buffer_len: 238454\n",
      "[NOR] Episode: 390, Length: 654, Avg Reward: 184.117158588, e: 0.116476666667, Learning Rate: 0.01, buffer_len: 243021\n",
      "Loss: -3.90280771255\n",
      "[NOR] Episode: 400, Length: 699, Avg Reward: 176.947032007, e: 0.110235, Learning Rate: 0.01, buffer_len: 248371\n",
      "Loss: 0.491518825293\n",
      "[NOR] Episode: 410, Length: 453, Avg Reward: 126.078457626, e: 0.102749666667, Learning Rate: 0.01, buffer_len: 254787\n",
      "Loss: -6.38367414474\n",
      "[NOR] Episode: 420, Length: 603, Avg Reward: 175.244746342, e: 0.096368, Learning Rate: 0.01, buffer_len: 260257\n",
      "Loss: -7.46209144592\n",
      "[NOR] Episode: 430, Length: 503, Avg Reward: 172.876331077, e: 0.0903713333333, Learning Rate: 0.01, buffer_len: 265397\n",
      "Loss: -1.53324520588\n",
      "[NOR] Episode: 440, Length: 490, Avg Reward: 146.753287069, e: 0.0837365, Learning Rate: 0.01, buffer_len: 271084\n",
      "Loss: 1.68910646439\n",
      "[NOR] Episode: 450, Length: 695, Avg Reward: 168.289875897, e: 0.0761998333333, Learning Rate: 0.01, buffer_len: 277544\n",
      "Loss: -0.0398534536362\n",
      "[NOR] Episode: 460, Length: 595, Avg Reward: 176.793766101, e: 0.069551, Learning Rate: 0.01, buffer_len: 283243\n",
      "Loss: -1.64237356186\n",
      "[NOR] Episode: 470, Length: 801, Avg Reward: 155.044803481, e: 0.0613236666667, Learning Rate: 0.01, buffer_len: 290295\n",
      "Loss: -3.64336872101\n",
      "[NOR] Episode: 480, Length: 508, Avg Reward: 158.379601791, e: 0.054599, Learning Rate: 0.01, buffer_len: 296059\n",
      "Loss: -4.28047800064\n",
      "[NOR] Episode: 490, Length: 730, Avg Reward: 84.0875027182, e: 0.05, Learning Rate: 0.01, buffer_len: 303260\n",
      "Loss: -1.61727190018\n",
      "[NOR] Episode: 500, Length: 622, Avg Reward: 149.942440561, e: 0.05, Learning Rate: 0.01, buffer_len: 310313\n",
      "Loss: -2.16830539703\n",
      "[NOR] Episode: 510, Length: 836, Avg Reward: 132.861973085, e: 0.05, Learning Rate: 0.01, buffer_len: 317825\n",
      "Loss: -0.932911336422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 11:02:21,762] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55/openaigym.video.0.27716.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 520, Length: 704, Avg Reward: 102.145918062, e: 0.05, Learning Rate: 0.01, buffer_len: 326051\n",
      "Loss: -2.73225212097\n",
      "[NOR] Episode: 530, Length: 574, Avg Reward: 139.902687615, e: 0.05, Learning Rate: 0.01, buffer_len: 332902\n",
      "Loss: -3.37321710587\n",
      "[NOR] Episode: 540, Length: 937, Avg Reward: 138.346785993, e: 0.05, Learning Rate: 0.01, buffer_len: 340181\n",
      "Loss: 2.24881196022\n",
      "[NOR] Episode: 550, Length: 566, Avg Reward: 156.955330163, e: 0.05, Learning Rate: 0.01, buffer_len: 346494\n",
      "Loss: -1.19764828682\n",
      "[NOR] Episode: 560, Length: 611, Avg Reward: 133.053876399, e: 0.05, Learning Rate: 0.01, buffer_len: 352436\n",
      "Loss: -1.76882362366\n",
      "[MAX] Episode: 566, Length: 474, Reward: 229.69342739, buffer_len: 355709\n",
      "[NOR] Episode: 570, Length: 683, Avg Reward: 126.008719404, e: 0.05, Learning Rate: 0.01, buffer_len: 358084\n",
      "Loss: -1.83122324944\n",
      "[NOR] Episode: 580, Length: 478, Avg Reward: 135.694028198, e: 0.05, Learning Rate: 0.01, buffer_len: 364483\n",
      "Loss: -9.27827644348\n",
      "[NOR] Episode: 590, Length: 555, Avg Reward: 164.205117933, e: 0.05, Learning Rate: 0.01, buffer_len: 370148\n",
      "Loss: -2.59020853043\n",
      "[MAX] Episode: 598, Length: 419, Reward: 230.142617066, buffer_len: 374632\n",
      "[NOR] Episode: 600, Length: 647, Avg Reward: 171.713449305, e: 0.05, Learning Rate: 0.01, buffer_len: 375884\n",
      "Loss: -1.5971903801\n",
      "[NOR] Episode: 610, Length: 514, Avg Reward: 94.2652070928, e: 0.05, Learning Rate: 0.01, buffer_len: 381852\n",
      "Loss: -2.9048614502\n",
      "[NOR] Episode: 620, Length: 869, Avg Reward: 110.267899461, e: 0.05, Learning Rate: 0.01, buffer_len: 387475\n",
      "Loss: -2.01936459541\n",
      "[NOR] Episode: 630, Length: 524, Avg Reward: 120.358266334, e: 0.05, Learning Rate: 0.01, buffer_len: 393224\n",
      "Loss: 2.15712285042\n",
      "[NOR] Episode: 640, Length: 488, Avg Reward: 145.38349858, e: 0.05, Learning Rate: 0.01, buffer_len: 399871\n",
      "Loss: -5.25781536102\n",
      "[NOR] Episode: 650, Length: 620, Avg Reward: 142.418918469, e: 0.05, Learning Rate: 0.01, buffer_len: 405420\n",
      "Loss: -1.94129371643\n",
      "[NOR] Episode: 660, Length: 568, Avg Reward: 145.065406986, e: 0.05, Learning Rate: 0.01, buffer_len: 410884\n",
      "Loss: -14.2540855408\n",
      "[NOR] Episode: 670, Length: 668, Avg Reward: 87.667809088, e: 0.05, Learning Rate: 0.01, buffer_len: 417189\n",
      "Loss: -8.93282699585\n",
      "[NOR] Episode: 680, Length: 563, Avg Reward: 67.7075826353, e: 0.05, Learning Rate: 0.01, buffer_len: 423219\n",
      "Loss: -4.06246900558\n",
      "[NOR] Episode: 690, Length: 468, Avg Reward: 58.7266464665, e: 0.05, Learning Rate: 0.01, buffer_len: 428420\n",
      "Loss: -5.39530277252\n",
      "[NOR] Episode: 700, Length: 262, Avg Reward: 22.4197238952, e: 0.05, Learning Rate: 0.01, buffer_len: 432707\n",
      "Loss: -10.9470329285\n",
      "[NOR] Episode: 710, Length: 407, Avg Reward: 17.9808282909, e: 0.05, Learning Rate: 0.01, buffer_len: 437120\n",
      "Loss: -5.12806653976\n",
      "[NOR] Episode: 720, Length: 391, Avg Reward: -0.43746738296, e: 0.05, Learning Rate: 0.01, buffer_len: 440706\n",
      "Loss: -3.77003884315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 11:09:25,194] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55/openaigym.video.0.27716.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 730, Length: 404, Avg Reward: 127.252255688, e: 0.05, Learning Rate: 0.01, buffer_len: 444958\n",
      "Loss: -1.62529611588\n",
      "[NOR] Episode: 740, Length: 505, Avg Reward: 52.0928802653, e: 0.05, Learning Rate: 0.01, buffer_len: 449163\n",
      "Loss: -5.46403264999\n",
      "[NOR] Episode: 750, Length: 243, Avg Reward: 99.3296176444, e: 0.05, Learning Rate: 0.01, buffer_len: 453202\n",
      "Loss: -2.77278590202\n",
      "[NOR] Episode: 760, Length: 420, Avg Reward: 40.8728026399, e: 0.05, Learning Rate: 0.01, buffer_len: 457099\n",
      "Loss: -0.582754075527\n",
      "[NOR] Episode: 770, Length: 291, Avg Reward: -8.29672123571, e: 0.05, Learning Rate: 0.01, buffer_len: 461084\n",
      "Loss: -0.343167483807\n",
      "[NOR] Episode: 780, Length: 637, Avg Reward: 71.205509793, e: 0.05, Learning Rate: 0.01, buffer_len: 465194\n",
      "Loss: -4.40495967865\n",
      "[NOR] Episode: 790, Length: 578, Avg Reward: 15.4993917253, e: 0.05, Learning Rate: 0.01, buffer_len: 469169\n",
      "Loss: 13.8341121674\n",
      "[NOR] Episode: 800, Length: 574, Avg Reward: 60.820480122, e: 0.05, Learning Rate: 0.01, buffer_len: 473653\n",
      "Loss: -5.31620883942\n",
      "[NOR] Episode: 810, Length: 318, Avg Reward: 104.321442571, e: 0.05, Learning Rate: 0.01, buffer_len: 477479\n",
      "Loss: -2.13248538971\n",
      "[NOR] Episode: 820, Length: 419, Avg Reward: 85.6061963981, e: 0.05, Learning Rate: 0.01, buffer_len: 481791\n",
      "Loss: -4.31511116028\n",
      "[NOR] Episode: 830, Length: 269, Avg Reward: 142.06988256, e: 0.05, Learning Rate: 0.01, buffer_len: 485685\n",
      "Loss: -10.822640419\n",
      "[NOR] Episode: 840, Length: 489, Avg Reward: 102.5263475, e: 0.05, Learning Rate: 0.01, buffer_len: 490202\n",
      "Loss: 3.64088892937\n",
      "[NOR] Episode: 850, Length: 538, Avg Reward: 181.012684441, e: 0.05, Learning Rate: 0.01, buffer_len: 495278\n",
      "Loss: 1.11106753349\n",
      "[NOR] Episode: 860, Length: 415, Avg Reward: 170.458929938, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.11544060707\n",
      "[NOR] Episode: 870, Length: 394, Avg Reward: 195.493073804, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.75627946854\n",
      "[NOR] Episode: 880, Length: 616, Avg Reward: 184.323064089, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.985687017441\n",
      "[NOR] Episode: 890, Length: 1000, Avg Reward: 160.183844136, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.38060736656\n",
      "[NOR] Episode: 900, Length: 480, Avg Reward: 174.009302439, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.71503067017\n",
      "[NOR] Episode: 910, Length: 403, Avg Reward: 191.211614227, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.53085911274\n",
      "[NOR] Episode: 920, Length: 437, Avg Reward: 183.532144717, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.91517817974\n",
      "[NOR] Episode: 930, Length: 572, Avg Reward: 194.608613652, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.61907589436\n",
      "[NOR] Episode: 940, Length: 514, Avg Reward: 174.494130245, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.62054181099\n",
      "[NOR] Episode: 950, Length: 406, Avg Reward: 196.161182086, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.337939947844\n",
      "[NOR] Episode: 960, Length: 388, Avg Reward: 193.616616328, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.23957300186\n",
      "[MAX] Episode: 966, Length: 381, Reward: 244.360753091, buffer_len: 500000\n",
      "[NOR] Episode: 970, Length: 365, Avg Reward: 171.985529547, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.35118889809\n",
      "[NOR] Episode: 980, Length: 474, Avg Reward: 184.092300898, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.3957695961\n",
      "[NOR] Episode: 990, Length: 389, Avg Reward: 174.04571227, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.066340059042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 11:17:12,423] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55/openaigym.video.0.27716.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 1000, Length: 367, Avg Reward: 194.484978962, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.15144920349\n",
      "[NOR] Episode: 1010, Length: 364, Avg Reward: 208.253204158, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.837719917297\n",
      "[NOR] Episode: 1020, Length: 307, Avg Reward: 206.633619102, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.265700161457\n",
      "[NOR] Episode: 1030, Length: 350, Avg Reward: 169.098889432, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.81208229065\n",
      "[NOR] Episode: 1040, Length: 357, Avg Reward: 192.049465174, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.50517392159\n",
      "[NOR] Episode: 1050, Length: 314, Avg Reward: 173.199658965, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.5434602499\n",
      "[NOR] Episode: 1060, Length: 360, Avg Reward: 204.912166051, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.92996001244\n",
      "[NOR] Episode: 1070, Length: 382, Avg Reward: 199.603278539, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.23724985123\n",
      "[NOR] Episode: 1080, Length: 386, Avg Reward: 187.233010685, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.87317800522\n",
      "[NOR] Episode: 1090, Length: 384, Avg Reward: 165.214342407, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.638864994049\n",
      "[NOR] Episode: 1100, Length: 408, Avg Reward: 198.905803511, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.12963867188\n",
      "[NOR] Episode: 1110, Length: 330, Avg Reward: 55.1030758913, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.32317113876\n",
      "[NOR] Episode: 1120, Length: 336, Avg Reward: 179.476191171, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.57215023041\n",
      "[NOR] Episode: 1130, Length: 395, Avg Reward: 141.1162443, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.21795856953\n",
      "[NOR] Episode: 1140, Length: 259, Avg Reward: 117.852146307, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0603955686092\n",
      "[NOR] Episode: 1150, Length: 541, Avg Reward: 134.055938412, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.45061302185\n",
      "[NOR] Episode: 1160, Length: 378, Avg Reward: 83.5594504745, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.23070371151\n",
      "[NOR] Episode: 1170, Length: 305, Avg Reward: 96.413858843, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.80420351028\n",
      "[NOR] Episode: 1180, Length: 236, Avg Reward: 28.4919959931, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.25470733643\n",
      "[NOR] Episode: 1190, Length: 317, Avg Reward: 108.714240535, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.5500998497\n",
      "[NOR] Episode: 1200, Length: 498, Avg Reward: 181.278337074, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.888802707195\n",
      "[NOR] Episode: 1210, Length: 369, Avg Reward: 130.759227958, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.233998924494\n",
      "[NOR] Episode: 1220, Length: 508, Avg Reward: 189.446046832, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.58011817932\n",
      "[NOR] Episode: 1230, Length: 644, Avg Reward: 157.772401707, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.778145551682\n",
      "[NOR] Episode: 1240, Length: 558, Avg Reward: 154.683801714, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.114747658372\n",
      "[NOR] Episode: 1250, Length: 1000, Avg Reward: 162.606117174, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.432608127594\n",
      "[NOR] Episode: 1260, Length: 1000, Avg Reward: 81.3411333536, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.80933642387\n",
      "[NOR] Episode: 1270, Length: 994, Avg Reward: 38.1162399662, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.0717830658\n",
      "[NOR] Episode: 1280, Length: 558, Avg Reward: 85.529716652, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.33162367344\n",
      "[NOR] Episode: 1290, Length: 1000, Avg Reward: 22.5755262131, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.34605622292\n",
      "[NOR] Episode: 1300, Length: 1000, Avg Reward: -64.1182668642, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.76743793488\n",
      "[NOR] Episode: 1310, Length: 1000, Avg Reward: 0.226343386319, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.66856336594\n",
      "[NOR] Episode: 1320, Length: 861, Avg Reward: 44.0180955591, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.80332899094\n",
      "[NOR] Episode: 1330, Length: 632, Avg Reward: 47.9835671895, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.81210398674\n",
      "[NOR] Episode: 1340, Length: 211, Avg Reward: -6.93414723419, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.70668566227\n",
      "[NOR] Episode: 1350, Length: 1000, Avg Reward: -54.5705110285, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.45717537403\n",
      "[NOR] Episode: 1360, Length: 817, Avg Reward: 30.9284549076, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.70728492737\n",
      "[NOR] Episode: 1370, Length: 712, Avg Reward: 93.9804263819, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.64431190491\n",
      "[NOR] Episode: 1380, Length: 609, Avg Reward: 143.271569995, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.38790512085\n",
      "[NOR] Episode: 1390, Length: 378, Avg Reward: 110.438255156, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.71758460999\n",
      "[NOR] Episode: 1400, Length: 646, Avg Reward: 153.426068127, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.44741463661\n",
      "[NOR] Episode: 1410, Length: 442, Avg Reward: 141.41465609, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.07056331635\n",
      "[NOR] Episode: 1420, Length: 402, Avg Reward: 135.850180301, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.72608935833\n",
      "[NOR] Episode: 1430, Length: 791, Avg Reward: 160.425412443, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.57620239258\n",
      "[NOR] Episode: 1440, Length: 603, Avg Reward: 169.580793205, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.0622856617\n",
      "[NOR] Episode: 1450, Length: 435, Avg Reward: 119.410009187, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.73678207397\n",
      "[NOR] Episode: 1460, Length: 918, Avg Reward: 160.287690781, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.11698150635\n",
      "[NOR] Episode: 1470, Length: 323, Avg Reward: 143.397581671, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.92061328888\n",
      "[NOR] Episode: 1480, Length: 1000, Avg Reward: 139.897579906, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.233990818262\n",
      "[NOR] Episode: 1490, Length: 362, Avg Reward: 111.237931506, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.94478368759\n",
      "[NOR] Episode: 1500, Length: 104, Avg Reward: 56.4032383428, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.2187452316\n",
      "[NOR] Episode: 1510, Length: 633, Avg Reward: 104.687672609, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.935465276241\n",
      "[NOR] Episode: 1520, Length: 1000, Avg Reward: 104.563528015, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.887871980667\n",
      "[NOR] Episode: 1530, Length: 194, Avg Reward: 97.4561703942, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.546802163124\n",
      "[NOR] Episode: 1540, Length: 897, Avg Reward: 84.1970685801, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.7440340519\n",
      "[NOR] Episode: 1550, Length: 446, Avg Reward: 71.4168993859, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.73639178276\n",
      "[NOR] Episode: 1560, Length: 942, Avg Reward: 164.562931418, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.157352924347\n",
      "[NOR] Episode: 1570, Length: 378, Avg Reward: 126.809290512, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.59737968445\n",
      "[NOR] Episode: 1580, Length: 1000, Avg Reward: 138.159934467, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.37368869781\n",
      "[NOR] Episode: 1590, Length: 348, Avg Reward: 40.2184160206, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.468760252\n",
      "[NOR] Episode: 1600, Length: 500, Avg Reward: 55.1770472472, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.52449440956\n",
      "[NOR] Episode: 1610, Length: 713, Avg Reward: 14.4654142615, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.77309250832\n",
      "[NOR] Episode: 1620, Length: 1000, Avg Reward: -44.1480001705, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.15645313263\n",
      "[NOR] Episode: 1630, Length: 194, Avg Reward: 107.852964963, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.37784075737\n",
      "[NOR] Episode: 1640, Length: 245, Avg Reward: 21.6121707539, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.298197776079\n",
      "[NOR] Episode: 1650, Length: 745, Avg Reward: 155.788683532, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.430974364281\n",
      "[NOR] Episode: 1660, Length: 1000, Avg Reward: 0.676777735496, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0310584306717\n",
      "[NOR] Episode: 1670, Length: 788, Avg Reward: -23.442112592, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.13657808304\n",
      "[NOR] Episode: 1680, Length: 1000, Avg Reward: 6.7261519601, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.936201810837\n",
      "[NOR] Episode: 1690, Length: 380, Avg Reward: 51.9126914622, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.239515125751\n",
      "[NOR] Episode: 1700, Length: 240, Avg Reward: 162.755915544, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.639952898026\n",
      "[NOR] Episode: 1710, Length: 381, Avg Reward: 106.786932885, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.3293710947\n",
      "[NOR] Episode: 1720, Length: 327, Avg Reward: 116.312192099, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.73061084747\n",
      "[NOR] Episode: 1730, Length: 403, Avg Reward: 133.537271273, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.548311591148\n",
      "[NOR] Episode: 1740, Length: 399, Avg Reward: 108.060554422, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.45037257671\n",
      "[NOR] Episode: 1750, Length: 305, Avg Reward: 139.225254012, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.04053604603\n",
      "[NOR] Episode: 1760, Length: 284, Avg Reward: 202.651488223, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.726115942\n",
      "[NOR] Episode: 1770, Length: 266, Avg Reward: 176.495438392, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.43897247314\n",
      "[NOR] Episode: 1780, Length: 354, Avg Reward: 128.774995735, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.69152188301\n",
      "[NOR] Episode: 1790, Length: 1000, Avg Reward: 141.049591958, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.566063523293\n",
      "[NOR] Episode: 1800, Length: 466, Avg Reward: 169.180573147, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.473891466856\n",
      "[NOR] Episode: 1810, Length: 239, Avg Reward: 137.360691979, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0347723960876\n",
      "[MAX] Episode: 1816, Length: 220, Reward: 246.250094326, buffer_len: 500000\n",
      "[NOR] Episode: 1820, Length: 197, Avg Reward: 160.891983383, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.37290215492\n",
      "[NOR] Episode: 1830, Length: 236, Avg Reward: 180.901345852, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.869631886482\n",
      "[MAX] Episode: 1831, Length: 209, Reward: 260.557282057, buffer_len: 500000\n",
      "[NOR] Episode: 1840, Length: 344, Avg Reward: 193.104135189, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.35451483727\n",
      "[NOR] Episode: 1850, Length: 274, Avg Reward: 206.398142824, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.41563487053\n",
      "[NOR] Episode: 1860, Length: 299, Avg Reward: 216.063370003, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.48095178604\n",
      "[MAX] Episode: 1867, Length: 236, Reward: 271.434416936, buffer_len: 500000\n",
      "[NOR] Episode: 1870, Length: 562, Avg Reward: 225.774804009, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.54433012009\n",
      "[NOR] Episode: 1880, Length: 335, Avg Reward: 133.846234653, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.61555838585\n",
      "[NOR] Episode: 1890, Length: 126, Avg Reward: 160.747037479, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.83640861511\n",
      "[NOR] Episode: 1900, Length: 249, Avg Reward: 211.533754723, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.518149614334\n",
      "[NOR] Episode: 1910, Length: 167, Avg Reward: 145.513540157, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.05487203598\n",
      "[NOR] Episode: 1920, Length: 557, Avg Reward: 209.848017776, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.170433670282\n",
      "[NOR] Episode: 1930, Length: 313, Avg Reward: 216.947402909, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.90896558762\n",
      "[NOR] Episode: 1940, Length: 224, Avg Reward: 210.227026235, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.1116437912\n",
      "[NOR] Episode: 1950, Length: 214, Avg Reward: 153.153281133, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.19612598419\n",
      "[NOR] Episode: 1960, Length: 196, Avg Reward: 118.68697489, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.28334069252\n",
      "[NOR] Episode: 1970, Length: 143, Avg Reward: 138.730713467, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.77955532074\n",
      "[NOR] Episode: 1980, Length: 409, Avg Reward: 191.716488575, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.527369141579\n",
      "[NOR] Episode: 1990, Length: 291, Avg Reward: 154.771001347, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.89307975769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 11:51:10,890] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55/openaigym.video.0.27716.video002000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 2000, Length: 127, Avg Reward: 43.995813235, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.0286655426\n",
      "[NOR] Episode: 2010, Length: 240, Avg Reward: 84.0501289719, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.6355175972\n",
      "[NOR] Episode: 2020, Length: 134, Avg Reward: 136.204084089, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.792154073715\n",
      "[NOR] Episode: 2030, Length: 242, Avg Reward: 102.398188033, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.03542375565\n",
      "[NOR] Episode: 2040, Length: 225, Avg Reward: 136.963581985, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.33333337307\n",
      "[NOR] Episode: 2050, Length: 260, Avg Reward: 116.022964447, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.22750210762\n",
      "[NOR] Episode: 2060, Length: 204, Avg Reward: 125.769399224, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.71191978455\n",
      "[NOR] Episode: 2070, Length: 275, Avg Reward: 162.933239741, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.31904840469\n",
      "[NOR] Episode: 2080, Length: 158, Avg Reward: 62.7099508695, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.407140612602\n",
      "[NOR] Episode: 2090, Length: 233, Avg Reward: 178.716098451, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.30442166328\n",
      "[NOR] Episode: 2100, Length: 249, Avg Reward: 154.172906999, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.70160245895\n",
      "[NOR] Episode: 2110, Length: 137, Avg Reward: 93.4452290497, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.964011132717\n",
      "[NOR] Episode: 2120, Length: 119, Avg Reward: 77.2137863469, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.8213121891\n",
      "[NOR] Episode: 2130, Length: 164, Avg Reward: 92.0580699889, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.038738489151\n",
      "[NOR] Episode: 2140, Length: 156, Avg Reward: 45.9926039401, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.858513712883\n",
      "[NOR] Episode: 2150, Length: 133, Avg Reward: 62.704998772, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.10337734222\n",
      "[NOR] Episode: 2160, Length: 188, Avg Reward: 103.03676013, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.91806840897\n",
      "[NOR] Episode: 2170, Length: 219, Avg Reward: 176.76841786, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.7934679985\n",
      "[NOR] Episode: 2180, Length: 226, Avg Reward: 191.001261318, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.97462701797\n",
      "[NOR] Episode: 2190, Length: 254, Avg Reward: 133.685060907, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.26984131336\n",
      "[NOR] Episode: 2200, Length: 251, Avg Reward: 170.147817424, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.57980728149\n",
      "[NOR] Episode: 2210, Length: 328, Avg Reward: 164.562114527, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.78612327576\n",
      "[NOR] Episode: 2220, Length: 172, Avg Reward: 125.482606485, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.87635326385\n",
      "[NOR] Episode: 2230, Length: 282, Avg Reward: 164.947338022, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.86741781235\n",
      "[NOR] Episode: 2240, Length: 543, Avg Reward: 189.719078785, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.19270706177\n",
      "[NOR] Episode: 2250, Length: 250, Avg Reward: 186.750965382, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.40979743004\n",
      "[NOR] Episode: 2260, Length: 744, Avg Reward: 137.567764453, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.78974914551\n",
      "[NOR] Episode: 2270, Length: 559, Avg Reward: 103.876528988, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.08403897285\n",
      "[NOR] Episode: 2280, Length: 351, Avg Reward: 169.054545924, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.08924388885\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-07828891774f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterp1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mupdate_target_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-5-2d38915836af>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, keep_prob, e, learning_rate, print_step, update_target_step, episodes, max_episode_length, batch_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_e\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0mr_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mep_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/cristian/tfinterface/tfinterface/reinforcement/expanded_state_env.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/wrappers/monitoring.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/wrappers/time_limit.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyLinearImpulse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpulse_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36mBeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mcontactListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mBeginContact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 40000.\n",
    "model.fit(\n",
    "    env, print_step=10, \n",
    "    episodes=int(1e5), max_episode_length=10000, batch_size=32,\n",
    "    learning_rate = 0.01, # lambda t: 0.05 * k / (k + t)\n",
    "    e = interp1d([0, 300000], [0.4, 0.05], fill_value=0.05, bounds_error=False),\n",
    "    keep_prob = 0.5,\n",
    "    update_target_step = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 12:11:47,778] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "256.995211545\n",
      "219.719334228\n",
      "188.150320307\n",
      "225.556655162\n",
      "242.314481766\n",
      "232.00207613\n",
      "-20.6487954378\n",
      "228.875880237\n",
      "244.614869857\n",
      "207.152128157\n",
      "213.577989837\n",
      "219.722156779\n",
      "228.111420993\n",
      "188.291081195\n",
      "248.019976222\n",
      "115.839584285\n",
      "208.234549767\n",
      "221.813891917\n",
      "225.499633792\n",
      "204.782779931\n",
      "147.330067555\n",
      "217.343215756\n",
      "222.146525834\n",
      "229.015016524\n",
      "179.817766037\n",
      "19.3131672435\n",
      "251.024919233\n",
      "218.619111896\n",
      "244.743774959\n",
      "231.125136202\n",
      "212.506361632\n",
      "31.9656486777\n",
      "223.620479943\n",
      "203.400869115\n",
      "218.980732617\n",
      "212.252630485\n",
      "221.465818493\n",
      "223.956464597\n",
      "254.314942186\n",
      "199.561276632\n",
      "223.12134933\n",
      "238.142993847\n",
      "229.657699388\n",
      "190.477349433\n",
      "206.487812275\n",
      "235.674270079\n",
      "191.74325577\n",
      "239.389023056\n",
      "220.721559604\n",
      "211.528843596\n",
      "200.210569897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 12:15:48,643] Finished writing results. You can upload them to the scoreboard via gym.upload('/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic-2/tmp/monitor55')\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <type 'exceptions.TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-73880f683081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_polygon\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLunarLanderContinuous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLunarLander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/window/xlib/__init__.pyc\u001b[0m in \u001b[0;36mdispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# Check for the events specific to this window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         while xlib.XCheckWindowEvent(_x_display, _window,\n\u001b[0;32m--> 853\u001b[0;31m                                      0x1ffffff, byref(e)):\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;31m# Key events are filtered by the xlib window event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# handler so they get a shot at the prefiltered event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: <type 'exceptions.TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/models/{name}.model.{value}.max\".format(path = os.getcwd(), name = name, value=\"271.434416936\")\n",
    "logs_path = \"{path}/logs/{name}\".format(path = os.getcwd(), name = name)\n",
    "\n",
    "\n",
    "model_run = LunarLander(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path,\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total = 0.\n",
    "    ep = 0\n",
    "    while not done and ep < 700:\n",
    "        ep += 1\n",
    "        a = model_run.predict(s, 0.0)\n",
    "        s, r, done, info = env.step(a)\n",
    "        total += r\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "    print(total)\n",
    "env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-16 14:52:50,439] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-197.609169383\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <type 'exceptions.TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-512006406cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_polygon\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLunarLanderContinuous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLunarLander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/window/xlib/__init__.pyc\u001b[0m in \u001b[0;36mdispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# Check for the events specific to this window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         while xlib.XCheckWindowEvent(_x_display, _window,\n\u001b[0;32m--> 853\u001b[0;31m                                      0x1ffffff, byref(e)):\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;31m# Key events are filtered by the xlib window event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# handler so they get a shot at the prefiltered event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: <type 'exceptions.TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "\n",
    "from pynput.keyboard import Key, Listener\n",
    "import time\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "ACTION = 0\n",
    "\n",
    "UP = False\n",
    "LEFT = False\n",
    "RIGHT = False\n",
    "\n",
    "def set_action():\n",
    "    global ACTION\n",
    "    \n",
    "    if RIGHT:\n",
    "        ACTION = 3\n",
    "    elif LEFT:\n",
    "        ACTION = 1\n",
    "    elif UP:\n",
    "        ACTION = 0\n",
    "    else:\n",
    "        ACTION = 2\n",
    "        \n",
    "\n",
    "def on_press(key):\n",
    "    global UP, LEFT, RIGHT\n",
    "\n",
    "    if key == Key.left:\n",
    "        LEFT = True\n",
    "        RIGHT = False\n",
    "    elif key == Key.right:\n",
    "        RIGHT = True\n",
    "        LEFT = False\n",
    "    elif key == Key.down:\n",
    "        UP = True\n",
    "        \n",
    "    set_action()\n",
    "\n",
    "def on_release(key):\n",
    "    global UP, LEFT, RIGHT\n",
    "    \n",
    "    if key == Key.left:\n",
    "        LEFT = False\n",
    "    elif key == Key.right:\n",
    "        RIGHT = False\n",
    "    elif key == Key.down:\n",
    "        UP = False\n",
    "        \n",
    "    set_action()\n",
    "\n",
    "\n",
    "# Collect events until released\n",
    "with Listener(\n",
    "        on_press=on_press,\n",
    "        on_release=on_release):\n",
    "\n",
    "\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        total = 0.\n",
    "        ep = 0\n",
    "        while not done and ep < 700:\n",
    "            ep += 1\n",
    "            a = ACTION\n",
    "            s, r, done, info = env.step(a)\n",
    "            total += r\n",
    "            env.render()\n",
    "            time.sleep(0.02)\n",
    "        print(total)\n",
    "    \n",
    "    env.render(close=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
