{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tfinterface.model_base import ModelBase\n",
    "from tfinterface.reinforcement import ExperienceReplay\n",
    "from tfinterface.utils import select_columns, soft_if, get_run, map_gradients\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from tfinterface.reinforcement import ExpandedStateEnv\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "name = \"dpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.slice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def twodtensor2list(tensor,m,n):\n",
    "    s = [[tf.slice(tensor,[j,i],[1,1]) for i in range(n)] for j in range(m)]\n",
    "    fs = []\n",
    "    for l in s:\n",
    "        fs.extend(l)\n",
    "    return fs\n",
    "\n",
    "def grads_all_comp(y, shapey, x, shapex):\n",
    "    yl = twodtensor2list(y,shapey[0],shapey[1])\n",
    "    grads = [tf.gradients(yle,x)[0] for yle in yl]\n",
    "    gradsp = tf.stack(grads)\n",
    "    gradst = tf.reshape(gradsp,shape=(shapey[0],shapey[1],shapex[0],shapex[1]))\n",
    "    return gradst\n",
    "\n",
    "\n",
    "reduce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.        ,  0.00258938,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.01449001,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.01792335],\n",
       "        [ 0.01353496,  0.        ,  0.        ,  0.        ]], dtype=float32)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.placeholder(tf.int32, [None], name='a')\n",
    "sa = tf.one_hot(a, 4)\n",
    "net = sa\n",
    "net = tf.layers.dense(net, 8, activation=tf.nn.sigmoid, use_bias=False)\n",
    "net = tf.layers.dense(net, 1, activation=tf.nn.sigmoid, use_bias=False)\n",
    "ls = [ tf.gradients(net[i, 0], sa)[0] for i in range(4) ] |> sum\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# sess.run(l)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run([- ls * sa ], feed_dict={\n",
    "        a: [1, 2, 3, 0]\n",
    "    }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 10, 15, 20],\n",
       "       [12,  9,  6,  3]], dtype=int32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.placeholder(tf.int32, [None, 1], name='a')\n",
    "b = tf.placeholder(tf.int32, [None, 4], name='a')\n",
    "\n",
    "c = a * b\n",
    "\n",
    "sess.run(c, {\n",
    "        a: [[5], [3]], b: [[1, 2, 3, 4], [4, 3, 2, 1]]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 2 |> (**)$(?, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Inputs(object):\n",
    "    def __init__(self, n_states, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.episode_length = tf.placeholder(tf.int64, [], name='episode_length')\n",
    "\n",
    "            self.s = tf.placeholder(tf.float32, [None, n_states], name='s')\n",
    "            self.a = tf.placeholder(tf.int32, [None], name='a')\n",
    "            self.r = tf.placeholder(tf.float32, [None], name='r')\n",
    "            self.v1 = tf.placeholder(tf.float32, [None], name='Q1')\n",
    "            self.done = tf.placeholder(tf.float32, [None], name='done')\n",
    "            \n",
    "            self.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            self.keep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\n",
    "            self.training = tf.placeholder(tf.bool, [], name='training')\n",
    "            \n",
    "            self.pi = tf.placeholder(tf.float32, [], name='pi')\n",
    "            \n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, actor, inputs, n_actions, n_states, y, batch_size, scope, actor_scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            ops = dict(\n",
    "                trainable=True,\n",
    "                kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01)\n",
    "            )\n",
    "\n",
    "            sparse_a = tf.one_hot(inputs.a, n_actions)\n",
    "            net = inputs.s\n",
    "            net = tf.concat([net, sparse_a], 1)\n",
    "            net = tf.layers.dense(net, 64, activation=tf.nn.relu, name=\"relu_layer\", **ops)\n",
    "#             net = tf.layers.dropout(net, inputs.keep_prob)\n",
    "            net = tf.layers.dense(net, 64, activation=tf.nn.relu, name=\"relu_layer2\", **ops)\n",
    "            \n",
    "            \n",
    "            self.Q = tf.layers.dense(net, n_actions, name='Q', **ops)[:, 0]\n",
    "            \n",
    "#             with tf.name_scope(\"dQda\"):\n",
    "            self.dQda = [ tf.gradients(self.Q[i], sparse_a)[0] for i in range(batch_size) ] |> sum\n",
    "\n",
    "            self.target = soft_if(inputs.done, inputs.r,  inputs.r + y * inputs.v1)\n",
    "\n",
    "            self.error = self.target - self.Q\n",
    "            self.loss = self.error |> tf.nn.l2_loss |> tf.reduce_mean\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "            \n",
    "            trainer = tf.train.AdamOptimizer(inputs.learning_rate)\n",
    "            self.gradients = trainer.compute_gradients(self.loss, var_list=self.variables)\n",
    "            self.gradients = map_gradients(tf.clip_by_norm$(?, 10), self.gradients)\n",
    "            \n",
    "            self.update = trainer.apply_gradients(self.gradients)\n",
    "\n",
    "            avg_error, std_error = tf.nn.moments(self.error, [0])\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('avg_target', tf.reduce_mean(self.target)),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.scalar('avg_error', avg_error),\n",
    "                tf.summary.scalar('std_error', std_error),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    sparse_a,\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n",
    "        with tf.variable_scope(actor_scope):\n",
    "            \n",
    "            actor.loss = -actor.P * self.dQda |> tf.reduce_mean\n",
    "            \n",
    "            trainer = tf.train.AdamOptimizer(inputs.learning_rate)\n",
    "            actor.gradients = trainer.compute_gradients(actor.loss, var_list=actor.variables)\n",
    "            actor.gradients = map_gradients(tf.clip_by_norm$(?, 10), actor.gradients)\n",
    "            \n",
    "            actor.update = trainer.apply_gradients(actor.gradients)\n",
    "            \n",
    "            actor.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', actor.loss),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in actor.variables ])),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), actor.variables[i]) for i in range(len(actor.variables))\n",
    "            ])\n",
    "            \n",
    "class Actor(object):\n",
    "    def __init__(self, inputs, n_actions, n_states, y, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            ops = dict(\n",
    "                trainable=True,\n",
    "                kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01)\n",
    "            )\n",
    "\n",
    "            net = inputs.s\n",
    "\n",
    "            net = tf.layers.dense(net, 64, activation=tf.nn.relu, name=\"relu_layer\", use_bias=True, **ops)\n",
    "            net = tf.nn.dropout(net, inputs.keep_prob)\n",
    "\n",
    "            self.P = tf.layers.dense(net, n_actions, activation=tf.nn.softmax, name='P', use_bias=False, **ops)\n",
    "\n",
    "            self.Pa = select_columns(self.P, inputs.a)\n",
    "\n",
    "#             self.loss = - tf.log(tf.clip_by_value(self.Pa, 1e-3, 1.0)) * target_critic.error\n",
    "#             self.loss = tf.reduce_mean(self.loss)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "#             self.update = tf.train.AdamOptimizer(inputs.learning_rate).minimize(self.loss, var_list=self.variables)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LunarLander(ModelBase):\n",
    "    \n",
    "    def define_model(self, n_actions, n_states, batch_size, y=0.98, buffer_length=50000, pi=0.1):\n",
    "        self.global_max = float('-inf')\n",
    "\n",
    "        self.replay_buffer = ExperienceReplay(max_length=buffer_length)\n",
    "\n",
    "\n",
    "        with self.graph.as_default(), tf.device(\"cpu:0\"):\n",
    "\n",
    "            self.inputs = Inputs(n_states, \"inputs\")\n",
    "    \n",
    "            self.actor = Actor(self.inputs, n_actions, n_states, y, \"actor\")\n",
    "            self.critic = Critic(self.actor, self.inputs, n_actions, n_states, y, batch_size, \"critic\", \"actor\")\n",
    "            \n",
    "            self.target_actor = Actor(self.inputs, n_actions, n_states, y, \"target_actor\")\n",
    "            self.target_critic = Critic(self.target_actor, self.inputs, n_actions, n_states, y, batch_size, \"target_critic\", \"target_actor\")\n",
    "            \n",
    "\n",
    "            self.update = tf.group(self.critic.update, self.actor.update)\n",
    "\n",
    "            self.episode_length_summary = tf.summary.scalar('episode_length', self.inputs.episode_length)\n",
    "\n",
    "            self.summaries = tf.summary.merge([self.actor.summaries, self.critic.summaries, self.target_critic.summaries])\n",
    "\n",
    "#             with tf.name_scope(\"update_target\"):\n",
    "            self.update_target = tf.group(*([\n",
    "                t.assign_add(pi * (a - t)) for t, a in zip(self.target_critic.variables, self.critic.variables)\n",
    "            ] + [\n",
    "                t.assign_add(pi * (a - t)) for t, a in zip(self.target_actor.variables, self.actor.variables)\n",
    "            ]))\n",
    "\n",
    "    \n",
    "    \n",
    "    def predict_feed(self, S):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.keep_prob: 1.0,\n",
    "            self.inputs.training: False\n",
    "        }\n",
    "    \n",
    "    def predict(self, state, e = 0.0):\n",
    "        predict_feed = self.predict_feed([state])\n",
    "        actions = self.sess.run(self.actor.P, feed_dict=predict_feed)\n",
    "        actions = actions[0]\n",
    "        n = len(actions)\n",
    "\n",
    "        if random.random() < e:\n",
    "            return random.randint(0, n-1)\n",
    "        else:\n",
    "            return np.random.choice(n, p=actions)\n",
    "    \n",
    "    def fit_feed(self, S, A, R, Q1, Done, learning_rate, keep_prob):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.a: A,\n",
    "            self.inputs.r: R,\n",
    "            self.inputs.v1: Q1,\n",
    "            self.inputs.done: Done,\n",
    "            self.inputs.learning_rate: learning_rate,\n",
    "            self.inputs.keep_prob: keep_prob,\n",
    "            self.inputs.training: True\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def fit(self, env, keep_prob=0.5, e=0.01, learning_rate=0.01, print_step=10, \n",
    "            update_target_step = 32, episodes=100000, max_episode_length=float('inf'), batch_size=32):\n",
    "        \n",
    "        r_total = 0.\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            ep_step = 0\n",
    "            s = env.reset()\n",
    "            episode_length = 0\n",
    "            ep_reward = 0.\n",
    "            \n",
    "            while not done and ep_step <= max_episode_length:\n",
    "                self.global_step += 1\n",
    "                episode_length += 1\n",
    "                ep_step += 1\n",
    "                \n",
    "                \n",
    "                _learning_rate = learning_rate(self.global_step) if hasattr(learning_rate, '__call__') else learning_rate\n",
    "                _e = e(self.global_step) if hasattr(e, '__call__') else e\n",
    "                \n",
    "                \n",
    "                a = self.predict(s, e = _e)\n",
    "                s1, r, done, info = env.step(a)\n",
    "                r_total += r\n",
    "                ep_reward += r\n",
    "                \n",
    "                \n",
    "                self.replay_buffer.append((s, a, r, s1, float(done)))\n",
    "                s = s1\n",
    "                \n",
    "                if len(self.replay_buffer) < batch_size:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                S, A, R, S1, Done = self.replay_buffer.random_batch(batch_size).unzip()\n",
    "                predict_feed = self.predict_feed(S1)\n",
    "#                 Q1 = self.sess.run(self.target_critic.Q, feed_dict=predict_feed)\n",
    "                Q1 = self.sess.run(self.target_critic.Q, feed_dict={\n",
    "                    self.inputs.a: A,\n",
    "                    self.inputs.s: S\n",
    "                })\n",
    "\n",
    "                \n",
    "                fit_feed = self.fit_feed(S, A, R, Q1, Done, _learning_rate, keep_prob)\n",
    "                _, summaries = self.sess.run([self.update, self.summaries], feed_dict=fit_feed)\n",
    "                self.writer.add_summary(summaries, self.global_step)\n",
    "                \n",
    "                \n",
    "                if self.global_step % update_target_step == 0:\n",
    "                    self.sess.run(self.update_target)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            episode_length_summary = self.sess.run(self.episode_length_summary,\n",
    "                                                   feed_dict={self.inputs.episode_length: episode_length})\n",
    "            self.writer.add_summary(episode_length_summary, self.global_step)\n",
    "\n",
    "\n",
    "            if ep_reward >= self.global_max:\n",
    "                print(\"[MAX] Episode: {}, Length: {}, Reward: {}, buffer_len: {}\".format(episode, episode_length, ep_reward, len(self.replay_buffer)))\n",
    "                self.save(model_path = self.model_path + \".{score}\".format(score = ep_reward))\n",
    "                self.global_max = ep_reward\n",
    "\n",
    "\n",
    "            if episode % print_step == 0 and episode > 0:\n",
    "                avg_r = r_total / print_step\n",
    "                actor_loss = self.sess.run(self.actor.loss, feed_dict=fit_feed)\n",
    "                print(\"[NOR] Episode: {}, Length: {}, Avg Reward: {}, e: {}, Learning Rate: {}, buffer_len: {}\".format(episode, episode_length, avg_r, _e, _learning_rate, len(self.replay_buffer)))\n",
    "                print(\"Loss: {}\".format(actor_loss))\n",
    "                self.save()\n",
    "                r_total = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-18 14:52:52,577] Making new env: LunarLander-v2\n",
      "[2017-03-18 14:52:52,580] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-18 14:52:52,581] Creating monitor directory monitor/40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'critic/add_31:0' shape=(?, 4) dtype=float32>,\n",
       " <tf.Tensor 'actor/P/Softmax:0' shape=(?, 4) dtype=float32>)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = get_run()\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = wrappers.Monitor(env, \"monitor/{run}\".format(run = run))\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/models/{name}\".format(path = os.getcwd(), name = name)\n",
    "logs_path = \"{path}/logs/{run}\".format(path = os.getcwd(), run = run)\n",
    "batch_size = 32\n",
    "\n",
    "model = LunarLander(\n",
    "    n_actions, n_states, batch_size, y=0.95, \n",
    "    buffer_length=500000,\n",
    "    model_path = model_path,\n",
    "    logs_path = logs_path,\n",
    "    restore = False,\n",
    "    pi = 0.002\n",
    ")\n",
    "\n",
    "model.critic.dQda, model.actor.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-18 14:52:55,282] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/dpg/monitor/40/openaigym.video.40.23423.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 0, Length: 71, Reward: -260.347299277, buffer_len: 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-18 14:52:56,639] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/dpg/monitor/40/openaigym.video.40.23423.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 2, Length: 64, Reward: -202.87439848, buffer_len: 234\n",
      "[MAX] Episode: 3, Length: 83, Reward: -191.039266365, buffer_len: 317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-18 14:53:00,085] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/dpg/monitor/40/openaigym.video.40.23423.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 9, Length: 76, Reward: -155.875062461, buffer_len: 935\n",
      "[MAX] Episode: 10, Length: 1000, Reward: -6.99694937642, buffer_len: 1935\n",
      "[NOR] Episode: 10, Length: 1000, Avg Reward: -317.27660028, e: 0.397743666667, Learning Rate: 0.01, buffer_len: 1935\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 20, Length: 134, Avg Reward: -272.382091151, e: 0.396582833333, Learning Rate: 0.01, buffer_len: 2930\n",
      "Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-18 14:53:10,930] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/dpg/monitor/40/openaigym.video.40.23423.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 30, Length: 127, Avg Reward: -197.55498628, e: 0.395529333333, Learning Rate: 0.01, buffer_len: 3833\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 40, Length: 90, Avg Reward: -253.438403021, e: 0.394422166667, Learning Rate: 0.01, buffer_len: 4782\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 50, Length: 90, Avg Reward: -218.0291636, e: 0.393411833333, Learning Rate: 0.01, buffer_len: 5648\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 60, Length: 104, Avg Reward: -203.487916624, e: 0.392305833333, Learning Rate: 0.01, buffer_len: 6596\n",
      "Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-18 14:53:22,862] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/dpg/monitor/40/openaigym.video.40.23423.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 70, Length: 100, Avg Reward: -288.091998268, e: 0.391262833333, Learning Rate: 0.01, buffer_len: 7490\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 80, Length: 116, Avg Reward: -208.415860583, e: 0.390054166667, Learning Rate: 0.01, buffer_len: 8526\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 90, Length: 109, Avg Reward: -327.674628503, e: 0.388910833333, Learning Rate: 0.01, buffer_len: 9506\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 100, Length: 60, Avg Reward: -232.226460037, e: 0.3878585, Learning Rate: 0.01, buffer_len: 10408\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 110, Length: 103, Avg Reward: -314.0914296, e: 0.386658, Learning Rate: 0.01, buffer_len: 11437\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 120, Length: 86, Avg Reward: -234.591025184, e: 0.3855975, Learning Rate: 0.01, buffer_len: 12346\n",
      "Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-18 14:53:42,828] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/dpg/monitor/40/openaigym.video.40.23423.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 130, Length: 70, Avg Reward: -233.767467998, e: 0.384546333333, Learning Rate: 0.01, buffer_len: 13247\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 140, Length: 76, Avg Reward: -249.280537292, e: 0.383519666667, Learning Rate: 0.01, buffer_len: 14127\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 150, Length: 87, Avg Reward: -299.690477827, e: 0.382480166667, Learning Rate: 0.01, buffer_len: 15018\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 160, Length: 76, Avg Reward: -331.495711536, e: 0.381465166667, Learning Rate: 0.01, buffer_len: 15888\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 170, Length: 102, Avg Reward: -229.156550204, e: 0.380373166667, Learning Rate: 0.01, buffer_len: 16824\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 180, Length: 99, Avg Reward: -172.548531054, e: 0.379270666667, Learning Rate: 0.01, buffer_len: 17769\n",
      "Loss: 0.0\n",
      "[NOR] Episode: 190, Length: 78, Avg Reward: -248.525776095, e: 0.378153, Learning Rate: 0.01, buffer_len: 18727\n",
      "Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "k = 40000.\n",
    "model.fit(\n",
    "    env, print_step=10, \n",
    "    episodes=int(1e5), max_episode_length=10000, batch_size=batch_size,\n",
    "    learning_rate = 0.01, # lambda t: 0.05 * k / (k + t)\n",
    "    e = interp1d([0, 300000], [0.4, 0.05], fill_value=0.05, bounds_error=False),\n",
    "    keep_prob = 0.5,\n",
    "    update_target_step = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/{name}\".format(path = os.getcwd(), name = name)\n",
    "logs_path = \"{path}/logs/\".format(path = os.getcwd(), name = name)\n",
    "\n",
    "\n",
    "model_run = LunarLander(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path,\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total = 0.\n",
    "    ep = 0\n",
    "    while not done and ep < 700:\n",
    "        ep += 1\n",
    "        a = model_run.predict(s, 0.0)\n",
    "        s, r, done, info = env.step(a)\n",
    "        total += r\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "    \n",
    "    print(total)\n",
    "    \n",
    "env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coconut",
   "language": "coconut",
   "name": "coconut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3.6
   },
   "file_extension": ".coco",
   "mimetype": "text/x-python3",
   "name": "coconut",
   "pygments_lexer": "coconut"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
