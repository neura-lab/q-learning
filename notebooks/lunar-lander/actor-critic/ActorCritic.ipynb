{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'module' object has no attribute '__module__'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "from tfinterface.reinforcement import DeepActorCritic, ExpandedStateEnv\n",
    "from tfinterface.interfaces import EnvironmentInterface\n",
    "from tfinterface.model_base import ModelBase\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "import numbers\n",
    "\n",
    "\n",
    "def get_run():\n",
    "    try:\n",
    "        with open(\"run.txt\") as f:\n",
    "            run = int(f.read().split(\"/n\")[0])\n",
    "    except:\n",
    "        run = -1\n",
    "    \n",
    "    with open(\"run.txt\", 'w+') as f:\n",
    "        run += 1\n",
    "        \n",
    "        f.seek(0)\n",
    "        f.write(str(run))\n",
    "        f.truncate()\n",
    "        \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LanderAC(DeepActorCritic):\n",
    "    \n",
    "    def define_actor_network(self, inputs, n_actions, n_states):\n",
    "        ops = dict(\n",
    "            trainable=True,\n",
    "            kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "            use_bias=False,\n",
    "            bias_initializer=None\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            inputs.s\n",
    "            |> tf.layers.dense$(?, 128, activation=tf.nn.relu, name='relu_layer', **ops)\n",
    "            |> tf.nn.dropout$(?, inputs.keep_prob)\n",
    "            |> tf.layers.dense$(?, n_actions, activation=tf.nn.softmax, name='softmax_layer', **ops)\n",
    "        )\n",
    "\n",
    "\n",
    "    def define_critic_network(self, inputs, n_actions, n_states):\n",
    "        ops = dict(\n",
    "            trainable=True,\n",
    "            kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "            use_bias=False,\n",
    "            bias_initializer=None\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            inputs.s\n",
    "            |> tf.layers.dense$(?, 128, activation=tf.nn.relu, name='relu_layer', **ops)\n",
    "            |> tf.layers.dense$(?, 1, name='linear_layer', **ops)\n",
    "            |> (lambda t: t[:, 0])\n",
    "        )\n",
    "    \n",
    "    def fit(self, env, keep_prob=0.5, e=0., learning_rate=0.01, print_step=10, update_target=1, episodes=100000, max_episode_length=float('inf'), batch_size=32):\n",
    "        r_total = 0.\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            ep_step = 0\n",
    "            s = env.reset()\n",
    "            episode_length = 0\n",
    "            ep_reward = 0.\n",
    "\n",
    "\n",
    "            while not done and ep_step <= max_episode_length:\n",
    "                self.global_step += 1\n",
    "                episode_length += 1\n",
    "                ep_step += 1\n",
    "\n",
    "                _learning_rate = learning_rate(self.global_step) if hasattr(learning_rate, '__call__') else learning_rate\n",
    "                _e = e(self.global_step) if hasattr(e, '__call__') else e\n",
    "\n",
    "                a = self.choose_action(s, keep_prob, e=_e)\n",
    "                s1, r, done, info = env.step(a)\n",
    "                r_total += r\n",
    "                ep_reward += r\n",
    "\n",
    "                self.replay_buffer.append((s, a, r, s1, float(done)))\n",
    "\n",
    "                S, A, R, S1, Done = self.replay_buffer.random_batch(batch_size).unzip()\n",
    "                V1 = self.sess.run(self.target_critic.V, feed_dict={self.inputs.s: S1, self.inputs.keep_prob: 1.0})\n",
    "\n",
    "                feed_dict = self.fit_feed(S, A, R, V1, Done, _learning_rate, True)\n",
    "\n",
    "                \n",
    "                _, summaries = self.sess.run([self.update, self.summaries], feed_dict=feed_dict)\n",
    "                self.writer.add_summary(summaries)\n",
    "\n",
    "                if self.global_step % update_target == 0:\n",
    "                    self.sess.run(self.update_target)\n",
    "\n",
    "                s = s1\n",
    "\n",
    "\n",
    "\n",
    "            episode_length_summary = self.sess.run(self.episode_length_summary,\n",
    "                                                   feed_dict={self.inputs.episode_length: episode_length})\n",
    "            self.writer.add_summary(episode_length_summary)\n",
    "\n",
    "\n",
    "            if ep_reward >= self.global_max:\n",
    "                print(\"[MAX] Episode: {}, Length: {}, Reward: {}, buffer_len: {}\".format(episode, episode_length, ep_reward, len(self.replay_buffer)))\n",
    "#                 self.save(model_path = self.model_path + \".max\")\n",
    "                self.global_max = ep_reward\n",
    "\n",
    "\n",
    "            if episode % print_step == 0 and episode > 0:\n",
    "                avg_r = r_total / print_step\n",
    "                actor_loss = self.sess.run(self.actor.loss, feed_dict=feed_dict)\n",
    "                print(\"[NOR] Episode: {}, Length: {}, e: {}, Avg Reward: {}, Learning Rate: {}, buffer_len: {}\".format(episode, episode_length, _e, avg_r, _learning_rate, len(self.replay_buffer)))\n",
    "                print(\"Loss: {}\".format(actor_loss))\n",
    "                self.save()\n",
    "                r_total = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-12 16:26:09,688] Making new env: LunarLander-v2\n",
      "[2017-03-12 16:26:10,250] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Discrete(4)\n",
      "Run: 46\n"
     ]
    }
   ],
   "source": [
    "run = get_run()\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = wrappers.Monitor(env, \"tmp/monitor{}\".format(run))\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "print(env.action_space)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path = os.getcwd() + \"/actor-critic.model\"\n",
    "logs_path = \"logs/run{}\".format(run)\n",
    "\n",
    "print(\"Run: {}\".format(run))\n",
    "\n",
    "model = LanderAC(\n",
    "    n_actions, n_states, y=0.9999, \n",
    "    buffer_length=500000, pi=0.1,\n",
    "    model_path = model_path,\n",
    "    logs_path = logs_path,\n",
    "    restore = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-12 16:26:11,216] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/tmp/monitor46/openaigym.video.0.11598.video000000.mp4\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LanderAC' object has no attribute 'choose_action'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ee8b5d685455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mupdate_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m )\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-0370d3e4386d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, keep_prob, e, learning_rate, print_step, update_target, episodes, max_episode_length, batch_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mep_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LanderAC' object has no attribute 'choose_action'"
     ]
    }
   ],
   "source": [
    "k = 40000.\n",
    "model.fit(\n",
    "    env, print_step=10, \n",
    "    episodes=int(1e5), max_episode_length=10000, batch_size=32,\n",
    "    learning_rate = 0.01, # lambda t: 0.05 * k / (k + t)\n",
    "    e = interp1d([0, 100000], [0.3, 0.05], fill_value=0.05, bounds_error=False),\n",
    "    update_target = 1,\n",
    "    keep_prob = 0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 21:48:38,328] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Discrete(4)\n",
      "147.728599704\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-921da6990744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "print(env.action_space)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path = os.getcwd() + \"/actor-critic.model\"\n",
    "logs_path = \"logs/run0\"\n",
    "\n",
    "\n",
    "model_run = LanderAC(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path,\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total = 0.\n",
    "    ep = 0\n",
    "    while not done and ep < 700:\n",
    "        ep += 1\n",
    "        a = model_run.choose_action(s, 1.0)\n",
    "        s, r, done, info = env.step(a)\n",
    "        total += r\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "    print(total)\n",
    "env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# gym.upload(\"tmp/monitor{}\".format(run), api_key='sk_WASyK12rQxais3gwyG4Vg', ignore_open_monitors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gym.upload?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coconut (Python 2)",
   "language": "coconut",
   "name": "coconut2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3.6
   },
   "file_extension": ".coco",
   "mimetype": "text/x-python3",
   "name": "coconut",
   "pygments_lexer": "coconut"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
