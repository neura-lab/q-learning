{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'module' object has no attribute '__module__'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "from tfinterface.reinforcement import DeepActorCritic, ExpandedStateEnv\n",
    "from tfinterface.interfaces import EnvironmentInterface\n",
    "from tfinterface.model_base import ModelBase\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "import numbers\n",
    "\n",
    "\n",
    "def get_run():\n",
    "    try:\n",
    "        with open(\"run.txt\") as f:\n",
    "            run = int(f.read().split(\"/n\")[0])\n",
    "    except:\n",
    "        run = -1\n",
    "    \n",
    "    with open(\"run.txt\", 'w+') as f:\n",
    "        run += 1\n",
    "        \n",
    "        f.seek(0)\n",
    "        f.write(str(run))\n",
    "        f.truncate()\n",
    "        \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LanderAC(DeepActorCritic):\n",
    "    \n",
    "    def define_actor_network(self, inputs, n_actions, n_states):\n",
    "        ops = dict(\n",
    "            trainable=True,\n",
    "            kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "            use_bias=False,\n",
    "            bias_initializer=None\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            inputs.s\n",
    "            |> tf.layers.dense$(?, 128, activation=tf.nn.relu, name='relu_layer', **ops)\n",
    "            |> tf.nn.dropout$(?, inputs.keep_prob)\n",
    "            |> tf.layers.dense$(?, n_actions, activation=tf.nn.softmax, name='softmax_layer', **ops)\n",
    "        )\n",
    "\n",
    "\n",
    "    def define_critic_network(self, inputs, n_actions, n_states):\n",
    "        ops = dict(\n",
    "            trainable=True,\n",
    "            kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "            use_bias=False,\n",
    "            bias_initializer=None\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            inputs.s\n",
    "            |> tf.layers.dense$(?, 128, activation=tf.nn.relu, name='relu_layer', **ops)\n",
    "            |> tf.layers.dense$(?, 1, name='linear_layer', **ops)\n",
    "            |> (lambda t: t[:, 0])\n",
    "        )\n",
    "    \n",
    "    def fit(self, env, keep_prob=0.5, e=0., learning_rate=0.01, print_step=10, update_target=1, episodes=100000, max_episode_length=float('inf'), batch_size=32):\n",
    "        r_total = 0.\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            ep_step = 0\n",
    "            s = env.reset()\n",
    "            episode_length = 0\n",
    "            ep_reward = 0.\n",
    "\n",
    "\n",
    "            while not done and ep_step <= max_episode_length:\n",
    "                self.global_step += 1\n",
    "                episode_length += 1\n",
    "                ep_step += 1\n",
    "\n",
    "                _learning_rate = learning_rate(self.global_step) if hasattr(learning_rate, '__call__') else learning_rate\n",
    "                _e = e(self.global_step) if hasattr(e, '__call__') else e\n",
    "\n",
    "                a = self.choose_action(s, keep_prob, e=_e)\n",
    "                s1, r, done, info = env.step(a)\n",
    "                r_total += r\n",
    "                ep_reward += r\n",
    "\n",
    "                self.replay_buffer.append((s, a, r, s1, float(done)))\n",
    "\n",
    "                S, A, R, S1, Done = self.replay_buffer.random_batch(batch_size).unzip()\n",
    "                V1 = self.sess.run(self.target_critic.V, feed_dict={self.inputs.s: S1, self.inputs.keep_prob: 1.0})\n",
    "\n",
    "                feed_dict = self.fit_feed(S, A, R, V1, Done, _learning_rate, True)\n",
    "\n",
    "                \n",
    "                _, summaries = self.sess.run([self.update, self.summaries], feed_dict=feed_dict)\n",
    "                self.writer.add_summary(summaries)\n",
    "\n",
    "                if self.global_step % update_target == 0:\n",
    "                    self.sess.run(self.update_target)\n",
    "\n",
    "                s = s1\n",
    "\n",
    "\n",
    "\n",
    "            episode_length_summary = self.sess.run(self.episode_length_summary,\n",
    "                                                   feed_dict={self.inputs.episode_length: episode_length})\n",
    "            self.writer.add_summary(episode_length_summary)\n",
    "\n",
    "\n",
    "            if ep_reward >= self.global_max:\n",
    "                print(\"[MAX] Episode: {}, Length: {}, Reward: {}, buffer_len: {}\".format(episode, episode_length, ep_reward, len(self.replay_buffer)))\n",
    "#                 self.save(model_path = self.model_path + \".max\")\n",
    "                self.global_max = ep_reward\n",
    "\n",
    "\n",
    "            if episode % print_step == 0 and episode > 0:\n",
    "                avg_r = r_total / print_step\n",
    "                actor_loss = self.sess.run(self.actor.loss, feed_dict=feed_dict)\n",
    "                print(\"[NOR] Episode: {}, Length: {}, e: {}, Avg Reward: {}, Learning Rate: {}, buffer_len: {}\".format(episode, episode_length, _e, avg_r, _learning_rate, len(self.replay_buffer)))\n",
    "                print(\"Loss: {}\".format(actor_loss))\n",
    "                self.save()\n",
    "                r_total = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 23:28:36,190] Making new env: LunarLander-v2\n",
      "[2017-03-08 23:28:36,194] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-08 23:28:36,195] Creating monitor directory tmp/monitor43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Discrete(4)\n",
      "Run: 43\n"
     ]
    }
   ],
   "source": [
    "run = get_run()\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = wrappers.Monitor(env, \"tmp/monitor{}\".format(run))\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "print(env.action_space)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path = os.getcwd() + \"/actor-critic.model\"\n",
    "logs_path = \"logs/run{}\".format(run)\n",
    "\n",
    "print(\"Run: {}\".format(run))\n",
    "\n",
    "model = LanderAC(\n",
    "    n_actions, n_states, y=0.9999, \n",
    "    buffer_length=500000, pi=0.1,\n",
    "    model_path = model_path,\n",
    "    logs_path = logs_path,\n",
    "    restore = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 23:28:37,557] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/tmp/monitor43/openaigym.video.5.4373.video000000.mp4\n",
      "[2017-03-08 23:28:47,419] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/tmp/monitor43/openaigym.video.5.4373.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 0, Length: 734, Reward: 186.581236974, buffer_len: 734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 23:29:08,659] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/tmp/monitor43/openaigym.video.5.4373.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 10, Length: 132, e: 0.288245, Avg Reward: 8.76421429705, Learning Rate: 0.01, buffer_len: 4703\n",
      "Loss: -3.33818101883\n",
      "[NOR] Episode: 20, Length: 1000, e: 0.28064, Avg Reward: -37.3684527483, Learning Rate: 0.01, buffer_len: 7745\n",
      "Loss: -6.54964876175\n",
      "[MAX] Episode: 24, Length: 978, Reward: 213.573827224, buffer_len: 10038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 23:29:32,126] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/tmp/monitor43/openaigym.video.5.4373.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 30, Length: 1000, e: 0.260975, Avg Reward: 64.2716256871, Learning Rate: 0.01, buffer_len: 15611\n",
      "Loss: -4.47060441971\n",
      "[NOR] Episode: 40, Length: 288, e: 0.245155, Avg Reward: 47.8759990875, Learning Rate: 0.01, buffer_len: 21939\n",
      "Loss: -3.02051925659\n",
      "[MAX] Episode: 42, Length: 585, Reward: 237.618499196, buffer_len: 22710\n",
      "[NOR] Episode: 50, Length: 219, e: 0.2314925, Avg Reward: 27.7660034664, Learning Rate: 0.01, buffer_len: 27404\n",
      "Loss: -1.26924729347\n",
      "[NOR] Episode: 60, Length: 1000, e: 0.2139775, Avg Reward: 94.4065900239, Learning Rate: 0.01, buffer_len: 34410\n",
      "Loss: -1.14252781868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 23:31:15,838] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/tmp/monitor43/openaigym.video.5.4373.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 70, Length: 369, e: 0.1974925, Avg Reward: 151.98138393, Learning Rate: 0.01, buffer_len: 41004\n",
      "Loss: 1.15239810944\n",
      "[NOR] Episode: 80, Length: 660, e: 0.1810275, Avg Reward: 131.586723522, Learning Rate: 0.01, buffer_len: 47590\n",
      "Loss: -1.15628731251\n",
      "[MAX] Episode: 81, Length: 526, Reward: 239.807085716, buffer_len: 48116\n",
      "[NOR] Episode: 90, Length: 377, e: 0.16731, Avg Reward: 146.080888223, Learning Rate: 0.01, buffer_len: 53077\n",
      "Loss: -1.77882409096\n",
      "[NOR] Episode: 100, Length: 477, e: 0.149095, Avg Reward: 106.732988924, Learning Rate: 0.01, buffer_len: 60363\n",
      "Loss: -3.39716291428\n",
      "[MAX] Episode: 109, Length: 271, Reward: 260.499954768, buffer_len: 65435\n",
      "[NOR] Episode: 110, Length: 647, e: 0.1347975, Avg Reward: 165.909044415, Learning Rate: 0.01, buffer_len: 66082\n",
      "Loss: 1.04456341267\n",
      "[NOR] Episode: 120, Length: 212, e: 0.1219175, Avg Reward: 150.496565588, Learning Rate: 0.01, buffer_len: 71234\n",
      "Loss: -1.59784340858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 23:33:37,887] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/tmp/monitor43/openaigym.video.5.4373.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 130, Length: 213, e: 0.1128025, Avg Reward: 153.927652825, Learning Rate: 0.01, buffer_len: 74880\n",
      "Loss: -1.42150211334\n",
      "[MAX] Episode: 132, Length: 459, Reward: 261.399608601, buffer_len: 75712\n",
      "[NOR] Episode: 140, Length: 337, e: 0.10506, Avg Reward: 55.349863492, Learning Rate: 0.01, buffer_len: 77977\n",
      "Loss: -0.557442307472\n",
      "[NOR] Episode: 150, Length: 1000, e: 0.09368, Avg Reward: 140.156037309, Learning Rate: 0.01, buffer_len: 82529\n",
      "Loss: -1.90528202057\n",
      "[NOR] Episode: 160, Length: 503, e: 0.0794675, Avg Reward: 160.491117633, Learning Rate: 0.01, buffer_len: 88214\n",
      "Loss: 0.143522560596\n",
      "[NOR] Episode: 170, Length: 501, e: 0.06763, Avg Reward: 192.250683488, Learning Rate: 0.01, buffer_len: 92949\n",
      "Loss: 2.47724103928\n",
      "[NOR] Episode: 180, Length: 416, e: 0.05553, Avg Reward: 192.446568575, Learning Rate: 0.01, buffer_len: 97789\n",
      "Loss: -2.74128437042\n",
      "[NOR] Episode: 190, Length: 332, e: 0.05, Avg Reward: 161.977755564, Learning Rate: 0.01, buffer_len: 102133\n",
      "Loss: -2.06088757515\n",
      "[NOR] Episode: 200, Length: 365, e: 0.05, Avg Reward: 204.353307768, Learning Rate: 0.01, buffer_len: 105832\n",
      "Loss: -0.21593028307\n",
      "[NOR] Episode: 210, Length: 436, e: 0.05, Avg Reward: 194.330224487, Learning Rate: 0.01, buffer_len: 109586\n",
      "Loss: 0.823321342468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 23:35:48,833] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/tmp/monitor43/openaigym.video.5.4373.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 220, Length: 300, e: 0.05, Avg Reward: 181.374250832, Learning Rate: 0.01, buffer_len: 114216\n",
      "Loss: -5.18792486191\n",
      "[NOR] Episode: 230, Length: 142, e: 0.05, Avg Reward: 173.993680611, Learning Rate: 0.01, buffer_len: 117434\n",
      "Loss: -2.24826216698\n",
      "[NOR] Episode: 240, Length: 341, e: 0.05, Avg Reward: 161.029299104, Learning Rate: 0.01, buffer_len: 121242\n",
      "Loss: 1.23340845108\n",
      "[NOR] Episode: 250, Length: 355, e: 0.05, Avg Reward: 185.099105843, Learning Rate: 0.01, buffer_len: 125808\n",
      "Loss: -0.663695812225\n",
      "[NOR] Episode: 260, Length: 278, e: 0.05, Avg Reward: 177.04394541, Learning Rate: 0.01, buffer_len: 129618\n",
      "Loss: 1.45378124714\n",
      "[NOR] Episode: 270, Length: 1000, e: 0.05, Avg Reward: 123.120892663, Learning Rate: 0.01, buffer_len: 134749\n",
      "Loss: 2.85730266571\n",
      "[NOR] Episode: 280, Length: 349, e: 0.05, Avg Reward: 57.9963975971, Learning Rate: 0.01, buffer_len: 137886\n",
      "Loss: -3.53489947319\n",
      "[NOR] Episode: 290, Length: 342, e: 0.05, Avg Reward: 142.663286478, Learning Rate: 0.01, buffer_len: 141756\n",
      "Loss: 1.5398658514\n",
      "[NOR] Episode: 300, Length: 143, e: 0.05, Avg Reward: 101.917573849, Learning Rate: 0.01, buffer_len: 145311\n",
      "Loss: -2.93589782715\n",
      "[NOR] Episode: 310, Length: 116, e: 0.05, Avg Reward: -43.6025238622, Learning Rate: 0.01, buffer_len: 146650\n",
      "Loss: -0.97941493988\n",
      "[NOR] Episode: 320, Length: 172, e: 0.05, Avg Reward: -61.4167934796, Learning Rate: 0.01, buffer_len: 149128\n",
      "Loss: -1.55367076397\n",
      "[NOR] Episode: 330, Length: 80, e: 0.05, Avg Reward: -0.647578610319, Learning Rate: 0.01, buffer_len: 151983\n",
      "Loss: 0.00913637876511\n",
      "[NOR] Episode: 340, Length: 178, e: 0.05, Avg Reward: 84.9417344413, Learning Rate: 0.01, buffer_len: 154820\n",
      "Loss: -0.227080136538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 23:37:59,015] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/tmp/monitor43/openaigym.video.5.4373.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 350, Length: 166, e: 0.05, Avg Reward: 48.8927487185, Learning Rate: 0.01, buffer_len: 157419\n",
      "Loss: -3.04878330231\n",
      "[NOR] Episode: 360, Length: 193, e: 0.05, Avg Reward: 68.3675388163, Learning Rate: 0.01, buffer_len: 162062\n",
      "Loss: 8.08561134338\n",
      "[NOR] Episode: 370, Length: 373, e: 0.05, Avg Reward: 169.870368258, Learning Rate: 0.01, buffer_len: 166297\n",
      "Loss: 2.64764308929\n",
      "[NOR] Episode: 380, Length: 362, e: 0.05, Avg Reward: 179.130460558, Learning Rate: 0.01, buffer_len: 170802\n",
      "Loss: -3.52459049225\n",
      "[NOR] Episode: 390, Length: 445, e: 0.05, Avg Reward: 154.739487229, Learning Rate: 0.01, buffer_len: 174234\n",
      "Loss: 5.67033004761\n",
      "[NOR] Episode: 400, Length: 402, e: 0.05, Avg Reward: 151.101330451, Learning Rate: 0.01, buffer_len: 179202\n",
      "Loss: -2.79199528694\n",
      "[NOR] Episode: 410, Length: 339, e: 0.05, Avg Reward: 156.19050289, Learning Rate: 0.01, buffer_len: 183980\n",
      "Loss: -0.380550384521\n",
      "[NOR] Episode: 420, Length: 312, e: 0.05, Avg Reward: 67.3777637877, Learning Rate: 0.01, buffer_len: 186635\n",
      "Loss: -2.91837215424\n",
      "[NOR] Episode: 430, Length: 386, e: 0.05, Avg Reward: 97.982830069, Learning Rate: 0.01, buffer_len: 192322\n",
      "Loss: -0.127992868423\n",
      "[NOR] Episode: 440, Length: 334, e: 0.05, Avg Reward: 170.129980811, Learning Rate: 0.01, buffer_len: 196518\n",
      "Loss: 3.61351561546\n",
      "[NOR] Episode: 450, Length: 624, e: 0.05, Avg Reward: 79.7242002324, Learning Rate: 0.01, buffer_len: 202440\n",
      "Loss: -2.36219358444\n",
      "[NOR] Episode: 460, Length: 241, e: 0.05, Avg Reward: -29.2516549813, Learning Rate: 0.01, buffer_len: 207143\n",
      "Loss: -0.450484156609\n",
      "[NOR] Episode: 470, Length: 188, e: 0.05, Avg Reward: 30.9356333592, Learning Rate: 0.01, buffer_len: 209602\n",
      "Loss: -2.21275734901\n",
      "[NOR] Episode: 480, Length: 214, e: 0.05, Avg Reward: 70.0385412831, Learning Rate: 0.01, buffer_len: 213352\n",
      "Loss: -4.2979593277\n",
      "[NOR] Episode: 490, Length: 368, e: 0.05, Avg Reward: 138.299806345, Learning Rate: 0.01, buffer_len: 218786\n",
      "Loss: -4.09869766235\n",
      "[NOR] Episode: 500, Length: 685, e: 0.05, Avg Reward: 65.7784337556, Learning Rate: 0.01, buffer_len: 223223\n",
      "Loss: -2.85246300697\n",
      "[NOR] Episode: 510, Length: 414, e: 0.05, Avg Reward: 93.9493533535, Learning Rate: 0.01, buffer_len: 228602\n",
      "Loss: -20.2585430145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 23:41:53,652] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/tmp/monitor43/openaigym.video.5.4373.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 520, Length: 466, e: 0.05, Avg Reward: 124.545270191, Learning Rate: 0.01, buffer_len: 232075\n",
      "Loss: -3.6137008667\n",
      "[NOR] Episode: 530, Length: 409, e: 0.05, Avg Reward: 90.0197450786, Learning Rate: 0.01, buffer_len: 238531\n",
      "Loss: 5.89645957947\n",
      "[NOR] Episode: 540, Length: 420, e: 0.05, Avg Reward: 164.446920094, Learning Rate: 0.01, buffer_len: 243366\n",
      "Loss: 5.87877035141\n",
      "[NOR] Episode: 550, Length: 1000, e: 0.05, Avg Reward: 79.6580206459, Learning Rate: 0.01, buffer_len: 246833\n",
      "Loss: -2.02893853188\n",
      "[NOR] Episode: 560, Length: 158, e: 0.05, Avg Reward: 138.184247546, Learning Rate: 0.01, buffer_len: 252224\n",
      "Loss: 0.116968482733\n",
      "[NOR] Episode: 570, Length: 394, e: 0.05, Avg Reward: 167.501094221, Learning Rate: 0.01, buffer_len: 256108\n",
      "Loss: -1.6836514473\n",
      "[NOR] Episode: 580, Length: 524, e: 0.05, Avg Reward: 186.251079863, Learning Rate: 0.01, buffer_len: 260879\n",
      "Loss: -1.28253304958\n",
      "[NOR] Episode: 590, Length: 334, e: 0.05, Avg Reward: 205.562821323, Learning Rate: 0.01, buffer_len: 264893\n",
      "Loss: -1.30047023296\n",
      "[NOR] Episode: 600, Length: 391, e: 0.05, Avg Reward: 141.272473661, Learning Rate: 0.01, buffer_len: 268035\n",
      "Loss: 0.439007520676\n",
      "[NOR] Episode: 610, Length: 357, e: 0.05, Avg Reward: 173.356754417, Learning Rate: 0.01, buffer_len: 271928\n",
      "Loss: -1.56039893627\n",
      "[NOR] Episode: 620, Length: 210, e: 0.05, Avg Reward: 151.07952277, Learning Rate: 0.01, buffer_len: 274676\n",
      "Loss: -0.543941259384\n",
      "[NOR] Episode: 630, Length: 251, e: 0.05, Avg Reward: 175.869392821, Learning Rate: 0.01, buffer_len: 277915\n",
      "Loss: 7.94116592407\n",
      "[NOR] Episode: 640, Length: 292, e: 0.05, Avg Reward: 199.896774075, Learning Rate: 0.01, buffer_len: 281492\n",
      "Loss: 0.683724522591\n",
      "[NOR] Episode: 650, Length: 458, e: 0.05, Avg Reward: 183.463604269, Learning Rate: 0.01, buffer_len: 286062\n",
      "Loss: -3.21236276627\n",
      "[NOR] Episode: 660, Length: 397, e: 0.05, Avg Reward: 166.658321497, Learning Rate: 0.01, buffer_len: 291205\n",
      "Loss: -11.6550807953\n",
      "[NOR] Episode: 670, Length: 448, e: 0.05, Avg Reward: 201.382076363, Learning Rate: 0.01, buffer_len: 295688\n",
      "Loss: -3.22233724594\n",
      "[NOR] Episode: 680, Length: 491, e: 0.05, Avg Reward: 165.970211223, Learning Rate: 0.01, buffer_len: 300004\n",
      "Loss: -2.64421463013\n",
      "[NOR] Episode: 690, Length: 539, e: 0.05, Avg Reward: 170.992649111, Learning Rate: 0.01, buffer_len: 305582\n",
      "Loss: 2.19706821442\n",
      "[NOR] Episode: 700, Length: 415, e: 0.05, Avg Reward: 176.789010355, Learning Rate: 0.01, buffer_len: 311047\n",
      "Loss: 4.5781955719\n",
      "[NOR] Episode: 710, Length: 362, e: 0.05, Avg Reward: 188.376174467, Learning Rate: 0.01, buffer_len: 315598\n",
      "Loss: -25.4832115173\n",
      "[NOR] Episode: 720, Length: 413, e: 0.05, Avg Reward: 187.06591485, Learning Rate: 0.01, buffer_len: 320125\n",
      "Loss: -2.36125063896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 23:47:10,819] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/tmp/monitor43/openaigym.video.5.4373.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 730, Length: 445, e: 0.05, Avg Reward: 155.49336141, Learning Rate: 0.01, buffer_len: 323951\n",
      "Loss: 1.35682320595\n",
      "[NOR] Episode: 740, Length: 281, e: 0.05, Avg Reward: 197.293985441, Learning Rate: 0.01, buffer_len: 327942\n",
      "Loss: -3.08729600906\n",
      "[NOR] Episode: 750, Length: 264, e: 0.05, Avg Reward: 217.015940356, Learning Rate: 0.01, buffer_len: 331230\n",
      "Loss: 0.377942085266\n",
      "[NOR] Episode: 760, Length: 388, e: 0.05, Avg Reward: 202.407697194, Learning Rate: 0.01, buffer_len: 335269\n",
      "Loss: -0.0894748270512\n",
      "[NOR] Episode: 770, Length: 390, e: 0.05, Avg Reward: 185.952365653, Learning Rate: 0.01, buffer_len: 340056\n",
      "Loss: -0.041238874197\n",
      "[NOR] Episode: 780, Length: 598, e: 0.05, Avg Reward: 173.576232568, Learning Rate: 0.01, buffer_len: 345355\n",
      "Loss: -3.795327425\n",
      "[NOR] Episode: 790, Length: 441, e: 0.05, Avg Reward: 170.063655872, Learning Rate: 0.01, buffer_len: 350719\n",
      "Loss: -2.32701253891\n",
      "[NOR] Episode: 800, Length: 405, e: 0.05, Avg Reward: 184.265564166, Learning Rate: 0.01, buffer_len: 355096\n",
      "Loss: -2.28039979935\n",
      "[NOR] Episode: 810, Length: 428, e: 0.05, Avg Reward: 206.349868797, Learning Rate: 0.01, buffer_len: 359447\n",
      "Loss: -2.57940840721\n",
      "[NOR] Episode: 820, Length: 393, e: 0.05, Avg Reward: 192.443662346, Learning Rate: 0.01, buffer_len: 362943\n",
      "Loss: -2.11183071136\n",
      "[NOR] Episode: 830, Length: 341, e: 0.05, Avg Reward: 186.291074372, Learning Rate: 0.01, buffer_len: 365990\n",
      "Loss: -3.04389095306\n",
      "[NOR] Episode: 840, Length: 255, e: 0.05, Avg Reward: 171.599345176, Learning Rate: 0.01, buffer_len: 369058\n",
      "Loss: -0.987833976746\n",
      "[NOR] Episode: 850, Length: 329, e: 0.05, Avg Reward: 196.847232223, Learning Rate: 0.01, buffer_len: 373054\n",
      "Loss: -0.492507457733\n",
      "[NOR] Episode: 860, Length: 461, e: 0.05, Avg Reward: 203.411475945, Learning Rate: 0.01, buffer_len: 377195\n",
      "Loss: -3.35609412193\n",
      "[NOR] Episode: 870, Length: 513, e: 0.05, Avg Reward: 196.288466261, Learning Rate: 0.01, buffer_len: 381437\n",
      "Loss: -2.01412510872\n",
      "[NOR] Episode: 880, Length: 330, e: 0.05, Avg Reward: 177.65226335, Learning Rate: 0.01, buffer_len: 385432\n",
      "Loss: -2.93307852745\n",
      "[NOR] Episode: 890, Length: 332, e: 0.05, Avg Reward: 208.236292156, Learning Rate: 0.01, buffer_len: 389145\n",
      "Loss: -1.03854584694\n",
      "[NOR] Episode: 900, Length: 424, e: 0.05, Avg Reward: 205.64086227, Learning Rate: 0.01, buffer_len: 392959\n",
      "Loss: -1.37017297745\n",
      "[NOR] Episode: 910, Length: 562, e: 0.05, Avg Reward: 201.745980331, Learning Rate: 0.01, buffer_len: 396804\n",
      "Loss: -3.04136896133\n",
      "[MAX] Episode: 917, Length: 287, Reward: 264.511332971, buffer_len: 398973\n",
      "[NOR] Episode: 920, Length: 311, e: 0.05, Avg Reward: 177.766120724, Learning Rate: 0.01, buffer_len: 399923\n",
      "Loss: -3.71695971489\n",
      "[NOR] Episode: 930, Length: 219, e: 0.05, Avg Reward: 174.094323812, Learning Rate: 0.01, buffer_len: 403329\n",
      "Loss: 1.02835655212\n",
      "[NOR] Episode: 940, Length: 279, e: 0.05, Avg Reward: 185.281803715, Learning Rate: 0.01, buffer_len: 407295\n",
      "Loss: -0.13734215498\n",
      "[NOR] Episode: 950, Length: 276, e: 0.05, Avg Reward: 225.134784126, Learning Rate: 0.01, buffer_len: 410934\n",
      "Loss: -4.93406581879\n",
      "[NOR] Episode: 960, Length: 789, e: 0.05, Avg Reward: 213.794884795, Learning Rate: 0.01, buffer_len: 414648\n",
      "Loss: 10.4292163849\n",
      "[NOR] Episode: 970, Length: 353, e: 0.05, Avg Reward: 217.231166889, Learning Rate: 0.01, buffer_len: 417850\n",
      "Loss: -3.47314238548\n",
      "[NOR] Episode: 980, Length: 287, e: 0.05, Avg Reward: 202.735887912, Learning Rate: 0.01, buffer_len: 421454\n",
      "Loss: 0.433971971273\n",
      "[NOR] Episode: 990, Length: 225, e: 0.05, Avg Reward: 197.238337397, Learning Rate: 0.01, buffer_len: 425541\n",
      "Loss: -0.044681429863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 23:52:55,906] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/tmp/monitor43/openaigym.video.5.4373.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 1000, Length: 1000, e: 0.05, Avg Reward: 100.007555524, Learning Rate: 0.01, buffer_len: 433411\n",
      "Loss: -0.0355783104897\n",
      "[NOR] Episode: 1010, Length: 1000, e: 0.05, Avg Reward: 159.441273339, Learning Rate: 0.01, buffer_len: 437824\n",
      "Loss: -0.753508210182\n",
      "[NOR] Episode: 1020, Length: 232, e: 0.05, Avg Reward: 213.414597847, Learning Rate: 0.01, buffer_len: 440960\n",
      "Loss: -5.68595314026\n",
      "[NOR] Episode: 1030, Length: 375, e: 0.05, Avg Reward: 169.392162183, Learning Rate: 0.01, buffer_len: 444111\n",
      "Loss: -1.78346991539\n",
      "[NOR] Episode: 1040, Length: 190, e: 0.05, Avg Reward: 197.883674202, Learning Rate: 0.01, buffer_len: 446994\n",
      "Loss: 5.00374126434\n",
      "[NOR] Episode: 1050, Length: 263, e: 0.05, Avg Reward: 166.884857618, Learning Rate: 0.01, buffer_len: 449841\n",
      "Loss: -3.50586009026\n",
      "[NOR] Episode: 1060, Length: 358, e: 0.05, Avg Reward: 214.492849972, Learning Rate: 0.01, buffer_len: 452911\n",
      "Loss: -0.321946591139\n",
      "[NOR] Episode: 1070, Length: 299, e: 0.05, Avg Reward: 198.930713135, Learning Rate: 0.01, buffer_len: 455989\n",
      "Loss: -12.2367773056\n",
      "[MAX] Episode: 1073, Length: 244, Reward: 267.406740858, buffer_len: 456788\n",
      "[NOR] Episode: 1080, Length: 277, e: 0.05, Avg Reward: 199.692782747, Learning Rate: 0.01, buffer_len: 458758\n",
      "Loss: -3.28596329689\n",
      "[NOR] Episode: 1090, Length: 282, e: 0.05, Avg Reward: 104.271088692, Learning Rate: 0.01, buffer_len: 461372\n",
      "Loss: -2.47930979729\n",
      "[NOR] Episode: 1100, Length: 329, e: 0.05, Avg Reward: 213.602561932, Learning Rate: 0.01, buffer_len: 464366\n",
      "Loss: -3.30673694611\n",
      "[NOR] Episode: 1110, Length: 264, e: 0.05, Avg Reward: 197.439033442, Learning Rate: 0.01, buffer_len: 467961\n",
      "Loss: -0.326173484325\n",
      "[NOR] Episode: 1120, Length: 285, e: 0.05, Avg Reward: 194.601615449, Learning Rate: 0.01, buffer_len: 470864\n",
      "Loss: -4.65030050278\n",
      "[NOR] Episode: 1130, Length: 258, e: 0.05, Avg Reward: 165.943053274, Learning Rate: 0.01, buffer_len: 473817\n",
      "Loss: -4.05853462219\n",
      "[NOR] Episode: 1140, Length: 319, e: 0.05, Avg Reward: 159.520271286, Learning Rate: 0.01, buffer_len: 476860\n",
      "Loss: 6.44841623306\n",
      "[NOR] Episode: 1150, Length: 217, e: 0.05, Avg Reward: 86.5913532029, Learning Rate: 0.01, buffer_len: 479352\n",
      "Loss: 1.55849742889\n",
      "[NOR] Episode: 1160, Length: 347, e: 0.05, Avg Reward: 175.178320219, Learning Rate: 0.01, buffer_len: 482105\n",
      "Loss: -1.67281126976\n",
      "[NOR] Episode: 1170, Length: 272, e: 0.05, Avg Reward: 61.2528942978, Learning Rate: 0.01, buffer_len: 485070\n",
      "Loss: 0.134821206331\n",
      "[NOR] Episode: 1180, Length: 304, e: 0.05, Avg Reward: 154.801208856, Learning Rate: 0.01, buffer_len: 489579\n",
      "Loss: -0.613305687904\n",
      "[NOR] Episode: 1190, Length: 299, e: 0.05, Avg Reward: 132.533904988, Learning Rate: 0.01, buffer_len: 492398\n",
      "Loss: -6.44462108612\n",
      "[NOR] Episode: 1200, Length: 275, e: 0.05, Avg Reward: 89.8423755884, Learning Rate: 0.01, buffer_len: 495373\n",
      "Loss: -1.7472012043\n",
      "[NOR] Episode: 1210, Length: 241, e: 0.05, Avg Reward: 123.228952477, Learning Rate: 0.01, buffer_len: 499561\n",
      "Loss: 2.63221931458\n",
      "[NOR] Episode: 1220, Length: 209, e: 0.05, Avg Reward: 207.892126505, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.02719116211\n",
      "[NOR] Episode: 1230, Length: 224, e: 0.05, Avg Reward: 165.742049002, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.13670134544\n",
      "[NOR] Episode: 1240, Length: 227, e: 0.05, Avg Reward: 109.710095329, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.15527796745\n",
      "[NOR] Episode: 1250, Length: 277, e: 0.05, Avg Reward: 75.5133912695, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.02510118484\n",
      "[NOR] Episode: 1260, Length: 71, e: 0.05, Avg Reward: 43.4630547367, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.36355066299\n",
      "[NOR] Episode: 1270, Length: 93, e: 0.05, Avg Reward: -19.1836923543, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.505966305733\n",
      "[NOR] Episode: 1280, Length: 74, e: 0.05, Avg Reward: 3.86161460839, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.671349644661\n",
      "[NOR] Episode: 1290, Length: 85, e: 0.05, Avg Reward: -9.12541667373, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.37534546852\n",
      "[NOR] Episode: 1300, Length: 240, e: 0.05, Avg Reward: -20.5339283846, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.88957023621\n",
      "[NOR] Episode: 1310, Length: 593, e: 0.05, Avg Reward: 53.9245912255, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.379313588142\n",
      "[NOR] Episode: 1320, Length: 86, e: 0.05, Avg Reward: -26.0232462652, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.5859327316\n",
      "[NOR] Episode: 1330, Length: 243, e: 0.05, Avg Reward: 124.823200944, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.284196972847\n",
      "[NOR] Episode: 1340, Length: 91, e: 0.05, Avg Reward: 92.9933757477, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.13580513\n",
      "[NOR] Episode: 1350, Length: 104, e: 0.05, Avg Reward: 39.300677601, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.55808591843\n",
      "[NOR] Episode: 1360, Length: 274, e: 0.05, Avg Reward: 129.056554197, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.38862895966\n",
      "[NOR] Episode: 1370, Length: 114, e: 0.05, Avg Reward: 71.2173023927, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.815845608711\n",
      "[NOR] Episode: 1380, Length: 315, e: 0.05, Avg Reward: 188.774051326, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.889446020126\n",
      "[NOR] Episode: 1390, Length: 376, e: 0.05, Avg Reward: 165.7737263, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.17390942574\n",
      "[NOR] Episode: 1400, Length: 272, e: 0.05, Avg Reward: 174.168404646, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.26659393311\n",
      "[NOR] Episode: 1410, Length: 406, e: 0.05, Avg Reward: 157.662117512, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.820297300816\n",
      "[NOR] Episode: 1420, Length: 470, e: 0.05, Avg Reward: 108.689141529, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.12143182755\n",
      "[NOR] Episode: 1430, Length: 174, e: 0.05, Avg Reward: 96.602485488, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.75592041016\n",
      "[NOR] Episode: 1440, Length: 324, e: 0.05, Avg Reward: 119.903160613, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0175436064601\n",
      "[NOR] Episode: 1450, Length: 418, e: 0.05, Avg Reward: 144.43496483, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.19056224823\n",
      "[NOR] Episode: 1460, Length: 470, e: 0.05, Avg Reward: 131.236439779, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.728102326393\n",
      "[NOR] Episode: 1470, Length: 298, e: 0.05, Avg Reward: 142.852886105, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.44135141373\n",
      "[NOR] Episode: 1480, Length: 251, e: 0.05, Avg Reward: 62.4747485574, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.326469063759\n",
      "[NOR] Episode: 1490, Length: 313, e: 0.05, Avg Reward: 74.0046921105, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.24655067921\n",
      "[NOR] Episode: 1500, Length: 406, e: 0.05, Avg Reward: 96.0221355987, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.07833218575\n",
      "[NOR] Episode: 1510, Length: 169, e: 0.05, Avg Reward: 108.587955352, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.85966134071\n",
      "[NOR] Episode: 1520, Length: 265, e: 0.05, Avg Reward: 78.5877055093, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.979785501957\n",
      "[NOR] Episode: 1530, Length: 407, e: 0.05, Avg Reward: 108.341203194, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.79890346527\n",
      "[NOR] Episode: 1540, Length: 254, e: 0.05, Avg Reward: 43.1003234758, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.27957820892\n",
      "[NOR] Episode: 1550, Length: 270, e: 0.05, Avg Reward: 51.7021375367, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.58237457275\n",
      "[NOR] Episode: 1560, Length: 253, e: 0.05, Avg Reward: 1.28145752185, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.94786310196\n",
      "[NOR] Episode: 1570, Length: 265, e: 0.05, Avg Reward: 65.8951826575, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.123950719833\n",
      "[NOR] Episode: 1580, Length: 195, e: 0.05, Avg Reward: -4.92162917912, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.65895843506\n",
      "[NOR] Episode: 1590, Length: 693, e: 0.05, Avg Reward: 112.346438094, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.23516345024\n",
      "[NOR] Episode: 1600, Length: 189, e: 0.05, Avg Reward: 113.606029633, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.687908887863\n",
      "[NOR] Episode: 1610, Length: 261, e: 0.05, Avg Reward: 49.6731899275, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.11634206772\n",
      "[NOR] Episode: 1620, Length: 273, e: 0.05, Avg Reward: 38.8912582827, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.825705766678\n",
      "[NOR] Episode: 1630, Length: 239, e: 0.05, Avg Reward: 35.6430402547, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.91894388199\n",
      "[NOR] Episode: 1640, Length: 458, e: 0.05, Avg Reward: 161.667942553, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.27372741699\n",
      "[NOR] Episode: 1650, Length: 311, e: 0.05, Avg Reward: 155.986418028, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.68073773384\n",
      "[NOR] Episode: 1660, Length: 249, e: 0.05, Avg Reward: 171.596333902, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.32817697525\n",
      "[NOR] Episode: 1670, Length: 1000, e: 0.05, Avg Reward: 12.0091245814, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.32909941673\n",
      "[NOR] Episode: 1680, Length: 161, e: 0.05, Avg Reward: -16.1071573437, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.54058241844\n",
      "[NOR] Episode: 1690, Length: 365, e: 0.05, Avg Reward: 105.257091316, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.67678070068\n",
      "[NOR] Episode: 1700, Length: 364, e: 0.05, Avg Reward: 75.6276813401, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.29269695282\n",
      "[NOR] Episode: 1710, Length: 122, e: 0.05, Avg Reward: 98.1987290331, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.23434185982\n",
      "[NOR] Episode: 1720, Length: 164, e: 0.05, Avg Reward: 119.041178955, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.78378415108\n",
      "[NOR] Episode: 1730, Length: 280, e: 0.05, Avg Reward: 103.841966318, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.67107963562\n",
      "[NOR] Episode: 1740, Length: 454, e: 0.05, Avg Reward: 182.004170694, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.567433297634\n",
      "[NOR] Episode: 1750, Length: 233, e: 0.05, Avg Reward: 181.38864934, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.54531216621\n",
      "[NOR] Episode: 1760, Length: 322, e: 0.05, Avg Reward: 124.126910443, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.25332260132\n",
      "[NOR] Episode: 1770, Length: 348, e: 0.05, Avg Reward: 65.5549783455, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.39938926697\n",
      "[NOR] Episode: 1780, Length: 1000, e: 0.05, Avg Reward: -8.06563636135, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.96382474899\n",
      "[NOR] Episode: 1790, Length: 108, e: 0.05, Avg Reward: 75.175282496, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.26914429665\n",
      "[NOR] Episode: 1800, Length: 373, e: 0.05, Avg Reward: 75.4441411671, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.58214759827\n",
      "[NOR] Episode: 1810, Length: 281, e: 0.05, Avg Reward: 90.7488626534, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.49882984161\n",
      "[NOR] Episode: 1820, Length: 270, e: 0.05, Avg Reward: 87.3604160454, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.53590190411\n",
      "[NOR] Episode: 1830, Length: 152, e: 0.05, Avg Reward: 73.7522056891, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.41915225983\n",
      "[NOR] Episode: 1840, Length: 424, e: 0.05, Avg Reward: 181.30481362, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.17993927\n",
      "[NOR] Episode: 1850, Length: 260, e: 0.05, Avg Reward: 116.804349006, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.6026391983\n",
      "[NOR] Episode: 1860, Length: 111, e: 0.05, Avg Reward: 165.545993365, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.37161040306\n",
      "[NOR] Episode: 1870, Length: 350, e: 0.05, Avg Reward: 175.243758536, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.961658596992\n",
      "[NOR] Episode: 1880, Length: 342, e: 0.05, Avg Reward: 207.907326893, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.0673456192\n",
      "[MAX] Episode: 1888, Length: 441, Reward: 267.702837399, buffer_len: 500000\n",
      "[NOR] Episode: 1890, Length: 305, e: 0.05, Avg Reward: 107.454998863, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.98991799355\n",
      "[NOR] Episode: 1900, Length: 139, e: 0.05, Avg Reward: 75.8814590248, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.95097255707\n",
      "[NOR] Episode: 1910, Length: 245, e: 0.05, Avg Reward: 112.642239258, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.12411797047\n",
      "[NOR] Episode: 1920, Length: 353, e: 0.05, Avg Reward: 182.359487373, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.73602962494\n",
      "[NOR] Episode: 1930, Length: 265, e: 0.05, Avg Reward: 148.948511135, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.79807853699\n",
      "[NOR] Episode: 1940, Length: 1000, e: 0.05, Avg Reward: 162.729791747, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.236284017563\n",
      "[NOR] Episode: 1950, Length: 939, e: 0.05, Avg Reward: 73.7651423339, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.33679574728\n",
      "[NOR] Episode: 1960, Length: 314, e: 0.05, Avg Reward: 137.943095503, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.42247962952\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ee8b5d685455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mupdate_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m )\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-0370d3e4386d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, keep_prob, e, learning_rate, print_step, update_target, episodes, max_episode_length, batch_size)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mr_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mep_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m     \u001b[0mfetch_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_handles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0mfetch_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_fetchable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;34m\"\"\"The full name of this operation.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 40000.\n",
    "model.fit(\n",
    "    env, print_step=10, \n",
    "    episodes=int(1e5), max_episode_length=10000, batch_size=32,\n",
    "    learning_rate = 0.01, # lambda t: 0.05 * k / (k + t)\n",
    "    e = interp1d([0, 100000], [0.3, 0.05], fill_value=0.05, bounds_error=False),\n",
    "    update_target = 1,\n",
    "    keep_prob = 0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-08 21:48:38,328] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Discrete(4)\n",
      "147.728599704\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-921da6990744>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "print(env.action_space)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path = os.getcwd() + \"/actor-critic.model\"\n",
    "logs_path = \"logs/run0\"\n",
    "\n",
    "\n",
    "model_run = LanderAC(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path,\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total = 0.\n",
    "    ep = 0\n",
    "    while not done and ep < 700:\n",
    "        ep += 1\n",
    "        a = model_run.choose_action(s, 1.0)\n",
    "        s, r, done, info = env.step(a)\n",
    "        total += r\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "    print(total)\n",
    "env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# gym.upload(\"tmp/monitor{}\".format(run), api_key='sk_WASyK12rQxais3gwyG4Vg', ignore_open_monitors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gym.upload?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coconut (Python 2)",
   "language": "coconut",
   "name": "coconut2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3.6
   },
   "file_extension": ".coco",
   "mimetype": "text/x-python3",
   "name": "coconut",
   "pygments_lexer": "coconut"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
