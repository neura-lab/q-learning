{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'module' object has no attribute '__module__'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "from tfinterface.reinforcement import DeepActorCritic, ExpandedStateEnv\n",
    "from tfinterface.interfaces import EnvironmentInterface\n",
    "from tfinterface.model_base import ModelBase\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "import numbers\n",
    "\n",
    "\n",
    "def get_run():\n",
    "    try:\n",
    "        with open(\"run.txt\") as f:\n",
    "            run = int(f.read().split(\"/n\")[0])\n",
    "    except:\n",
    "        run = -1\n",
    "    \n",
    "    with open(\"run.txt\", 'w+') as f:\n",
    "        run += 1\n",
    "        \n",
    "        f.seek(0)\n",
    "        f.write(str(run))\n",
    "        f.truncate()\n",
    "        \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LanderAC(DeepActorCritic):\n",
    "    \n",
    "    def define_actor_network(self, inputs, n_actions, n_states):\n",
    "        ops = dict(\n",
    "            trainable=True,\n",
    "            kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "            use_bias=False,\n",
    "            bias_initializer=None\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            inputs.s\n",
    "            |> tf.layers.dense$(?, 128, activation=tf.nn.relu, name='relu_layer', **ops)\n",
    "            |> tf.nn.dropout$(?, inputs.keep_prob)\n",
    "            |> tf.layers.dense$(?, n_actions, activation=tf.nn.softmax, name='softmax_layer', **ops)\n",
    "        )\n",
    "\n",
    "\n",
    "    def define_critic_network(self, inputs, n_actions, n_states):\n",
    "        ops = dict(\n",
    "            trainable=True,\n",
    "            kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "            use_bias=False,\n",
    "            bias_initializer=None\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            inputs.s\n",
    "            |> tf.layers.dense$(?, 128, activation=tf.nn.relu, name='relu_layer', **ops)\n",
    "            |> tf.layers.dense$(?, 1, name='linear_layer', **ops)\n",
    "            |> (lambda t: t[:, 0])\n",
    "        )\n",
    "    \n",
    "    def fit(self, env, keep_prob=0.5, e=0., learning_rate=0.01, print_step=10, update_target=1, episodes=100000, max_episode_length=float('inf'), batch_size=32):\n",
    "        r_total = 0.\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            ep_step = 0\n",
    "            s = env.reset()\n",
    "            episode_length = 0\n",
    "            ep_reward = 0.\n",
    "\n",
    "\n",
    "            while not done and ep_step <= max_episode_length:\n",
    "                self.global_step += 1\n",
    "                episode_length += 1\n",
    "                ep_step += 1\n",
    "\n",
    "                _learning_rate = learning_rate(self.global_step) if hasattr(learning_rate, '__call__') else learning_rate\n",
    "                _e = e(self.global_step) if hasattr(e, '__call__') else e\n",
    "\n",
    "                a = self.choose_action(s, keep_prob, e=_e)\n",
    "                s1, r, done, info = env.step(a)\n",
    "                r_total += r\n",
    "                ep_reward += r\n",
    "\n",
    "                self.replay_buffer.append((s, a, r, s1, float(done)))\n",
    "\n",
    "                S, A, R, S1, Done = self.replay_buffer.random_batch(batch_size).unzip()\n",
    "                V1 = self.sess.run(self.target_critic.V, feed_dict={self.inputs.s: S1, self.inputs.keep_prob: 1.0})\n",
    "\n",
    "                feed_dict = self.fit_feed(S, A, R, V1, Done, _learning_rate, True)\n",
    "\n",
    "                \n",
    "                _, summaries = self.sess.run([self.update, self.summaries], feed_dict=feed_dict)\n",
    "                self.writer.add_summary(summaries)\n",
    "\n",
    "                if self.global_step % update_target == 0:\n",
    "                    self.sess.run(self.update_target)\n",
    "\n",
    "                s = s1\n",
    "\n",
    "\n",
    "\n",
    "            episode_length_summary = self.sess.run(self.episode_length_summary,\n",
    "                                                   feed_dict={self.inputs.episode_length: episode_length})\n",
    "            self.writer.add_summary(episode_length_summary)\n",
    "\n",
    "\n",
    "            if ep_reward >= self.global_max:\n",
    "                print(\"[MAX] Episode: {}, Length: {}, Reward: {}, buffer_len: {}\".format(episode, episode_length, ep_reward, len(self.replay_buffer)))\n",
    "                self.save(model_path = self.model_path + \".max\")\n",
    "                self.global_max = ep_reward\n",
    "\n",
    "\n",
    "            if episode % print_step == 0 and episode > 0:\n",
    "                avg_r = r_total / print_step\n",
    "                actor_loss = self.sess.run(self.actor.loss, feed_dict=feed_dict)\n",
    "                print(\"[NOR] Episode: {}, Length: {}, e: {}, Avg Reward: {}, Learning Rate: {}, buffer_len: {}\".format(episode, episode_length, _e, avg_r, _learning_rate, len(self.replay_buffer)))\n",
    "                print(\"Loss: {}\".format(actor_loss))\n",
    "                self.save()\n",
    "                r_total = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:11:13,215] Making new env: LunarLander-v2\n",
      "[2017-03-04 19:11:13,219] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-04 19:11:13,219] Creating monitor directory tmp/monitor36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Discrete(4)\n",
      "Run: 36\n"
     ]
    }
   ],
   "source": [
    "run = get_run()\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = wrappers.Monitor(env, \"tmp/monitor{}\".format(run))\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "print(env.action_space)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path = os.getcwd() + \"/actor-critic.model\"\n",
    "logs_path = \"logs/run{}\".format(run)\n",
    "\n",
    "print(\"Run: {}\".format(run))\n",
    "\n",
    "model = LanderAC(\n",
    "    n_actions, n_states, y=0.9999, \n",
    "    buffer_length=500000, pi=0.1,\n",
    "    model_path = model_path,\n",
    "    logs_path = logs_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:11:14,404] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander-actor-critic/tmp/monitor36/openaigym.video.1.15919.video000000.mp4\n",
      "[2017-03-04 19:11:15,605] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander-actor-critic/tmp/monitor36/openaigym.video.1.15919.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 0, Length: 85, Reward: -571.101488109, buffer_len: 85\n",
      "[MAX] Episode: 2, Length: 72, Reward: -558.427635262, buffer_len: 227\n",
      "[MAX] Episode: 3, Length: 56, Reward: -374.769981865, buffer_len: 283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:11:17,682] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander-actor-critic/tmp/monitor36/openaigym.video.1.15919.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 10, Length: 69, e: 0.29948875, Avg Reward: -617.693441672, Learning Rate: 0.01, buffer_len: 819\n",
      "Loss: -4.87128400803\n",
      "[MAX] Episode: 17, Length: 59, Reward: -362.720455936, buffer_len: 1257\n",
      "[NOR] Episode: 20, Length: 61, e: 0.299084375, Avg Reward: -482.511761697, Learning Rate: 0.01, buffer_len: 1466\n",
      "Loss: 3.48610496521\n",
      "[MAX] Episode: 21, Length: 54, Reward: -306.262725344, buffer_len: 1520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:11:22,536] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander-actor-critic/tmp/monitor36/openaigym.video.1.15919.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 30, Length: 60, e: 0.298651875, Avg Reward: -531.426202984, Learning Rate: 0.01, buffer_len: 2158\n",
      "Loss: 0.833386421204\n",
      "[NOR] Episode: 40, Length: 68, e: 0.298190625, Avg Reward: -570.186718292, Learning Rate: 0.01, buffer_len: 2896\n",
      "Loss: 4.84059238434\n",
      "[NOR] Episode: 50, Length: 74, e: 0.29779125, Avg Reward: -496.402465529, Learning Rate: 0.01, buffer_len: 3535\n",
      "Loss: 1.01878547668\n",
      "[MAX] Episode: 52, Length: 57, Reward: -284.899332524, buffer_len: 3659\n",
      "[NOR] Episode: 60, Length: 60, e: 0.297381875, Avg Reward: -465.73124642, Learning Rate: 0.01, buffer_len: 4190\n",
      "Loss: 2.1835258007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:11:30,733] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander-actor-critic/tmp/monitor36/openaigym.video.1.15919.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 70, Length: 71, e: 0.296945625, Avg Reward: -590.644976548, Learning Rate: 0.01, buffer_len: 4888\n",
      "Loss: -0.034855723381\n",
      "[MAX] Episode: 75, Length: 58, Reward: -238.899010409, buffer_len: 5238\n",
      "[NOR] Episode: 80, Length: 89, e: 0.296493125, Avg Reward: -533.939736028, Learning Rate: 0.01, buffer_len: 5612\n",
      "Loss: 4.02430725098\n",
      "[NOR] Episode: 90, Length: 96, e: 0.29605, Avg Reward: -522.408789834, Learning Rate: 0.01, buffer_len: 6321\n",
      "Loss: 4.97219276428\n",
      "[NOR] Episode: 100, Length: 87, e: 0.29560375, Avg Reward: -542.612176749, Learning Rate: 0.01, buffer_len: 7035\n",
      "Loss: 1.28779625893\n",
      "[NOR] Episode: 110, Length: 62, e: 0.29517875, Avg Reward: -506.627986272, Learning Rate: 0.01, buffer_len: 7715\n",
      "Loss: 5.41174030304\n",
      "[NOR] Episode: 120, Length: 58, e: 0.2947325, Avg Reward: -569.7389836, Learning Rate: 0.01, buffer_len: 8429\n",
      "Loss: -1.61611795425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:11:43,936] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander-actor-critic/tmp/monitor36/openaigym.video.1.15919.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 130, Length: 74, e: 0.294261875, Avg Reward: -551.849909444, Learning Rate: 0.01, buffer_len: 9182\n",
      "Loss: -7.53889751434\n",
      "[MAX] Episode: 133, Length: 67, Reward: -152.193558724, buffer_len: 9396\n",
      "[NOR] Episode: 140, Length: 88, e: 0.29381625, Avg Reward: -492.695489815, Learning Rate: 0.01, buffer_len: 9895\n",
      "Loss: -7.35953712463\n",
      "[MAX] Episode: 150, Length: 58, Reward: -92.5424785727, buffer_len: 10667\n",
      "[NOR] Episode: 150, Length: 58, e: 0.29333375, Avg Reward: -408.062066576, Learning Rate: 0.01, buffer_len: 10667\n",
      "Loss: -8.86532783508\n",
      "[MAX] Episode: 151, Length: 72, Reward: -67.6398392542, buffer_len: 10739\n",
      "[NOR] Episode: 160, Length: 68, e: 0.292845625, Avg Reward: -315.556604986, Learning Rate: 0.01, buffer_len: 11448\n",
      "Loss: -16.2126865387\n",
      "[NOR] Episode: 170, Length: 99, e: 0.29232, Avg Reward: -223.784820495, Learning Rate: 0.01, buffer_len: 12289\n",
      "Loss: -5.08924198151\n",
      "[MAX] Episode: 174, Length: 123, Reward: -23.7613187799, buffer_len: 12674\n",
      "[NOR] Episode: 180, Length: 66, e: 0.29173125, Avg Reward: -158.250040705, Learning Rate: 0.01, buffer_len: 13231\n",
      "Loss: -1.86723327637\n",
      "[MAX] Episode: 185, Length: 98, Reward: -14.7547191687, buffer_len: 13700\n",
      "[NOR] Episode: 190, Length: 89, e: 0.2911325, Avg Reward: -113.223958619, Learning Rate: 0.01, buffer_len: 14189\n",
      "Loss: -11.4537706375\n",
      "[NOR] Episode: 200, Length: 82, e: 0.29045375, Avg Reward: -186.610783318, Learning Rate: 0.01, buffer_len: 15275\n",
      "Loss: -14.6633872986\n",
      "[NOR] Episode: 210, Length: 104, e: 0.289836875, Avg Reward: -150.63649911, Learning Rate: 0.01, buffer_len: 16262\n",
      "Loss: -11.2294101715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:12:10,069] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander-actor-critic/tmp/monitor36/openaigym.video.1.15919.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 220, Length: 92, e: 0.28924875, Avg Reward: -148.127760581, Learning Rate: 0.01, buffer_len: 17203\n",
      "Loss: -12.268453598\n",
      "[NOR] Episode: 230, Length: 84, e: 0.288654375, Avg Reward: -103.972376968, Learning Rate: 0.01, buffer_len: 18154\n",
      "Loss: -13.0516500473\n",
      "[NOR] Episode: 240, Length: 95, e: 0.28798125, Avg Reward: -161.962200794, Learning Rate: 0.01, buffer_len: 19231\n",
      "Loss: -8.52717399597\n",
      "[NOR] Episode: 250, Length: 104, e: 0.28731375, Avg Reward: -164.761372662, Learning Rate: 0.01, buffer_len: 20299\n",
      "Loss: -14.1318006516\n",
      "[NOR] Episode: 260, Length: 146, e: 0.286723125, Avg Reward: -150.81542635, Learning Rate: 0.01, buffer_len: 21244\n",
      "Loss: -26.0001525879\n",
      "[NOR] Episode: 270, Length: 99, e: 0.28604, Avg Reward: -162.677949998, Learning Rate: 0.01, buffer_len: 22337\n",
      "Loss: -26.1550369263\n",
      "[MAX] Episode: 277, Length: 105, Reward: -9.17961721343, buffer_len: 23025\n",
      "[NOR] Episode: 280, Length: 119, e: 0.285415625, Avg Reward: -114.827483527, Learning Rate: 0.01, buffer_len: 23336\n",
      "Loss: -32.6552734375\n",
      "[NOR] Episode: 290, Length: 124, e: 0.28472125, Avg Reward: -112.906403659, Learning Rate: 0.01, buffer_len: 24447\n",
      "Loss: -31.5366420746\n",
      "[NOR] Episode: 300, Length: 143, e: 0.284029375, Avg Reward: -101.709157254, Learning Rate: 0.01, buffer_len: 25554\n",
      "Loss: -27.6840362549\n",
      "[MAX] Episode: 303, Length: 137, Reward: 8.49435933182, buffer_len: 25877\n",
      "[NOR] Episode: 310, Length: 128, e: 0.283346875, Avg Reward: -113.955356108, Learning Rate: 0.01, buffer_len: 26646\n",
      "Loss: 11.7434511185\n",
      "[NOR] Episode: 320, Length: 107, e: 0.282680625, Avg Reward: -134.618685555, Learning Rate: 0.01, buffer_len: 27712\n",
      "Loss: -16.8053741455\n",
      "[NOR] Episode: 330, Length: 122, e: 0.28198625, Avg Reward: -209.994645593, Learning Rate: 0.01, buffer_len: 28823\n",
      "Loss: -22.4066028595\n",
      "[NOR] Episode: 340, Length: 76, e: 0.281289375, Avg Reward: -184.60793127, Learning Rate: 0.01, buffer_len: 29938\n",
      "Loss: -12.3989124298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:12:50,792] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander-actor-critic/tmp/monitor36/openaigym.video.1.15919.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 350, Length: 98, e: 0.280575, Avg Reward: -203.80922394, Learning Rate: 0.01, buffer_len: 31081\n",
      "Loss: -8.45300483704\n",
      "[NOR] Episode: 360, Length: 80, e: 0.27983125, Avg Reward: -138.310061803, Learning Rate: 0.01, buffer_len: 32271\n",
      "Loss: -24.2673435211\n",
      "[NOR] Episode: 370, Length: 112, e: 0.27908875, Avg Reward: -175.494501214, Learning Rate: 0.01, buffer_len: 33459\n",
      "Loss: -16.7844581604\n",
      "[NOR] Episode: 380, Length: 81, e: 0.278395625, Avg Reward: -193.94890201, Learning Rate: 0.01, buffer_len: 34568\n",
      "Loss: -5.3514919281\n",
      "[NOR] Episode: 390, Length: 105, e: 0.2776675, Avg Reward: -145.110874766, Learning Rate: 0.01, buffer_len: 35733\n",
      "Loss: -24.3319129944\n",
      "[NOR] Episode: 400, Length: 83, e: 0.27699, Avg Reward: -128.641432049, Learning Rate: 0.01, buffer_len: 36817\n",
      "Loss: -27.4935417175\n",
      "[NOR] Episode: 410, Length: 149, e: 0.276208125, Avg Reward: -143.588854345, Learning Rate: 0.01, buffer_len: 38068\n",
      "Loss: -1.73762726784\n",
      "[MAX] Episode: 414, Length: 1000, Reward: 43.1435405663, buffer_len: 39426\n",
      "[NOR] Episode: 420, Length: 109, e: 0.274900625, Avg Reward: -153.273680396, Learning Rate: 0.01, buffer_len: 40160\n",
      "Loss: -9.7711057663\n",
      "[NOR] Episode: 430, Length: 107, e: 0.274161875, Avg Reward: -148.027450678, Learning Rate: 0.01, buffer_len: 41342\n",
      "Loss: -14.072637558\n",
      "[NOR] Episode: 440, Length: 121, e: 0.27344625, Avg Reward: -130.051847669, Learning Rate: 0.01, buffer_len: 42487\n",
      "Loss: -28.6648674011\n",
      "[NOR] Episode: 450, Length: 108, e: 0.272685, Avg Reward: -137.856776709, Learning Rate: 0.01, buffer_len: 43705\n",
      "Loss: -5.53242206573\n",
      "[NOR] Episode: 460, Length: 94, e: 0.271935, Avg Reward: -134.826897988, Learning Rate: 0.01, buffer_len: 44905\n",
      "Loss: -7.82534503937\n",
      "[MAX] Episode: 466, Length: 1000, Reward: 43.2143371843, buffer_len: 46579\n",
      "[MAX] Episode: 470, Length: 386, Reward: 229.093321862, buffer_len: 47403\n",
      "[NOR] Episode: 470, Length: 386, e: 0.27037375, Avg Reward: -79.0275522457, Learning Rate: 0.01, buffer_len: 47403\n",
      "Loss: -7.684071064\n",
      "[NOR] Episode: 480, Length: 139, e: 0.269493125, Avg Reward: -146.894520885, Learning Rate: 0.01, buffer_len: 48812\n",
      "Loss: -48.9481315613\n",
      "[NOR] Episode: 490, Length: 112, e: 0.268180625, Avg Reward: -111.889976548, Learning Rate: 0.01, buffer_len: 50912\n",
      "Loss: -0.808093309402\n",
      "[NOR] Episode: 500, Length: 97, e: 0.26737375, Avg Reward: -106.102637908, Learning Rate: 0.01, buffer_len: 52203\n",
      "Loss: -7.63380384445\n",
      "[NOR] Episode: 510, Length: 98, e: 0.266535625, Avg Reward: -99.0456662872, Learning Rate: 0.01, buffer_len: 53544\n",
      "Loss: -7.87038707733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:13:54,865] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander-actor-critic/tmp/monitor36/openaigym.video.1.15919.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 520, Length: 136, e: 0.26577, Avg Reward: -126.311275635, Learning Rate: 0.01, buffer_len: 54769\n",
      "Loss: 21.3416976929\n",
      "[NOR] Episode: 530, Length: 138, e: 0.265001875, Avg Reward: -83.6070518493, Learning Rate: 0.01, buffer_len: 55998\n",
      "Loss: -5.23163795471\n",
      "[NOR] Episode: 540, Length: 167, e: 0.26429875, Avg Reward: -140.607679203, Learning Rate: 0.01, buffer_len: 57123\n",
      "Loss: 1.7297334671\n",
      "[NOR] Episode: 550, Length: 124, e: 0.2634575, Avg Reward: -103.510191416, Learning Rate: 0.01, buffer_len: 58469\n",
      "Loss: 3.47176456451\n",
      "[NOR] Episode: 560, Length: 126, e: 0.26267375, Avg Reward: -150.505023216, Learning Rate: 0.01, buffer_len: 59723\n",
      "Loss: -20.0330142975\n",
      "[NOR] Episode: 570, Length: 135, e: 0.261885625, Avg Reward: -141.318813177, Learning Rate: 0.01, buffer_len: 60984\n",
      "Loss: -1.41389131546\n",
      "[NOR] Episode: 580, Length: 119, e: 0.260984375, Avg Reward: -107.892814457, Learning Rate: 0.01, buffer_len: 62426\n",
      "Loss: -2.45063281059\n",
      "[NOR] Episode: 590, Length: 157, e: 0.260166875, Avg Reward: -132.526342262, Learning Rate: 0.01, buffer_len: 63734\n",
      "Loss: -15.7819366455\n",
      "[NOR] Episode: 600, Length: 149, e: 0.259380625, Avg Reward: -137.03844946, Learning Rate: 0.01, buffer_len: 64992\n",
      "Loss: -22.6998882294\n",
      "[NOR] Episode: 610, Length: 99, e: 0.25862375, Avg Reward: -161.715008513, Learning Rate: 0.01, buffer_len: 66203\n",
      "Loss: -10.5519046783\n",
      "[NOR] Episode: 620, Length: 143, e: 0.257829375, Avg Reward: -105.751946015, Learning Rate: 0.01, buffer_len: 67474\n",
      "Loss: -4.67875862122\n",
      "[NOR] Episode: 630, Length: 138, e: 0.257069375, Avg Reward: -157.651087566, Learning Rate: 0.01, buffer_len: 68690\n",
      "Loss: -9.22437858582\n",
      "[NOR] Episode: 640, Length: 114, e: 0.25624875, Avg Reward: -125.00879443, Learning Rate: 0.01, buffer_len: 70003\n",
      "Loss: -20.4154472351\n",
      "[NOR] Episode: 650, Length: 99, e: 0.255505, Avg Reward: -117.890996423, Learning Rate: 0.01, buffer_len: 71193\n",
      "Loss: -16.0898628235\n",
      "[NOR] Episode: 660, Length: 95, e: 0.254821875, Avg Reward: -121.419976028, Learning Rate: 0.01, buffer_len: 72286\n",
      "Loss: -9.72866821289\n",
      "[NOR] Episode: 670, Length: 122, e: 0.25397625, Avg Reward: -143.082608508, Learning Rate: 0.01, buffer_len: 73639\n",
      "Loss: -6.65160036087\n",
      "[NOR] Episode: 680, Length: 107, e: 0.253213125, Avg Reward: -107.909858769, Learning Rate: 0.01, buffer_len: 74860\n",
      "Loss: -18.3570556641\n",
      "[NOR] Episode: 690, Length: 129, e: 0.2524875, Avg Reward: -111.887794932, Learning Rate: 0.01, buffer_len: 76021\n",
      "Loss: -32.0323791504\n",
      "[NOR] Episode: 700, Length: 94, e: 0.25178, Avg Reward: -141.517549383, Learning Rate: 0.01, buffer_len: 77153\n",
      "Loss: -35.3493232727\n",
      "[NOR] Episode: 710, Length: 122, e: 0.251060625, Avg Reward: -129.948559685, Learning Rate: 0.01, buffer_len: 78304\n",
      "Loss: -26.0408172607\n",
      "[NOR] Episode: 720, Length: 106, e: 0.250393125, Avg Reward: -117.407344658, Learning Rate: 0.01, buffer_len: 79372\n",
      "Loss: -11.6309700012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:15:06,120] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander-actor-critic/tmp/monitor36/openaigym.video.1.15919.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 730, Length: 160, e: 0.24954875, Avg Reward: -84.3950131498, Learning Rate: 0.01, buffer_len: 80723\n",
      "Loss: -14.778585434\n",
      "[NOR] Episode: 740, Length: 117, e: 0.24880875, Avg Reward: -114.684216888, Learning Rate: 0.01, buffer_len: 81907\n",
      "Loss: -14.3544216156\n",
      "[NOR] Episode: 750, Length: 115, e: 0.248088125, Avg Reward: -121.595614412, Learning Rate: 0.01, buffer_len: 83060\n",
      "Loss: 1.81228160858\n",
      "[NOR] Episode: 760, Length: 152, e: 0.2472775, Avg Reward: -101.477921548, Learning Rate: 0.01, buffer_len: 84357\n",
      "Loss: -12.2868366241\n",
      "[NOR] Episode: 770, Length: 140, e: 0.246573125, Avg Reward: -109.785850455, Learning Rate: 0.01, buffer_len: 85484\n",
      "Loss: -27.5905380249\n",
      "[NOR] Episode: 780, Length: 90, e: 0.24576625, Avg Reward: -127.51100721, Learning Rate: 0.01, buffer_len: 86775\n",
      "Loss: -3.17818832397\n",
      "[NOR] Episode: 790, Length: 151, e: 0.245010625, Avg Reward: -103.812341884, Learning Rate: 0.01, buffer_len: 87984\n",
      "Loss: -18.2061004639\n",
      "[NOR] Episode: 800, Length: 133, e: 0.24425375, Avg Reward: -97.7047699563, Learning Rate: 0.01, buffer_len: 89195\n",
      "Loss: -10.0064754486\n",
      "[NOR] Episode: 810, Length: 337, e: 0.24324625, Avg Reward: -43.7123037839, Learning Rate: 0.01, buffer_len: 90807\n",
      "Loss: -6.8553109169\n",
      "[NOR] Episode: 820, Length: 103, e: 0.2425075, Avg Reward: -104.826848407, Learning Rate: 0.01, buffer_len: 91989\n",
      "Loss: -32.842716217\n",
      "[NOR] Episode: 830, Length: 82, e: 0.241669375, Avg Reward: -97.7818878309, Learning Rate: 0.01, buffer_len: 93330\n",
      "Loss: -21.4235191345\n",
      "[NOR] Episode: 840, Length: 141, e: 0.240831875, Avg Reward: -111.028496657, Learning Rate: 0.01, buffer_len: 94670\n",
      "Loss: 0.56634414196\n",
      "[NOR] Episode: 850, Length: 92, e: 0.24020125, Avg Reward: -115.360948445, Learning Rate: 0.01, buffer_len: 95679\n",
      "Loss: -9.9198141098\n",
      "[NOR] Episode: 860, Length: 145, e: 0.23951625, Avg Reward: -101.797708506, Learning Rate: 0.01, buffer_len: 96775\n",
      "Loss: -11.201505661\n",
      "[NOR] Episode: 870, Length: 138, e: 0.2387425, Avg Reward: -123.961302544, Learning Rate: 0.01, buffer_len: 98013\n",
      "Loss: -7.32780265808\n",
      "[NOR] Episode: 880, Length: 163, e: 0.237984375, Avg Reward: -85.1868147397, Learning Rate: 0.01, buffer_len: 99226\n",
      "Loss: -17.8704299927\n",
      "[NOR] Episode: 890, Length: 139, e: 0.237244375, Avg Reward: -106.82651839, Learning Rate: 0.01, buffer_len: 100410\n",
      "Loss: -13.9779243469\n",
      "[NOR] Episode: 900, Length: 104, e: 0.2361925, Avg Reward: -130.939672497, Learning Rate: 0.01, buffer_len: 102093\n",
      "Loss: -14.4421710968\n",
      "[NOR] Episode: 910, Length: 97, e: 0.2348175, Avg Reward: -89.7325567411, Learning Rate: 0.01, buffer_len: 104293\n",
      "Loss: -38.5577774048\n",
      "[NOR] Episode: 920, Length: 105, e: 0.233521875, Avg Reward: -108.204593433, Learning Rate: 0.01, buffer_len: 106366\n",
      "Loss: -13.006696701\n",
      "[NOR] Episode: 930, Length: 86, e: 0.23282625, Avg Reward: -90.5250285243, Learning Rate: 0.01, buffer_len: 107479\n",
      "Loss: 1.54881083965\n",
      "[NOR] Episode: 940, Length: 117, e: 0.231955625, Avg Reward: -118.715588457, Learning Rate: 0.01, buffer_len: 108872\n",
      "Loss: -1.32343459129\n",
      "[NOR] Episode: 950, Length: 71, e: 0.2301925, Avg Reward: -90.5825905772, Learning Rate: 0.01, buffer_len: 111693\n",
      "Loss: -4.59202528\n",
      "[NOR] Episode: 960, Length: 98, e: 0.229439375, Avg Reward: -126.424306566, Learning Rate: 0.01, buffer_len: 112898\n",
      "Loss: -18.4534511566\n",
      "[NOR] Episode: 970, Length: 115, e: 0.228754375, Avg Reward: -95.836273417, Learning Rate: 0.01, buffer_len: 113994\n",
      "Loss: -11.4673624039\n",
      "[NOR] Episode: 980, Length: 132, e: 0.22791375, Avg Reward: -118.251507487, Learning Rate: 0.01, buffer_len: 115339\n",
      "Loss: -15.6018199921\n",
      "[NOR] Episode: 990, Length: 492, e: 0.2264125, Avg Reward: -128.426251763, Learning Rate: 0.01, buffer_len: 117741\n",
      "Loss: -17.8716163635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:16:52,144] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander-actor-critic/tmp/monitor36/openaigym.video.1.15919.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 1000, Length: 100, e: 0.22566625, Avg Reward: -77.1760817529, Learning Rate: 0.01, buffer_len: 118935\n",
      "Loss: -3.14226031303\n",
      "[NOR] Episode: 1010, Length: 108, e: 0.2239075, Avg Reward: -83.9033328478, Learning Rate: 0.01, buffer_len: 121749\n",
      "Loss: -2.67626905441\n",
      "[NOR] Episode: 1020, Length: 102, e: 0.2228825, Avg Reward: -113.370661759, Learning Rate: 0.01, buffer_len: 123389\n",
      "Loss: 0.577072143555\n",
      "[NOR] Episode: 1030, Length: 81, e: 0.222238125, Avg Reward: -92.8782097531, Learning Rate: 0.01, buffer_len: 124420\n",
      "Loss: -11.7655467987\n",
      "[NOR] Episode: 1040, Length: 97, e: 0.2216075, Avg Reward: -109.5065765, Learning Rate: 0.01, buffer_len: 125429\n",
      "Loss: -15.7261075974\n",
      "[NOR] Episode: 1050, Length: 139, e: 0.22093875, Avg Reward: -103.379439651, Learning Rate: 0.01, buffer_len: 126499\n",
      "Loss: -8.03443527222\n",
      "[NOR] Episode: 1060, Length: 73, e: 0.2202775, Avg Reward: -95.7564398895, Learning Rate: 0.01, buffer_len: 127557\n",
      "Loss: -6.34093523026\n",
      "[NOR] Episode: 1070, Length: 92, e: 0.219536875, Avg Reward: -87.0022738557, Learning Rate: 0.01, buffer_len: 128742\n",
      "Loss: -47.9081115723\n",
      "[NOR] Episode: 1080, Length: 1000, e: 0.2182375, Avg Reward: -85.4106828548, Learning Rate: 0.01, buffer_len: 130821\n",
      "Loss: -24.2088871002\n",
      "[NOR] Episode: 1090, Length: 125, e: 0.2176, Avg Reward: -109.560767651, Learning Rate: 0.01, buffer_len: 131841\n",
      "Loss: -0.8835709095\n",
      "[NOR] Episode: 1100, Length: 150, e: 0.21631375, Avg Reward: -77.0380361873, Learning Rate: 0.01, buffer_len: 133899\n",
      "Loss: -32.0215072632\n",
      "[NOR] Episode: 1110, Length: 116, e: 0.21553625, Avg Reward: -86.1508751002, Learning Rate: 0.01, buffer_len: 135143\n",
      "Loss: -31.1751976013\n",
      "[NOR] Episode: 1120, Length: 110, e: 0.214673125, Avg Reward: -110.87881395, Learning Rate: 0.01, buffer_len: 136524\n",
      "Loss: -8.34396457672\n",
      "[NOR] Episode: 1130, Length: 125, e: 0.2139275, Avg Reward: -102.988671487, Learning Rate: 0.01, buffer_len: 137717\n",
      "Loss: -11.007235527\n",
      "[NOR] Episode: 1140, Length: 1000, e: 0.212665625, Avg Reward: -68.7523740228, Learning Rate: 0.01, buffer_len: 139736\n",
      "Loss: -15.8005695343\n",
      "[NOR] Episode: 1150, Length: 114, e: 0.211984375, Avg Reward: -86.3650929667, Learning Rate: 0.01, buffer_len: 140826\n",
      "Loss: 2.27307224274\n",
      "[NOR] Episode: 1160, Length: 251, e: 0.21120125, Avg Reward: -83.2845933857, Learning Rate: 0.01, buffer_len: 142079\n",
      "Loss: -9.08471870422\n",
      "[NOR] Episode: 1170, Length: 85, e: 0.210311875, Avg Reward: -71.5727755638, Learning Rate: 0.01, buffer_len: 143502\n",
      "Loss: -21.0244178772\n",
      "[NOR] Episode: 1180, Length: 77, e: 0.20961375, Avg Reward: -73.4342605624, Learning Rate: 0.01, buffer_len: 144619\n",
      "Loss: -24.8458023071\n",
      "[NOR] Episode: 1190, Length: 303, e: 0.208759375, Avg Reward: -72.3522873732, Learning Rate: 0.01, buffer_len: 145986\n",
      "Loss: -33.6609191895\n",
      "[NOR] Episode: 1200, Length: 101, e: 0.207896875, Avg Reward: -79.4962009927, Learning Rate: 0.01, buffer_len: 147366\n",
      "Loss: -1.0230948925\n",
      "[NOR] Episode: 1210, Length: 78, e: 0.207229375, Avg Reward: -107.94806896, Learning Rate: 0.01, buffer_len: 148434\n",
      "Loss: -17.5144786835\n",
      "[NOR] Episode: 1220, Length: 105, e: 0.20596375, Avg Reward: -80.4943956795, Learning Rate: 0.01, buffer_len: 150459\n",
      "Loss: -10.0640640259\n",
      "[NOR] Episode: 1230, Length: 177, e: 0.205303125, Avg Reward: -108.077195269, Learning Rate: 0.01, buffer_len: 151516\n",
      "Loss: -3.6019012928\n",
      "[NOR] Episode: 1240, Length: 89, e: 0.204639375, Avg Reward: -90.6964348294, Learning Rate: 0.01, buffer_len: 152578\n",
      "Loss: 33.2786598206\n",
      "[NOR] Episode: 1250, Length: 84, e: 0.20389875, Avg Reward: -88.8124599634, Learning Rate: 0.01, buffer_len: 153763\n",
      "Loss: 4.43685340881\n",
      "[NOR] Episode: 1260, Length: 149, e: 0.203068125, Avg Reward: -82.5339467947, Learning Rate: 0.01, buffer_len: 155092\n",
      "Loss: 1.60993003845\n",
      "[NOR] Episode: 1270, Length: 127, e: 0.20232375, Avg Reward: -84.166190887, Learning Rate: 0.01, buffer_len: 156283\n",
      "Loss: -17.7260227203\n",
      "[NOR] Episode: 1280, Length: 101, e: 0.20138125, Avg Reward: -105.902030087, Learning Rate: 0.01, buffer_len: 157791\n",
      "Loss: -13.1659069061\n",
      "[NOR] Episode: 1290, Length: 144, e: 0.200521875, Avg Reward: -81.0437508329, Learning Rate: 0.01, buffer_len: 159166\n",
      "Loss: -6.57403230667\n",
      "[NOR] Episode: 1300, Length: 126, e: 0.199670625, Avg Reward: -95.3388962519, Learning Rate: 0.01, buffer_len: 160528\n",
      "Loss: -8.64019393921\n",
      "[NOR] Episode: 1310, Length: 78, e: 0.19839875, Avg Reward: -106.762992209, Learning Rate: 0.01, buffer_len: 162563\n",
      "Loss: -17.3273868561\n",
      "[NOR] Episode: 1320, Length: 1000, e: 0.197009375, Avg Reward: -73.4818317797, Learning Rate: 0.01, buffer_len: 164786\n",
      "Loss: -26.2682285309\n",
      "[NOR] Episode: 1330, Length: 117, e: 0.196301875, Avg Reward: -106.369763517, Learning Rate: 0.01, buffer_len: 165918\n",
      "Loss: -9.15470123291\n",
      "[NOR] Episode: 1340, Length: 108, e: 0.19508375, Avg Reward: -84.5441452821, Learning Rate: 0.01, buffer_len: 167867\n",
      "Loss: -3.68647193909\n",
      "[NOR] Episode: 1350, Length: 116, e: 0.193991875, Avg Reward: -42.6627035059, Learning Rate: 0.01, buffer_len: 169614\n",
      "Loss: -8.27213478088\n",
      "[NOR] Episode: 1360, Length: 112, e: 0.193255625, Avg Reward: -99.9050024341, Learning Rate: 0.01, buffer_len: 170792\n",
      "Loss: -8.83478927612\n",
      "[NOR] Episode: 1370, Length: 87, e: 0.192006875, Avg Reward: -63.047332616, Learning Rate: 0.01, buffer_len: 172790\n",
      "Loss: -5.01071882248\n",
      "[NOR] Episode: 1380, Length: 88, e: 0.190763125, Avg Reward: -56.3822031347, Learning Rate: 0.01, buffer_len: 174780\n",
      "Loss: -29.3735084534\n",
      "[NOR] Episode: 1390, Length: 107, e: 0.19012125, Avg Reward: -96.1135666681, Learning Rate: 0.01, buffer_len: 175807\n",
      "Loss: -11.6503343582\n",
      "[NOR] Episode: 1400, Length: 119, e: 0.188924375, Avg Reward: -69.5132886893, Learning Rate: 0.01, buffer_len: 177722\n",
      "Loss: -10.1684379578\n",
      "[NOR] Episode: 1410, Length: 97, e: 0.1881875, Avg Reward: -98.0462866186, Learning Rate: 0.01, buffer_len: 178901\n",
      "Loss: -36.9498748779\n",
      "[NOR] Episode: 1420, Length: 79, e: 0.187443125, Avg Reward: -109.411594388, Learning Rate: 0.01, buffer_len: 180092\n",
      "Loss: 1.86670100689\n",
      "[NOR] Episode: 1430, Length: 132, e: 0.186765, Avg Reward: -99.8071396095, Learning Rate: 0.01, buffer_len: 181177\n",
      "Loss: -1.37826323509\n",
      "[NOR] Episode: 1440, Length: 104, e: 0.18609125, Avg Reward: -100.299163496, Learning Rate: 0.01, buffer_len: 182255\n",
      "Loss: -18.0959777832\n",
      "[NOR] Episode: 1450, Length: 102, e: 0.185384375, Avg Reward: -116.405095038, Learning Rate: 0.01, buffer_len: 183386\n",
      "Loss: -5.45307302475\n",
      "[NOR] Episode: 1460, Length: 88, e: 0.184678125, Avg Reward: -94.589877122, Learning Rate: 0.01, buffer_len: 184516\n",
      "Loss: -12.9728908539\n",
      "[NOR] Episode: 1470, Length: 109, e: 0.183995625, Avg Reward: -69.0320630729, Learning Rate: 0.01, buffer_len: 185608\n",
      "Loss: -14.0611076355\n",
      "[NOR] Episode: 1480, Length: 79, e: 0.1833075, Avg Reward: -71.2174372888, Learning Rate: 0.01, buffer_len: 186709\n",
      "Loss: -11.4323797226\n",
      "[NOR] Episode: 1490, Length: 130, e: 0.182068125, Avg Reward: -75.3301997869, Learning Rate: 0.01, buffer_len: 188692\n",
      "Loss: -5.91770553589\n",
      "[NOR] Episode: 1500, Length: 139, e: 0.18102375, Avg Reward: -69.5409294896, Learning Rate: 0.01, buffer_len: 190363\n",
      "Loss: -9.89337730408\n",
      "[NOR] Episode: 1510, Length: 232, e: 0.180084375, Avg Reward: -109.82200308, Learning Rate: 0.01, buffer_len: 191866\n",
      "Loss: -29.5675640106\n",
      "[NOR] Episode: 1520, Length: 139, e: 0.179030625, Avg Reward: -79.3389417149, Learning Rate: 0.01, buffer_len: 193552\n",
      "Loss: -1.60216593742\n",
      "[NOR] Episode: 1530, Length: 84, e: 0.1782525, Avg Reward: -70.6744077058, Learning Rate: 0.01, buffer_len: 194797\n",
      "Loss: -24.0425205231\n",
      "[NOR] Episode: 1540, Length: 68, e: 0.177574375, Avg Reward: -91.6684037799, Learning Rate: 0.01, buffer_len: 195882\n",
      "Loss: -11.3495025635\n",
      "[NOR] Episode: 1550, Length: 142, e: 0.17658125, Avg Reward: -102.542210749, Learning Rate: 0.01, buffer_len: 197471\n",
      "Loss: -7.95674848557\n",
      "[NOR] Episode: 1560, Length: 108, e: 0.175870625, Avg Reward: -99.8095618904, Learning Rate: 0.01, buffer_len: 198608\n",
      "Loss: -26.5005741119\n",
      "[NOR] Episode: 1570, Length: 78, e: 0.174076875, Avg Reward: -41.5412554534, Learning Rate: 0.01, buffer_len: 201478\n",
      "Loss: -5.13286447525\n",
      "[NOR] Episode: 1580, Length: 918, e: 0.171774375, Avg Reward: -50.7573294311, Learning Rate: 0.01, buffer_len: 205162\n",
      "Loss: -13.6901092529\n",
      "[NOR] Episode: 1590, Length: 164, e: 0.171006875, Avg Reward: -68.0055129807, Learning Rate: 0.01, buffer_len: 206390\n",
      "Loss: -6.54808092117\n",
      "[NOR] Episode: 1600, Length: 102, e: 0.170246875, Avg Reward: -56.1624055219, Learning Rate: 0.01, buffer_len: 207606\n",
      "Loss: -5.59101915359\n",
      "[NOR] Episode: 1610, Length: 130, e: 0.169168125, Avg Reward: -51.5810215321, Learning Rate: 0.01, buffer_len: 209332\n",
      "Loss: -6.70159578323\n",
      "[NOR] Episode: 1620, Length: 138, e: 0.16841125, Avg Reward: -76.3439263372, Learning Rate: 0.01, buffer_len: 210543\n",
      "Loss: -11.3524360657\n",
      "[NOR] Episode: 1630, Length: 104, e: 0.167655625, Avg Reward: -83.1447937789, Learning Rate: 0.01, buffer_len: 211752\n",
      "Loss: -16.2235469818\n",
      "[NOR] Episode: 1640, Length: 74, e: 0.166944375, Avg Reward: -88.71179397, Learning Rate: 0.01, buffer_len: 212890\n",
      "Loss: -5.98379611969\n",
      "[NOR] Episode: 1650, Length: 82, e: 0.16597125, Avg Reward: -68.8655558149, Learning Rate: 0.01, buffer_len: 214447\n",
      "Loss: -6.51380634308\n",
      "[NOR] Episode: 1660, Length: 944, e: 0.164715, Avg Reward: -74.990497156, Learning Rate: 0.01, buffer_len: 216457\n",
      "Loss: -6.3261256218\n",
      "[NOR] Episode: 1670, Length: 145, e: 0.163504375, Avg Reward: -21.0593603289, Learning Rate: 0.01, buffer_len: 218394\n",
      "Loss: -2.0083899498\n",
      "[NOR] Episode: 1680, Length: 182, e: 0.162060625, Avg Reward: -14.6033489394, Learning Rate: 0.01, buffer_len: 220704\n",
      "Loss: -9.91037082672\n",
      "[NOR] Episode: 1690, Length: 141, e: 0.161254375, Avg Reward: -80.536050133, Learning Rate: 0.01, buffer_len: 221994\n",
      "Loss: -4.20206785202\n",
      "[NOR] Episode: 1700, Length: 169, e: 0.159586875, Avg Reward: -16.9536547351, Learning Rate: 0.01, buffer_len: 224662\n",
      "Loss: -11.8717727661\n",
      "[NOR] Episode: 1710, Length: 95, e: 0.158524375, Avg Reward: -70.8344566437, Learning Rate: 0.01, buffer_len: 226362\n",
      "Loss: -10.732881546\n",
      "[NOR] Episode: 1720, Length: 76, e: 0.157595, Avg Reward: -49.5055147554, Learning Rate: 0.01, buffer_len: 227849\n",
      "Loss: -9.07371234894\n",
      "[NOR] Episode: 1730, Length: 76, e: 0.156981875, Avg Reward: -92.824727062, Learning Rate: 0.01, buffer_len: 228830\n",
      "Loss: -16.3903503418\n",
      "[NOR] Episode: 1740, Length: 531, e: 0.1560375, Avg Reward: -75.2724259123, Learning Rate: 0.01, buffer_len: 230341\n",
      "Loss: -10.2121315002\n",
      "[NOR] Episode: 1750, Length: 157, e: 0.15483375, Avg Reward: -38.3632531459, Learning Rate: 0.01, buffer_len: 232267\n",
      "Loss: -14.8794155121\n",
      "[NOR] Episode: 1760, Length: 146, e: 0.15414375, Avg Reward: -87.747679111, Learning Rate: 0.01, buffer_len: 233371\n",
      "Loss: -4.9781332016\n",
      "[NOR] Episode: 1770, Length: 127, e: 0.15227125, Avg Reward: -49.4309973402, Learning Rate: 0.01, buffer_len: 236367\n",
      "Loss: -7.88815498352\n",
      "[NOR] Episode: 1780, Length: 125, e: 0.151508125, Avg Reward: -77.0581731841, Learning Rate: 0.01, buffer_len: 237588\n",
      "Loss: -3.46833252907\n",
      "[NOR] Episode: 1790, Length: 139, e: 0.150465625, Avg Reward: -24.455227435, Learning Rate: 0.01, buffer_len: 239256\n",
      "Loss: -9.55252838135\n",
      "[NOR] Episode: 1800, Length: 114, e: 0.14957125, Avg Reward: -91.4818823773, Learning Rate: 0.01, buffer_len: 240687\n",
      "Loss: -9.86236286163\n",
      "[NOR] Episode: 1810, Length: 78, e: 0.1484775, Avg Reward: -21.1353219725, Learning Rate: 0.01, buffer_len: 242437\n",
      "Loss: -0.254537820816\n",
      "[NOR] Episode: 1820, Length: 84, e: 0.146594375, Avg Reward: -50.8045418866, Learning Rate: 0.01, buffer_len: 245450\n",
      "Loss: 5.29821777344\n",
      "[NOR] Episode: 1830, Length: 483, e: 0.145629375, Avg Reward: -54.5676873747, Learning Rate: 0.01, buffer_len: 246994\n",
      "Loss: -35.0169067383\n",
      "[NOR] Episode: 1840, Length: 122, e: 0.14330125, Avg Reward: -31.2654731824, Learning Rate: 0.01, buffer_len: 250719\n",
      "Loss: -12.5559902191\n",
      "[NOR] Episode: 1850, Length: 134, e: 0.14266, Avg Reward: -69.6903216481, Learning Rate: 0.01, buffer_len: 251745\n",
      "Loss: -9.50789451599\n",
      "[NOR] Episode: 1860, Length: 129, e: 0.141765625, Avg Reward: -52.8595415277, Learning Rate: 0.01, buffer_len: 253176\n",
      "Loss: -18.0990982056\n",
      "[NOR] Episode: 1870, Length: 109, e: 0.14106125, Avg Reward: -94.0456709267, Learning Rate: 0.01, buffer_len: 254303\n",
      "Loss: -12.3940505981\n",
      "[NOR] Episode: 1880, Length: 93, e: 0.13981625, Avg Reward: -57.5662171024, Learning Rate: 0.01, buffer_len: 256295\n",
      "Loss: -3.66228842735\n",
      "[NOR] Episode: 1890, Length: 148, e: 0.138435, Avg Reward: -48.6104624772, Learning Rate: 0.01, buffer_len: 258505\n",
      "Loss: 3.68834877014\n",
      "[NOR] Episode: 1900, Length: 86, e: 0.137765, Avg Reward: -78.8067740906, Learning Rate: 0.01, buffer_len: 259577\n",
      "Loss: 13.9784698486\n",
      "[NOR] Episode: 1910, Length: 84, e: 0.136999375, Avg Reward: -54.9317542021, Learning Rate: 0.01, buffer_len: 260802\n",
      "Loss: -8.1113948822\n",
      "[NOR] Episode: 1920, Length: 134, e: 0.1353475, Avg Reward: -21.6358129373, Learning Rate: 0.01, buffer_len: 263445\n",
      "Loss: -10.8512630463\n",
      "[NOR] Episode: 1930, Length: 141, e: 0.134068125, Avg Reward: -50.2977976093, Learning Rate: 0.01, buffer_len: 265492\n",
      "Loss: -6.81775569916\n",
      "[NOR] Episode: 1940, Length: 100, e: 0.132394375, Avg Reward: 8.06490778395, Learning Rate: 0.01, buffer_len: 268170\n",
      "Loss: -4.02621603012\n",
      "[NOR] Episode: 1950, Length: 1000, e: 0.1311075, Avg Reward: -51.079182013, Learning Rate: 0.01, buffer_len: 270229\n",
      "Loss: -3.68733882904\n",
      "[NOR] Episode: 1960, Length: 140, e: 0.129886875, Avg Reward: -43.7787168596, Learning Rate: 0.01, buffer_len: 272182\n",
      "Loss: -9.56925964355\n",
      "[NOR] Episode: 1970, Length: 135, e: 0.127420625, Avg Reward: -54.1236183985, Learning Rate: 0.01, buffer_len: 276128\n",
      "Loss: -14.8645973206\n",
      "[NOR] Episode: 1980, Length: 141, e: 0.126634375, Avg Reward: -73.1404724173, Learning Rate: 0.01, buffer_len: 277386\n",
      "Loss: -5.36996030807\n",
      "[NOR] Episode: 1990, Length: 1000, e: 0.124954375, Avg Reward: -39.1641320762, Learning Rate: 0.01, buffer_len: 280074\n",
      "Loss: -18.048166275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:24:44,949] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander-actor-critic/tmp/monitor36/openaigym.video.1.15919.video002000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 2000, Length: 108, e: 0.123595, Avg Reward: -64.189974407, Learning Rate: 0.01, buffer_len: 282249\n",
      "Loss: -51.3012275696\n",
      "[NOR] Episode: 2010, Length: 476, e: 0.12242875, Avg Reward: -48.4146722731, Learning Rate: 0.01, buffer_len: 284115\n",
      "Loss: -5.73314094543\n",
      "[NOR] Episode: 2020, Length: 1000, e: 0.120453125, Avg Reward: -39.1983509844, Learning Rate: 0.01, buffer_len: 287276\n",
      "Loss: -2.28845453262\n",
      "[NOR] Episode: 2030, Length: 1000, e: 0.118, Avg Reward: -36.2720403629, Learning Rate: 0.01, buffer_len: 291201\n",
      "Loss: -7.12934017181\n",
      "[NOR] Episode: 2040, Length: 786, e: 0.116169375, Avg Reward: -89.0619551062, Learning Rate: 0.01, buffer_len: 294130\n",
      "Loss: -38.3872871399\n",
      "[NOR] Episode: 2050, Length: 146, e: 0.114808125, Avg Reward: -47.6634918727, Learning Rate: 0.01, buffer_len: 296308\n",
      "Loss: -16.9381504059\n",
      "[NOR] Episode: 2060, Length: 153, e: 0.1127075, Avg Reward: -52.1363387606, Learning Rate: 0.01, buffer_len: 299669\n",
      "Loss: -11.5435142517\n",
      "[NOR] Episode: 2070, Length: 88, e: 0.111368125, Avg Reward: -59.4278380759, Learning Rate: 0.01, buffer_len: 301812\n",
      "Loss: -0.87250316143\n",
      "[NOR] Episode: 2080, Length: 142, e: 0.1104625, Avg Reward: -58.0289058925, Learning Rate: 0.01, buffer_len: 303261\n",
      "Loss: -9.41714382172\n",
      "[NOR] Episode: 2090, Length: 96, e: 0.108981875, Avg Reward: -25.4110888333, Learning Rate: 0.01, buffer_len: 305630\n",
      "Loss: -11.0198154449\n",
      "[NOR] Episode: 2100, Length: 137, e: 0.107650625, Avg Reward: -53.5331755166, Learning Rate: 0.01, buffer_len: 307760\n",
      "Loss: -15.0393466949\n",
      "[NOR] Episode: 2110, Length: 479, e: 0.105720625, Avg Reward: -76.039115069, Learning Rate: 0.01, buffer_len: 310848\n",
      "Loss: -6.47564554214\n",
      "[NOR] Episode: 2120, Length: 152, e: 0.102915, Avg Reward: -31.248652911, Learning Rate: 0.01, buffer_len: 315337\n",
      "Loss: -2.09065580368\n",
      "[NOR] Episode: 2130, Length: 120, e: 0.100546875, Avg Reward: -59.3028149378, Learning Rate: 0.01, buffer_len: 319126\n",
      "Loss: -13.4511175156\n",
      "[NOR] Episode: 2140, Length: 762, e: 0.0982875, Avg Reward: -21.1106699898, Learning Rate: 0.01, buffer_len: 322741\n",
      "Loss: -5.99813175201\n",
      "[NOR] Episode: 2150, Length: 115, e: 0.096625, Avg Reward: -35.6246671535, Learning Rate: 0.01, buffer_len: 325401\n",
      "Loss: -43.3696517944\n",
      "[NOR] Episode: 2160, Length: 129, e: 0.09348625, Avg Reward: -4.46602777118, Learning Rate: 0.01, buffer_len: 330423\n",
      "Loss: -4.09032154083\n",
      "[NOR] Episode: 2170, Length: 134, e: 0.092014375, Avg Reward: -67.0495752802, Learning Rate: 0.01, buffer_len: 332778\n",
      "Loss: 4.56723451614\n",
      "[NOR] Episode: 2180, Length: 147, e: 0.0903075, Avg Reward: -33.651052509, Learning Rate: 0.01, buffer_len: 335509\n",
      "Loss: -7.14210033417\n",
      "[NOR] Episode: 2190, Length: 305, e: 0.089335, Avg Reward: -37.4775199183, Learning Rate: 0.01, buffer_len: 337065\n",
      "Loss: -8.85657501221\n",
      "[NOR] Episode: 2200, Length: 223, e: 0.087934375, Avg Reward: -37.2351709229, Learning Rate: 0.01, buffer_len: 339306\n",
      "Loss: -5.26911354065\n",
      "[NOR] Episode: 2210, Length: 143, e: 0.086441875, Avg Reward: -41.1143678349, Learning Rate: 0.01, buffer_len: 341694\n",
      "Loss: -74.7021942139\n",
      "[NOR] Episode: 2220, Length: 413, e: 0.08540625, Avg Reward: -46.2489347135, Learning Rate: 0.01, buffer_len: 343351\n",
      "Loss: -14.1112527847\n",
      "[NOR] Episode: 2230, Length: 123, e: 0.084605625, Avg Reward: -73.2090966502, Learning Rate: 0.01, buffer_len: 344632\n",
      "Loss: 2.64290904999\n",
      "[NOR] Episode: 2240, Length: 74, e: 0.083828125, Avg Reward: -75.6248024036, Learning Rate: 0.01, buffer_len: 345876\n",
      "Loss: -10.8723602295\n",
      "[NOR] Episode: 2250, Length: 119, e: 0.082716875, Avg Reward: -32.7103046115, Learning Rate: 0.01, buffer_len: 347654\n",
      "Loss: -8.1580324173\n",
      "[NOR] Episode: 2260, Length: 107, e: 0.08132, Avg Reward: -29.2477335086, Learning Rate: 0.01, buffer_len: 349889\n",
      "Loss: 3.46233463287\n",
      "[NOR] Episode: 2270, Length: 869, e: 0.079505, Avg Reward: -0.990997980714, Learning Rate: 0.01, buffer_len: 352793\n",
      "Loss: -7.64655447006\n",
      "[NOR] Episode: 2280, Length: 130, e: 0.07729625, Avg Reward: -17.2273904737, Learning Rate: 0.01, buffer_len: 356327\n",
      "Loss: -8.32447147369\n",
      "[NOR] Episode: 2290, Length: 150, e: 0.07597625, Avg Reward: -35.3360253557, Learning Rate: 0.01, buffer_len: 358439\n",
      "Loss: -9.65323829651\n",
      "[NOR] Episode: 2300, Length: 214, e: 0.074605625, Avg Reward: -15.7231301884, Learning Rate: 0.01, buffer_len: 360632\n",
      "Loss: -14.3162527084\n",
      "[NOR] Episode: 2310, Length: 72, e: 0.0731125, Avg Reward: -36.5804329498, Learning Rate: 0.01, buffer_len: 363021\n",
      "Loss: -8.26006984711\n",
      "[NOR] Episode: 2320, Length: 174, e: 0.0705125, Avg Reward: -42.9109788143, Learning Rate: 0.01, buffer_len: 367181\n",
      "Loss: -5.21635150909\n",
      "[NOR] Episode: 2330, Length: 78, e: 0.0691025, Avg Reward: 29.7049366821, Learning Rate: 0.01, buffer_len: 369437\n",
      "Loss: -14.5707187653\n",
      "[NOR] Episode: 2340, Length: 230, e: 0.06823, Avg Reward: -9.01741875239, Learning Rate: 0.01, buffer_len: 370833\n",
      "Loss: -25.0010852814\n",
      "[NOR] Episode: 2350, Length: 229, e: 0.066156875, Avg Reward: 57.5640181549, Learning Rate: 0.01, buffer_len: 374150\n",
      "Loss: -3.62124609947\n",
      "[NOR] Episode: 2360, Length: 307, e: 0.06490125, Avg Reward: 36.4569528425, Learning Rate: 0.01, buffer_len: 376159\n",
      "Loss: -1.28088378906\n",
      "[NOR] Episode: 2370, Length: 226, e: 0.06367125, Avg Reward: -16.0477722982, Learning Rate: 0.01, buffer_len: 378127\n",
      "Loss: -2.44334053993\n",
      "[NOR] Episode: 2380, Length: 467, e: 0.061626875, Avg Reward: 47.922355154, Learning Rate: 0.01, buffer_len: 381398\n",
      "Loss: -2.46221160889\n",
      "[NOR] Episode: 2390, Length: 335, e: 0.058581875, Avg Reward: -22.5402007282, Learning Rate: 0.01, buffer_len: 386270\n",
      "Loss: -29.5154838562\n",
      "[NOR] Episode: 2400, Length: 330, e: 0.057106875, Avg Reward: 3.13625709255, Learning Rate: 0.01, buffer_len: 388630\n",
      "Loss: -5.14148807526\n",
      "[MAX] Episode: 2402, Length: 242, Reward: 239.876848014, buffer_len: 389171\n",
      "[NOR] Episode: 2410, Length: 441, e: 0.0551025, Avg Reward: 118.030515391, Learning Rate: 0.01, buffer_len: 391837\n",
      "Loss: -7.16158246994\n",
      "[NOR] Episode: 2420, Length: 129, e: 0.053540625, Avg Reward: -13.7639203857, Learning Rate: 0.01, buffer_len: 394336\n",
      "Loss: -11.364408493\n",
      "[NOR] Episode: 2430, Length: 126, e: 0.0511625, Avg Reward: 87.2272108149, Learning Rate: 0.01, buffer_len: 398141\n",
      "Loss: -19.6665554047\n",
      "[NOR] Episode: 2440, Length: 268, e: 0.05, Avg Reward: 25.9405921489, Learning Rate: 0.01, buffer_len: 400656\n",
      "Loss: -4.76928949356\n",
      "[NOR] Episode: 2450, Length: 342, e: 0.05, Avg Reward: 115.221784054, Learning Rate: 0.01, buffer_len: 405037\n",
      "Loss: -3.58107042313\n",
      "[NOR] Episode: 2460, Length: 145, e: 0.05, Avg Reward: -61.2178897404, Learning Rate: 0.01, buffer_len: 406945\n",
      "Loss: -10.8389225006\n",
      "[NOR] Episode: 2470, Length: 156, e: 0.05, Avg Reward: 67.9805852728, Learning Rate: 0.01, buffer_len: 409872\n",
      "Loss: 46.418006897\n",
      "[NOR] Episode: 2480, Length: 319, e: 0.05, Avg Reward: 2.54952527158, Learning Rate: 0.01, buffer_len: 412672\n",
      "Loss: -15.459985733\n",
      "[NOR] Episode: 2490, Length: 144, e: 0.05, Avg Reward: 29.9689692485, Learning Rate: 0.01, buffer_len: 414678\n",
      "Loss: -12.916009903\n",
      "[NOR] Episode: 2500, Length: 461, e: 0.05, Avg Reward: 46.6824174518, Learning Rate: 0.01, buffer_len: 417900\n",
      "Loss: 7.19887161255\n",
      "[MAX] Episode: 2504, Length: 411, Reward: 251.255719031, buffer_len: 419171\n",
      "[MAX] Episode: 2507, Length: 369, Reward: 254.635426697, buffer_len: 420087\n",
      "[NOR] Episode: 2510, Length: 341, e: 0.05, Avg Reward: 98.9472176071, Learning Rate: 0.01, buffer_len: 421221\n",
      "Loss: 4.46235656738\n",
      "[NOR] Episode: 2520, Length: 586, e: 0.05, Avg Reward: 38.3474664203, Learning Rate: 0.01, buffer_len: 424082\n",
      "Loss: -9.01809692383\n",
      "[NOR] Episode: 2530, Length: 124, e: 0.05, Avg Reward: -27.5083468809, Learning Rate: 0.01, buffer_len: 426047\n",
      "Loss: -8.78453063965\n",
      "[NOR] Episode: 2540, Length: 238, e: 0.05, Avg Reward: -36.07667167, Learning Rate: 0.01, buffer_len: 428372\n",
      "Loss: -4.63672447205\n",
      "[NOR] Episode: 2550, Length: 112, e: 0.05, Avg Reward: -68.0666556706, Learning Rate: 0.01, buffer_len: 430117\n",
      "Loss: 1.03809165955\n",
      "[NOR] Episode: 2560, Length: 192, e: 0.05, Avg Reward: -37.6414486321, Learning Rate: 0.01, buffer_len: 433161\n",
      "Loss: 2.52759170532\n",
      "[NOR] Episode: 2570, Length: 162, e: 0.05, Avg Reward: 8.48691019287, Learning Rate: 0.01, buffer_len: 436097\n",
      "Loss: 5.82654428482\n",
      "[NOR] Episode: 2580, Length: 113, e: 0.05, Avg Reward: -23.9536449178, Learning Rate: 0.01, buffer_len: 438134\n",
      "Loss: -3.99580693245\n",
      "[NOR] Episode: 2590, Length: 509, e: 0.05, Avg Reward: 12.0496801223, Learning Rate: 0.01, buffer_len: 441404\n",
      "Loss: -10.5794754028\n",
      "[NOR] Episode: 2600, Length: 215, e: 0.05, Avg Reward: -32.9894887351, Learning Rate: 0.01, buffer_len: 444063\n",
      "Loss: -23.4556999207\n",
      "[NOR] Episode: 2610, Length: 609, e: 0.05, Avg Reward: 64.0846384697, Learning Rate: 0.01, buffer_len: 448314\n",
      "Loss: -0.775989055634\n",
      "[NOR] Episode: 2620, Length: 502, e: 0.05, Avg Reward: -34.1603333627, Learning Rate: 0.01, buffer_len: 450611\n",
      "Loss: -5.6984872818\n",
      "[NOR] Episode: 2630, Length: 186, e: 0.05, Avg Reward: -54.3276094171, Learning Rate: 0.01, buffer_len: 452634\n",
      "Loss: -6.46573352814\n",
      "[NOR] Episode: 2640, Length: 99, e: 0.05, Avg Reward: 7.70091639236, Learning Rate: 0.01, buffer_len: 454465\n",
      "Loss: -12.5487480164\n",
      "[NOR] Episode: 2650, Length: 569, e: 0.05, Avg Reward: -13.228273651, Learning Rate: 0.01, buffer_len: 457344\n",
      "Loss: -6.30933666229\n",
      "[NOR] Episode: 2660, Length: 689, e: 0.05, Avg Reward: 17.2920609439, Learning Rate: 0.01, buffer_len: 459926\n",
      "Loss: 1.0188999176\n",
      "[NOR] Episode: 2670, Length: 138, e: 0.05, Avg Reward: 87.0550003697, Learning Rate: 0.01, buffer_len: 463263\n",
      "Loss: -10.8092451096\n",
      "[NOR] Episode: 2680, Length: 192, e: 0.05, Avg Reward: 100.538775568, Learning Rate: 0.01, buffer_len: 466118\n",
      "Loss: -9.4221534729\n",
      "[MAX] Episode: 2683, Length: 305, Reward: 256.51626723, buffer_len: 467256\n",
      "[NOR] Episode: 2690, Length: 460, e: 0.05, Avg Reward: 70.6202279513, Learning Rate: 0.01, buffer_len: 469963\n",
      "Loss: -6.81408882141\n",
      "[NOR] Episode: 2700, Length: 175, e: 0.05, Avg Reward: -49.0515764134, Learning Rate: 0.01, buffer_len: 472014\n",
      "Loss: -2.26607632637\n",
      "[NOR] Episode: 2710, Length: 293, e: 0.05, Avg Reward: -104.540917876, Learning Rate: 0.01, buffer_len: 474495\n",
      "Loss: -4.21499729156\n",
      "[NOR] Episode: 2720, Length: 491, e: 0.05, Avg Reward: 54.819083364, Learning Rate: 0.01, buffer_len: 477483\n",
      "Loss: 6.65303945541\n",
      "[NOR] Episode: 2730, Length: 378, e: 0.05, Avg Reward: -46.0333649895, Learning Rate: 0.01, buffer_len: 480936\n",
      "Loss: -6.47592115402\n",
      "[NOR] Episode: 2740, Length: 539, e: 0.05, Avg Reward: 66.9275964331, Learning Rate: 0.01, buffer_len: 485003\n",
      "Loss: -3.50819396973\n",
      "[NOR] Episode: 2750, Length: 163, e: 0.05, Avg Reward: 24.1902474435, Learning Rate: 0.01, buffer_len: 487661\n",
      "Loss: 0.982661247253\n",
      "[NOR] Episode: 2760, Length: 238, e: 0.05, Avg Reward: 150.139978238, Learning Rate: 0.01, buffer_len: 492740\n",
      "Loss: 0.0990097522736\n",
      "[NOR] Episode: 2770, Length: 301, e: 0.05, Avg Reward: -15.9026440008, Learning Rate: 0.01, buffer_len: 497777\n",
      "Loss: -9.11893177032\n",
      "[NOR] Episode: 2780, Length: 193, e: 0.05, Avg Reward: -52.4139442754, Learning Rate: 0.01, buffer_len: 499734\n",
      "Loss: -3.87567853928\n",
      "[NOR] Episode: 2790, Length: 166, e: 0.05, Avg Reward: -4.62478254737, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.5493221283\n",
      "[NOR] Episode: 2800, Length: 593, e: 0.05, Avg Reward: 123.655266569, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.52041924\n",
      "[NOR] Episode: 2810, Length: 186, e: 0.05, Avg Reward: -79.0263843936, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.7187786102\n",
      "[NOR] Episode: 2820, Length: 509, e: 0.05, Avg Reward: 47.8958528595, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.95339250565\n",
      "[NOR] Episode: 2830, Length: 316, e: 0.05, Avg Reward: 41.5676601086, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.98565006256\n",
      "[NOR] Episode: 2840, Length: 499, e: 0.05, Avg Reward: 104.985477739, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.12060689926\n",
      "[NOR] Episode: 2850, Length: 376, e: 0.05, Avg Reward: 58.0576250898, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.45620155334\n",
      "[NOR] Episode: 2860, Length: 469, e: 0.05, Avg Reward: 27.2301007938, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.11474943161\n",
      "[NOR] Episode: 2870, Length: 644, e: 0.05, Avg Reward: 110.563954232, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.02215194702\n",
      "[NOR] Episode: 2880, Length: 1000, e: 0.05, Avg Reward: 88.9944369929, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.68705868721\n",
      "[NOR] Episode: 2890, Length: 842, e: 0.05, Avg Reward: 95.3719278927, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.90936470032\n",
      "[NOR] Episode: 2900, Length: 488, e: 0.05, Avg Reward: 73.5271491237, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.97868919373\n",
      "[NOR] Episode: 2910, Length: 527, e: 0.05, Avg Reward: 73.7667003812, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.20802927017\n",
      "[NOR] Episode: 2920, Length: 1000, e: 0.05, Avg Reward: 63.071448005, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.83898353577\n",
      "[NOR] Episode: 2930, Length: 1000, e: 0.05, Avg Reward: 6.52207594653, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.11828279495\n",
      "[NOR] Episode: 2940, Length: 798, e: 0.05, Avg Reward: 65.1337973441, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.18551588058\n",
      "[NOR] Episode: 2950, Length: 1000, e: 0.05, Avg Reward: 21.9293395546, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.4593334198\n",
      "[NOR] Episode: 2960, Length: 956, e: 0.05, Avg Reward: 11.189560962, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.69215583801\n"
     ]
    }
   ],
   "source": [
    "k = 40000.\n",
    "model.fit(\n",
    "    env, print_step=10, \n",
    "    episodes=int(1e5), max_episode_length=10000, batch_size=32,\n",
    "    learning_rate = 0.01, # lambda t: 0.05 * k / (k + t)\n",
    "    e = interp1d([0, 400000], [0.3, 0.05], fill_value=0.05, bounds_error=False),\n",
    "    update_target = 1,\n",
    "    keep_prob = 0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 20:55:43,848] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Discrete(4)\n",
      "150.723606528\n",
      "209.676953188\n",
      "161.72128111\n",
      "81.9737582643\n",
      "171.20761489\n",
      "73.1185110583\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <type 'exceptions.TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-bf0154df444e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_polygon\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLunarLanderContinuous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLunarLander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/window/xlib/__init__.pyc\u001b[0m in \u001b[0;36mdispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# Check for the events specific to this window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         while xlib.XCheckWindowEvent(_x_display, _window,\n\u001b[0;32m--> 853\u001b[0;31m                                      0x1ffffff, byref(e)):\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;31m# Key events are filtered by the xlib window event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# handler so they get a shot at the prefiltered event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: <type 'exceptions.TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "print(env.action_space)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path = os.getcwd() + \"/actor-critic.model\"\n",
    "logs_path = \"logs/run0\"\n",
    "\n",
    "\n",
    "model_run = LanderAC(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path,\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total = 0.\n",
    "    ep = 0\n",
    "    while not done and ep < 1200:\n",
    "        ep += 1\n",
    "        a = model_run.choose_action(s, 1.0)\n",
    "        s, r, done, info = env.step(a)\n",
    "        total += r\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "    print(total)\n",
    "env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# gym.upload(\"tmp/monitor{}\".format(run), api_key='sk_WASyK12rQxais3gwyG4Vg', ignore_open_monitors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gym.upload?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coconut (Python 2)",
   "language": "coconut",
   "name": "coconut2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3.6
   },
   "file_extension": ".coco",
   "mimetype": "text/x-python3",
   "name": "coconut",
   "pygments_lexer": "coconut"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
