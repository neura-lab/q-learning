{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tfinterface.model_base import ModelBase\n",
    "from tfinterface.reinforcement import ExperienceReplay\n",
    "from tfinterface.utils import select_columns, soft_if, get_run\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from tfinterface.reinforcement import ExpandedStateEnv\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "name = \"actor-critic-reward-task\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Inputs(object):\n",
    "    def __init__(self, n_states, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.episode_length = tf.placeholder(tf.int64, [], name='episode_length')\n",
    "\n",
    "            self.s = tf.placeholder(tf.float32, [None, n_states], name='s')\n",
    "            self.a = tf.placeholder(tf.int32, [None], name='a')\n",
    "            self.r = tf.placeholder(tf.float32, [None], name='r')\n",
    "            self.v1 = tf.placeholder(tf.float32, [None], name='V1')\n",
    "            self.done = tf.placeholder(tf.float32, [None], name='done')\n",
    "            \n",
    "            self.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            self.keep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\n",
    "            self.training = tf.placeholder(tf.bool, [], name='training')\n",
    "            \n",
    "            self.pi = tf.placeholder(tf.float32, [], name='pi')\n",
    "            \n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, base_model, inputs, n_actions, n_states, y, reward_loss_proportion, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            \n",
    "            ops = dict(\n",
    "                trainable=True,\n",
    "                kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01)\n",
    "            )\n",
    "\n",
    "            net = inputs.s\n",
    "\n",
    "            net = tf.layers.dense(net, 64, activation=tf.nn.relu, name=\"relu_layer\", **ops)        \n",
    "            \n",
    "            self.V = tf.layers.dense(net, n_actions, name='V', **ops)[:, 0]\n",
    "            \n",
    "            r_net = net\n",
    "#             r_net = tf.layers.dense(r_net, 32, name=\"r_relu_layer\", activation=tf.nn.relu, **ops)\n",
    "            r = tf.layers.dense(r_net, 1, name='r', **ops)[:, 0]\n",
    "            \n",
    "            \n",
    "            self.r_loss = r - inputs.r |> tf.nn.l2_loss |> tf.reduce_mean\n",
    "\n",
    "            self.target = soft_if(inputs.done, inputs.r,  inputs.r + y * inputs.v1)\n",
    "\n",
    "            self.error = self.target - self.V\n",
    "            self.loss = Pipe(self.error, tf.nn.l2_loss, tf.reduce_mean) + reward_loss_proportion * self.r_loss\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "            self.update = tf.train.AdamOptimizer(inputs.learning_rate).minimize(self.loss, var_list=self.variables)\n",
    "\n",
    "            avg_error, std_error = tf.nn.moments(self.error, [0])\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('avg_target', tf.reduce_mean(self.target)),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.scalar('avg_error', avg_error),\n",
    "                tf.summary.scalar('std_error', std_error),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n",
    "class Actor(object):\n",
    "    def __init__(self, base_model, inputs, target_critic, n_actions, n_states, y, reward_loss_proportion, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            ops = dict(\n",
    "                trainable=True,\n",
    "                kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01)\n",
    "            )\n",
    "\n",
    "            net = inputs.s\n",
    "\n",
    "            net = tf.layers.dense(net, 128, activation=tf.nn.relu, name=\"relu_layer\", use_bias=True, **ops)\n",
    "            net = tf.nn.dropout(net, inputs.keep_prob)\n",
    "\n",
    "            self.P = tf.layers.dense(net, n_actions, activation=tf.nn.softmax, name='P', use_bias=False, **ops)\n",
    "            self.Pa = select_columns(self.P, inputs.a)\n",
    "            \n",
    "            r_net = net\n",
    "#             r_net = tf.layers.dense(r_net, 32, name=\"r_relu_layer\", activation=tf.nn.relu, **ops)\n",
    "            r = tf.layers.dense(r_net, 1, name='r', **ops)[:, 0]\n",
    "            \n",
    "            \n",
    "            self.r_loss = r - inputs.r |> tf.nn.l2_loss |> tf.reduce_mean\n",
    "            self.action_loss = - tf.log(tf.clip_by_value(self.Pa, 1e-3, 1.0)) * target_critic.error\n",
    "\n",
    "            self.loss = self.action_loss + reward_loss_proportion * self.r_loss\n",
    "            self.loss = tf.reduce_mean(self.loss)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "            self.update = tf.train.AdamOptimizer(inputs.learning_rate).minimize(self.loss, var_list=self.variables)\n",
    "\n",
    "            \n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LunarLander(ModelBase):\n",
    "    \n",
    "    def define_model(self, n_actions, n_states, reward_loss_proportion=0.1, y=0.98, buffer_length=50000, pi=0.1):\n",
    "        self.global_max = float('-inf')\n",
    "\n",
    "        self.replay_buffer = ExperienceReplay(max_length=buffer_length)\n",
    "\n",
    "\n",
    "        with self.graph.as_default(), tf.device(\"cpu:0\"):\n",
    "\n",
    "            self.inputs = Inputs(n_states, \"inputs\")\n",
    "\n",
    "            self.critic = Critic(self, self.inputs, n_actions, n_states, y, reward_loss_proportion, \"critic\")\n",
    "            self.target_critic = Critic(self, self.inputs, n_actions, n_states, y, reward_loss_proportion, \"target_critic\")\n",
    "            self.actor = Actor(self, self.inputs, self.target_critic, n_actions, n_states, y, reward_loss_proportion, \"actor\")\n",
    "\n",
    "            self.update = tf.group(self.critic.update, self.actor.update)\n",
    "\n",
    "            self.episode_length_summary = tf.summary.scalar('episode_length', self.inputs.episode_length)\n",
    "\n",
    "            self.summaries = tf.summary.merge([self.actor.summaries, self.critic.summaries, self.target_critic.summaries])\n",
    "\n",
    "            self.update_target = tf.group(*[\n",
    "                t.assign_add(pi * (a - t)) for t, a in zip(self.target_critic.variables, self.critic.variables)\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    def predict_feed(self, S):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.keep_prob: 1.0,\n",
    "            self.inputs.training: False\n",
    "        }\n",
    "    \n",
    "    def predict(self, state, e = 0.0):\n",
    "        predict_feed = self.predict_feed([state])\n",
    "        actions = self.sess.run(self.actor.P, feed_dict=predict_feed)\n",
    "        actions = actions[0]\n",
    "        n = len(actions)\n",
    "\n",
    "        if random.random() < e:\n",
    "            return random.randint(0, n-1)\n",
    "        else:\n",
    "            return np.random.choice(n, p=actions)\n",
    "    \n",
    "    def fit_feed(self, S, A, R, V1, Done, learning_rate, keep_prob):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.a: A,\n",
    "            self.inputs.r: R,\n",
    "            self.inputs.v1: V1,\n",
    "            self.inputs.done: Done,\n",
    "            self.inputs.learning_rate: learning_rate,\n",
    "            self.inputs.keep_prob: keep_prob,\n",
    "            self.inputs.training: True\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def fit(self, env, keep_prob=0.5, e=0.01, learning_rate=0.01, print_step=10, \n",
    "            update_target_step = 32, episodes=100000, max_episode_length=float('inf'), batch_size=32):\n",
    "        \n",
    "        r_total = 0.\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            ep_step = 0\n",
    "            s = env.reset()\n",
    "            episode_length = 0\n",
    "            ep_reward = 0.\n",
    "            \n",
    "            while not done and ep_step <= max_episode_length:\n",
    "                self.global_step += 1\n",
    "                episode_length += 1\n",
    "                ep_step += 1\n",
    "                \n",
    "                \n",
    "                _learning_rate = learning_rate(self.global_step) if hasattr(learning_rate, '__call__') else learning_rate\n",
    "                _e = e(self.global_step) if hasattr(e, '__call__') else e\n",
    "                \n",
    "                \n",
    "                a = self.predict(s, e = _e)\n",
    "                s1, r, done, info = env.step(a)\n",
    "                r_total += r\n",
    "                ep_reward += r\n",
    "                \n",
    "                \n",
    "                self.replay_buffer.append((s, a, r, s1, float(done)))\n",
    "                \n",
    "                \n",
    "                S, A, R, S1, Done = self.replay_buffer.random_batch(batch_size).unzip()\n",
    "                predict_feed = self.predict_feed(S1)\n",
    "                V1 = self.sess.run(self.target_critic.V, feed_dict=predict_feed)\n",
    "\n",
    "                \n",
    "                fit_feed = self.fit_feed(S, A, R, V1, Done, _learning_rate, keep_prob)\n",
    "                _, summaries = self.sess.run([self.update, self.summaries], feed_dict=fit_feed)\n",
    "                self.writer.add_summary(summaries, self.global_step)\n",
    "                \n",
    "                \n",
    "                if self.global_step % update_target_step == 0:\n",
    "                    self.sess.run(self.update_target)\n",
    "                \n",
    "                \n",
    "                s = s1\n",
    "                \n",
    "            \n",
    "            episode_length_summary = self.sess.run(self.episode_length_summary,\n",
    "                                                   feed_dict={self.inputs.episode_length: episode_length})\n",
    "            self.writer.add_summary(episode_length_summary, self.global_step)\n",
    "\n",
    "\n",
    "            if ep_reward >= self.global_max:\n",
    "                print(\"[MAX] Episode: {}, Length: {}, Reward: {}, buffer_len: {}\".format(episode, episode_length, ep_reward, len(self.replay_buffer)))\n",
    "                self.save(model_path = self.model_path + \".{score}\".format(score = ep_reward))\n",
    "                self.global_max = ep_reward\n",
    "\n",
    "\n",
    "            if episode % print_step == 0 and episode > 0:\n",
    "                avg_r = r_total / print_step\n",
    "                actor_loss = self.sess.run(self.actor.loss, feed_dict=fit_feed)\n",
    "                print(\"[NOR] Episode: {}, Length: {}, Avg Reward: {}, e: {}, Learning Rate: {}, buffer_len: {}\".format(episode, episode_length, avg_r, _e, _learning_rate, len(self.replay_buffer)))\n",
    "                print(\"Loss: {}\".format(actor_loss))\n",
    "                self.save()\n",
    "                r_total = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:16:54,713] Making new env: LunarLander-v2\n",
      "[2017-03-20 01:16:54,715] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-20 01:16:54,716] Creating monitor directory monitor/21\n"
     ]
    }
   ],
   "source": [
    "run = get_run()\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = wrappers.Monitor(env, \"monitor/{run}\".format(run = run))\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/models/{run}\".format(path = os.getcwd(), run = run)\n",
    "logs_path = \"{path}/logs/{run}\".format(path = os.getcwd(), run = run)\n",
    "\n",
    "\n",
    "model = LunarLander(\n",
    "    n_actions, n_states, y=0.9999, \n",
    "    buffer_length=500000,\n",
    "    model_path = model_path,\n",
    "    logs_path = logs_path,\n",
    "    restore = False,\n",
    "    pi = 0.005,\n",
    "    reward_loss_proportion = 0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:16:55,743] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video000000.mp4\n",
      "[2017-03-20 01:16:57,072] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 0, Length: 84, Reward: -465.086305932, buffer_len: 84\n",
      "[MAX] Episode: 2, Length: 71, Reward: -381.337579648, buffer_len: 227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:17:00,102] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 10, Length: 71, Avg Reward: -568.647667717, e: 0.398990833333, Learning Rate: 0.01, buffer_len: 866\n",
      "Loss: 15.0958900452\n",
      "[MAX] Episode: 16, Length: 62, Reward: -364.095665353, buffer_len: 1322\n",
      "[MAX] Episode: 18, Length: 56, Reward: -324.584533853, buffer_len: 1464\n",
      "[NOR] Episode: 20, Length: 53, Avg Reward: -487.224757581, e: 0.398164833333, Learning Rate: 0.01, buffer_len: 1574\n",
      "Loss: 232.254516602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:17:07,243] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 30, Length: 82, Avg Reward: -520.282610233, e: 0.397362166667, Learning Rate: 0.01, buffer_len: 2262\n",
      "Loss: 78.5168762207\n",
      "[NOR] Episode: 40, Length: 58, Avg Reward: -461.892474617, e: 0.396550166667, Learning Rate: 0.01, buffer_len: 2958\n",
      "Loss: 38.4052963257\n",
      "[MAX] Episode: 49, Length: 57, Reward: -232.955786218, buffer_len: 3585\n",
      "[NOR] Episode: 50, Length: 95, Avg Reward: -485.125928511, e: 0.395707833333, Learning Rate: 0.01, buffer_len: 3680\n",
      "Loss: 108.508590698\n",
      "[NOR] Episode: 60, Length: 63, Avg Reward: -526.870545775, e: 0.394821166667, Learning Rate: 0.01, buffer_len: 4440\n",
      "Loss: 113.263641357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:17:16,452] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 70, Length: 92, Avg Reward: -549.157585035, e: 0.3939415, Learning Rate: 0.01, buffer_len: 5194\n",
      "Loss: 3.13351631165\n",
      "[NOR] Episode: 80, Length: 71, Avg Reward: -535.567064727, e: 0.393072333333, Learning Rate: 0.01, buffer_len: 5939\n",
      "Loss: -28.2442169189\n",
      "[NOR] Episode: 90, Length: 92, Avg Reward: -505.477107744, e: 0.391618666667, Learning Rate: 0.01, buffer_len: 7185\n",
      "Loss: 32.0144271851\n",
      "[NOR] Episode: 100, Length: 57, Avg Reward: -495.664452294, e: 0.390522, Learning Rate: 0.01, buffer_len: 8125\n",
      "Loss: -32.346950531\n",
      "[NOR] Episode: 110, Length: 81, Avg Reward: -563.22241162, e: 0.389661, Learning Rate: 0.01, buffer_len: 8863\n",
      "Loss: -36.9103050232\n",
      "[NOR] Episode: 120, Length: 76, Avg Reward: -518.033899858, e: 0.388739333333, Learning Rate: 0.01, buffer_len: 9653\n",
      "Loss: -30.2687244415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:17:34,830] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 130, Length: 82, Reward: -102.677411209, buffer_len: 10481\n",
      "[NOR] Episode: 130, Length: 82, Avg Reward: -485.715502477, e: 0.387773333333, Learning Rate: 0.01, buffer_len: 10481\n",
      "Loss: -26.5765953064\n",
      "[NOR] Episode: 140, Length: 82, Avg Reward: -558.200017038, e: 0.386771166667, Learning Rate: 0.01, buffer_len: 11340\n",
      "Loss: 349.133239746\n",
      "[NOR] Episode: 150, Length: 83, Avg Reward: -345.684749484, e: 0.385841333333, Learning Rate: 0.01, buffer_len: 12137\n",
      "Loss: 183.941894531\n",
      "[MAX] Episode: 159, Length: 65, Reward: -34.9710501268, buffer_len: 12869\n",
      "[NOR] Episode: 160, Length: 92, Avg Reward: -266.988489895, e: 0.38488, Learning Rate: 0.01, buffer_len: 12961\n",
      "Loss: -16.1831111908\n",
      "[NOR] Episode: 170, Length: 131, Avg Reward: -344.447739031, e: 0.383767, Learning Rate: 0.01, buffer_len: 13915\n",
      "Loss: -30.5140113831\n",
      "[NOR] Episode: 180, Length: 112, Avg Reward: -394.277760334, e: 0.382616666667, Learning Rate: 0.01, buffer_len: 14901\n",
      "Loss: -25.5084228516\n",
      "[NOR] Episode: 190, Length: 78, Avg Reward: -402.320445061, e: 0.381420833333, Learning Rate: 0.01, buffer_len: 15926\n",
      "Loss: 121.075973511\n",
      "[NOR] Episode: 200, Length: 72, Avg Reward: -365.916073833, e: 0.380289166667, Learning Rate: 0.01, buffer_len: 16896\n",
      "Loss: -19.2831630707\n",
      "[NOR] Episode: 210, Length: 107, Avg Reward: -398.367536326, e: 0.378913666667, Learning Rate: 0.01, buffer_len: 18075\n",
      "Loss: 84.0764923096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:18:07,241] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 220, Length: 60, Avg Reward: -366.532786685, e: 0.377698, Learning Rate: 0.01, buffer_len: 19117\n",
      "Loss: 245.767990112\n",
      "[MAX] Episode: 221, Length: 104, Reward: -1.99339983876, buffer_len: 19221\n",
      "[NOR] Episode: 230, Length: 74, Avg Reward: -271.041101988, e: 0.3765955, Learning Rate: 0.01, buffer_len: 20062\n",
      "Loss: 109.348594666\n",
      "[NOR] Episode: 240, Length: 115, Avg Reward: -292.443406455, e: 0.375385666667, Learning Rate: 0.01, buffer_len: 21099\n",
      "Loss: 183.239929199\n",
      "[NOR] Episode: 250, Length: 100, Avg Reward: -230.282844587, e: 0.3743975, Learning Rate: 0.01, buffer_len: 21946\n",
      "Loss: 440.177032471\n",
      "[NOR] Episode: 260, Length: 75, Avg Reward: -233.146248467, e: 0.373386, Learning Rate: 0.01, buffer_len: 22813\n",
      "Loss: 1.23110103607\n",
      "[NOR] Episode: 270, Length: 133, Avg Reward: -206.480927764, e: 0.372284666667, Learning Rate: 0.01, buffer_len: 23757\n",
      "Loss: 183.366714478\n",
      "[NOR] Episode: 280, Length: 103, Avg Reward: -205.241125579, e: 0.371238166667, Learning Rate: 0.01, buffer_len: 24654\n",
      "Loss: 64.2373886108\n",
      "[NOR] Episode: 290, Length: 59, Avg Reward: -204.466327266, e: 0.370219666667, Learning Rate: 0.01, buffer_len: 25527\n",
      "Loss: -2.3677778244\n",
      "[NOR] Episode: 300, Length: 102, Avg Reward: -142.635904873, e: 0.369222166667, Learning Rate: 0.01, buffer_len: 26382\n",
      "Loss: 11.1356163025\n",
      "[NOR] Episode: 310, Length: 59, Avg Reward: -169.677065894, e: 0.368337833333, Learning Rate: 0.01, buffer_len: 27140\n",
      "Loss: 157.719909668\n",
      "[NOR] Episode: 320, Length: 90, Avg Reward: -154.478074104, e: 0.367291333333, Learning Rate: 0.01, buffer_len: 28037\n",
      "Loss: 8.84700584412\n",
      "[NOR] Episode: 330, Length: 103, Avg Reward: -154.899600268, e: 0.366306666667, Learning Rate: 0.01, buffer_len: 28881\n",
      "Loss: -2.31361818314\n",
      "[NOR] Episode: 340, Length: 84, Avg Reward: -172.18760789, e: 0.365337166667, Learning Rate: 0.01, buffer_len: 29712\n",
      "Loss: 364.034179688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:18:46,214] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 350, Length: 110, Avg Reward: -181.224244614, e: 0.3643, Learning Rate: 0.01, buffer_len: 30601\n",
      "Loss: 13.6606054306\n",
      "[NOR] Episode: 360, Length: 122, Avg Reward: -138.992321782, e: 0.3632745, Learning Rate: 0.01, buffer_len: 31480\n",
      "Loss: 27.2608184814\n",
      "[NOR] Episode: 370, Length: 88, Avg Reward: -151.274239669, e: 0.36227, Learning Rate: 0.01, buffer_len: 32341\n",
      "Loss: -12.0646209717\n",
      "[NOR] Episode: 380, Length: 83, Avg Reward: -145.666712812, e: 0.3612375, Learning Rate: 0.01, buffer_len: 33226\n",
      "Loss: 82.095413208\n",
      "[NOR] Episode: 390, Length: 104, Avg Reward: -144.645298222, e: 0.360305333333, Learning Rate: 0.01, buffer_len: 34025\n",
      "Loss: -5.29364299774\n",
      "[NOR] Episode: 400, Length: 100, Avg Reward: -147.361993644, e: 0.359227333333, Learning Rate: 0.01, buffer_len: 34949\n",
      "Loss: -0.340466737747\n",
      "[NOR] Episode: 410, Length: 86, Avg Reward: -155.013129596, e: 0.358225166667, Learning Rate: 0.01, buffer_len: 35808\n",
      "Loss: 246.041412354\n",
      "[NOR] Episode: 420, Length: 61, Avg Reward: -147.28020005, e: 0.3572965, Learning Rate: 0.01, buffer_len: 36604\n",
      "Loss: 18.2621574402\n",
      "[NOR] Episode: 430, Length: 109, Avg Reward: -141.5430578, e: 0.356226666667, Learning Rate: 0.01, buffer_len: 37521\n",
      "Loss: 221.490325928\n",
      "[NOR] Episode: 440, Length: 80, Avg Reward: -142.755818628, e: 0.355197666667, Learning Rate: 0.01, buffer_len: 38403\n",
      "Loss: -12.5431060791\n",
      "[NOR] Episode: 450, Length: 88, Avg Reward: -144.989745523, e: 0.354140666667, Learning Rate: 0.01, buffer_len: 39309\n",
      "Loss: 81.1373138428\n",
      "[NOR] Episode: 460, Length: 91, Avg Reward: -155.597018565, e: 0.353051, Learning Rate: 0.01, buffer_len: 40243\n",
      "Loss: 11.4981307983\n",
      "[NOR] Episode: 470, Length: 89, Avg Reward: -142.196476207, e: 0.352082666667, Learning Rate: 0.01, buffer_len: 41073\n",
      "Loss: 250.756698608\n",
      "[NOR] Episode: 480, Length: 102, Avg Reward: -141.534133165, e: 0.351200666667, Learning Rate: 0.01, buffer_len: 41829\n",
      "Loss: -2.628688097\n",
      "[NOR] Episode: 490, Length: 82, Avg Reward: -143.533784873, e: 0.350223, Learning Rate: 0.01, buffer_len: 42667\n",
      "Loss: -4.47993946075\n",
      "[NOR] Episode: 500, Length: 61, Avg Reward: -134.822681444, e: 0.3492255, Learning Rate: 0.01, buffer_len: 43522\n",
      "Loss: -12.1208181381\n",
      "[NOR] Episode: 510, Length: 85, Avg Reward: -138.918577684, e: 0.348202333333, Learning Rate: 0.01, buffer_len: 44399\n",
      "Loss: 286.244384766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:19:37,739] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 520, Length: 74, Avg Reward: -139.083742178, e: 0.347211833333, Learning Rate: 0.01, buffer_len: 45248\n",
      "Loss: -4.19596147537\n",
      "[NOR] Episode: 530, Length: 82, Avg Reward: -141.28004416, e: 0.346258666667, Learning Rate: 0.01, buffer_len: 46065\n",
      "Loss: -13.232670784\n",
      "[NOR] Episode: 540, Length: 65, Avg Reward: -140.537190509, e: 0.3452215, Learning Rate: 0.01, buffer_len: 46954\n",
      "Loss: -4.53147315979\n",
      "[NOR] Episode: 550, Length: 84, Avg Reward: -151.566395804, e: 0.344264833333, Learning Rate: 0.01, buffer_len: 47774\n",
      "Loss: 12.9827108383\n",
      "[NOR] Episode: 560, Length: 105, Avg Reward: -156.634969816, e: 0.343125, Learning Rate: 0.01, buffer_len: 48751\n",
      "Loss: 33.3791618347\n",
      "[NOR] Episode: 570, Length: 80, Avg Reward: -150.043270383, e: 0.3421135, Learning Rate: 0.01, buffer_len: 49618\n",
      "Loss: 1.03576886654\n",
      "[NOR] Episode: 580, Length: 84, Avg Reward: -140.424880361, e: 0.3411125, Learning Rate: 0.01, buffer_len: 50476\n",
      "Loss: -4.42746019363\n",
      "[MAX] Episode: 584, Length: 75, Reward: -1.02284458015, buffer_len: 50754\n",
      "[NOR] Episode: 590, Length: 92, Avg Reward: -137.693564715, e: 0.3401745, Learning Rate: 0.01, buffer_len: 51280\n",
      "Loss: -18.981842041\n",
      "[NOR] Episode: 600, Length: 81, Avg Reward: -161.484346468, e: 0.339263333333, Learning Rate: 0.01, buffer_len: 52061\n",
      "Loss: 149.120941162\n",
      "[NOR] Episode: 610, Length: 86, Avg Reward: -154.457070927, e: 0.338283333333, Learning Rate: 0.01, buffer_len: 52901\n",
      "Loss: -4.27369260788\n",
      "[NOR] Episode: 620, Length: 78, Avg Reward: -135.993448562, e: 0.3372275, Learning Rate: 0.01, buffer_len: 53806\n",
      "Loss: 138.583648682\n",
      "[NOR] Episode: 630, Length: 94, Avg Reward: -126.494671114, e: 0.3361145, Learning Rate: 0.01, buffer_len: 54760\n",
      "Loss: 36.4499092102\n",
      "[NOR] Episode: 640, Length: 99, Avg Reward: -109.484412388, e: 0.334985166667, Learning Rate: 0.01, buffer_len: 55728\n",
      "Loss: 248.14932251\n",
      "[NOR] Episode: 650, Length: 87, Avg Reward: -133.496280295, e: 0.333738, Learning Rate: 0.01, buffer_len: 56797\n",
      "Loss: -19.5670337677\n",
      "[NOR] Episode: 660, Length: 57, Avg Reward: -133.252288526, e: 0.3326845, Learning Rate: 0.01, buffer_len: 57700\n",
      "Loss: -5.1093993187\n",
      "[NOR] Episode: 670, Length: 67, Avg Reward: -156.264138817, e: 0.331513166667, Learning Rate: 0.01, buffer_len: 58704\n",
      "Loss: 42.9582977295\n",
      "[NOR] Episode: 680, Length: 92, Avg Reward: -116.538271984, e: 0.330442166667, Learning Rate: 0.01, buffer_len: 59622\n",
      "Loss: 11.055975914\n",
      "[NOR] Episode: 690, Length: 89, Avg Reward: -145.099707489, e: 0.329356, Learning Rate: 0.01, buffer_len: 60553\n",
      "Loss: 61.2349319458\n",
      "[NOR] Episode: 700, Length: 93, Avg Reward: -139.29140251, e: 0.328154333333, Learning Rate: 0.01, buffer_len: 61583\n",
      "Loss: 1.69349336624\n",
      "[NOR] Episode: 710, Length: 65, Avg Reward: -125.716529912, e: 0.3271405, Learning Rate: 0.01, buffer_len: 62452\n",
      "Loss: 0.680137395859\n",
      "[NOR] Episode: 720, Length: 88, Avg Reward: -160.741071009, e: 0.326088166667, Learning Rate: 0.01, buffer_len: 63354\n",
      "Loss: -2.51938772202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:20:48,371] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 730, Length: 108, Avg Reward: -141.077259991, e: 0.324897, Learning Rate: 0.01, buffer_len: 64375\n",
      "Loss: -41.9917869568\n",
      "[NOR] Episode: 740, Length: 94, Avg Reward: -145.798781335, e: 0.323835333333, Learning Rate: 0.01, buffer_len: 65285\n",
      "Loss: 85.9658584595\n",
      "[NOR] Episode: 750, Length: 116, Avg Reward: -131.065645827, e: 0.321629166667, Learning Rate: 0.01, buffer_len: 67176\n",
      "Loss: -0.947748541832\n",
      "[NOR] Episode: 760, Length: 101, Avg Reward: -122.703221643, e: 0.3205535, Learning Rate: 0.01, buffer_len: 68098\n",
      "Loss: 2.0998404026\n",
      "[NOR] Episode: 770, Length: 103, Avg Reward: -102.287777208, e: 0.319473166667, Learning Rate: 0.01, buffer_len: 69024\n",
      "Loss: -4.74233055115\n",
      "[NOR] Episode: 780, Length: 85, Avg Reward: -137.624951873, e: 0.318437166667, Learning Rate: 0.01, buffer_len: 69912\n",
      "Loss: 1.06264305115\n",
      "[NOR] Episode: 790, Length: 103, Avg Reward: -120.897634905, e: 0.317474666667, Learning Rate: 0.01, buffer_len: 70737\n",
      "Loss: -1.36531257629\n",
      "[NOR] Episode: 800, Length: 90, Avg Reward: -143.094303564, e: 0.3164515, Learning Rate: 0.01, buffer_len: 71614\n",
      "Loss: 4.2473950386\n",
      "[NOR] Episode: 810, Length: 86, Avg Reward: -127.364074325, e: 0.315454, Learning Rate: 0.01, buffer_len: 72469\n",
      "Loss: -18.6661262512\n",
      "[NOR] Episode: 820, Length: 72, Avg Reward: -151.154217161, e: 0.3145195, Learning Rate: 0.01, buffer_len: 73270\n",
      "Loss: 201.763961792\n",
      "[NOR] Episode: 830, Length: 96, Avg Reward: -130.344062626, e: 0.313481166667, Learning Rate: 0.01, buffer_len: 74160\n",
      "Loss: -17.6172103882\n",
      "[NOR] Episode: 840, Length: 70, Avg Reward: -126.225567513, e: 0.312621333333, Learning Rate: 0.01, buffer_len: 74897\n",
      "Loss: 223.182189941\n",
      "[NOR] Episode: 850, Length: 88, Avg Reward: -147.678078123, e: 0.311657666667, Learning Rate: 0.01, buffer_len: 75723\n",
      "Loss: 46.2680091858\n",
      "[NOR] Episode: 860, Length: 66, Avg Reward: -127.72770864, e: 0.310773333333, Learning Rate: 0.01, buffer_len: 76481\n",
      "Loss: -16.4890766144\n",
      "[NOR] Episode: 870, Length: 70, Avg Reward: -144.23661017, e: 0.309966, Learning Rate: 0.01, buffer_len: 77173\n",
      "Loss: 149.904678345\n",
      "[NOR] Episode: 880, Length: 67, Avg Reward: -144.571337485, e: 0.309029166667, Learning Rate: 0.01, buffer_len: 77976\n",
      "Loss: 10.900302887\n",
      "[NOR] Episode: 890, Length: 60, Avg Reward: -148.473208317, e: 0.3081075, Learning Rate: 0.01, buffer_len: 78766\n",
      "Loss: 6.11936807632\n",
      "[NOR] Episode: 900, Length: 73, Avg Reward: -146.541695692, e: 0.307171833333, Learning Rate: 0.01, buffer_len: 79568\n",
      "Loss: 19.1666908264\n",
      "[NOR] Episode: 910, Length: 129, Avg Reward: -142.13050705, e: 0.306198833333, Learning Rate: 0.01, buffer_len: 80402\n",
      "Loss: 242.595214844\n",
      "[NOR] Episode: 920, Length: 88, Avg Reward: -144.577879981, e: 0.305215333333, Learning Rate: 0.01, buffer_len: 81245\n",
      "Loss: -6.2545747757\n",
      "[NOR] Episode: 930, Length: 105, Avg Reward: -134.412978397, e: 0.3041595, Learning Rate: 0.01, buffer_len: 82150\n",
      "Loss: -11.9174900055\n",
      "[NOR] Episode: 940, Length: 62, Avg Reward: -141.938329275, e: 0.303201666667, Learning Rate: 0.01, buffer_len: 82971\n",
      "Loss: 194.463500977\n",
      "[NOR] Episode: 950, Length: 113, Avg Reward: -127.794351934, e: 0.302117833333, Learning Rate: 0.01, buffer_len: 83900\n",
      "Loss: -11.6819629669\n",
      "[NOR] Episode: 960, Length: 77, Avg Reward: -130.906310251, e: 0.301060833333, Learning Rate: 0.01, buffer_len: 84806\n",
      "Loss: 2.27377510071\n",
      "[NOR] Episode: 970, Length: 115, Avg Reward: -135.868445376, e: 0.299933833333, Learning Rate: 0.01, buffer_len: 85772\n",
      "Loss: -21.9230480194\n",
      "[NOR] Episode: 980, Length: 69, Avg Reward: -118.123716316, e: 0.299018, Learning Rate: 0.01, buffer_len: 86557\n",
      "Loss: 36.9776115417\n",
      "[NOR] Episode: 990, Length: 103, Avg Reward: -140.70013229, e: 0.297924833333, Learning Rate: 0.01, buffer_len: 87494\n",
      "Loss: -8.95525741577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:22:14,731] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 1000, Length: 110, Avg Reward: -141.521088862, e: 0.296867833333, Learning Rate: 0.01, buffer_len: 88400\n",
      "Loss: 32.9282608032\n",
      "[NOR] Episode: 1010, Length: 89, Avg Reward: -126.019514508, e: 0.295779333333, Learning Rate: 0.01, buffer_len: 89333\n",
      "Loss: -0.288255572319\n",
      "[NOR] Episode: 1020, Length: 126, Avg Reward: -129.392373177, e: 0.294601, Learning Rate: 0.01, buffer_len: 90343\n",
      "Loss: 4.70752477646\n",
      "[NOR] Episode: 1030, Length: 121, Avg Reward: -155.868059043, e: 0.293355, Learning Rate: 0.01, buffer_len: 91411\n",
      "Loss: -16.7612743378\n",
      "[NOR] Episode: 1040, Length: 90, Avg Reward: -154.468383948, e: 0.292187166667, Learning Rate: 0.01, buffer_len: 92412\n",
      "Loss: 54.5832824707\n",
      "[NOR] Episode: 1050, Length: 98, Avg Reward: -116.876803395, e: 0.290970333333, Learning Rate: 0.01, buffer_len: 93455\n",
      "Loss: 22.5468120575\n",
      "[NOR] Episode: 1060, Length: 82, Avg Reward: -120.874952371, e: 0.289863166667, Learning Rate: 0.01, buffer_len: 94404\n",
      "Loss: 398.600708008\n",
      "[NOR] Episode: 1070, Length: 94, Avg Reward: -128.431887434, e: 0.288781666667, Learning Rate: 0.01, buffer_len: 95331\n",
      "Loss: -6.89251279831\n",
      "[NOR] Episode: 1080, Length: 171, Avg Reward: -157.378258628, e: 0.287413166667, Learning Rate: 0.01, buffer_len: 96504\n",
      "Loss: -18.6820755005\n",
      "[NOR] Episode: 1090, Length: 98, Avg Reward: -131.634795394, e: 0.286259333333, Learning Rate: 0.01, buffer_len: 97493\n",
      "Loss: 42.9682235718\n",
      "[NOR] Episode: 1100, Length: 78, Avg Reward: -114.157023124, e: 0.284889666667, Learning Rate: 0.01, buffer_len: 98667\n",
      "Loss: 214.379730225\n",
      "[NOR] Episode: 1110, Length: 84, Avg Reward: -134.177296935, e: 0.283718333333, Learning Rate: 0.01, buffer_len: 99671\n",
      "Loss: 372.843933105\n",
      "[NOR] Episode: 1120, Length: 136, Avg Reward: -117.827843032, e: 0.282369666667, Learning Rate: 0.01, buffer_len: 100827\n",
      "Loss: 124.172576904\n",
      "[NOR] Episode: 1130, Length: 104, Avg Reward: -117.719432571, e: 0.281095666667, Learning Rate: 0.01, buffer_len: 101919\n",
      "Loss: 93.438041687\n",
      "[NOR] Episode: 1140, Length: 99, Avg Reward: -98.5402010948, e: 0.279800666667, Learning Rate: 0.01, buffer_len: 103029\n",
      "Loss: 0.962056219578\n",
      "[NOR] Episode: 1150, Length: 138, Avg Reward: -132.957896713, e: 0.278426333333, Learning Rate: 0.01, buffer_len: 104207\n",
      "Loss: -1.85011267662\n",
      "[NOR] Episode: 1160, Length: 91, Avg Reward: -123.540503517, e: 0.2771465, Learning Rate: 0.01, buffer_len: 105304\n",
      "Loss: -0.0130941867828\n",
      "[NOR] Episode: 1170, Length: 142, Avg Reward: -127.794917437, e: 0.275883, Learning Rate: 0.01, buffer_len: 106387\n",
      "Loss: 11.0947113037\n",
      "[NOR] Episode: 1180, Length: 95, Avg Reward: -118.548073884, e: 0.274579833333, Learning Rate: 0.01, buffer_len: 107504\n",
      "Loss: 316.598693848\n",
      "[NOR] Episode: 1190, Length: 102, Avg Reward: -119.413204909, e: 0.2734155, Learning Rate: 0.01, buffer_len: 108502\n",
      "Loss: 36.8209648132\n",
      "[NOR] Episode: 1200, Length: 119, Avg Reward: -142.518169584, e: 0.272035333333, Learning Rate: 0.01, buffer_len: 109685\n",
      "Loss: 78.2288513184\n",
      "[NOR] Episode: 1210, Length: 110, Avg Reward: -97.1151316653, e: 0.270698333333, Learning Rate: 0.01, buffer_len: 110831\n",
      "Loss: -5.18931531906\n",
      "[NOR] Episode: 1220, Length: 123, Avg Reward: -96.4260570521, e: 0.2683615, Learning Rate: 0.01, buffer_len: 112834\n",
      "Loss: -10.1316394806\n",
      "[NOR] Episode: 1230, Length: 133, Avg Reward: -100.982696459, e: 0.266939333333, Learning Rate: 0.01, buffer_len: 114053\n",
      "Loss: -21.6905975342\n",
      "[NOR] Episode: 1240, Length: 108, Avg Reward: -106.896144019, e: 0.265863666667, Learning Rate: 0.01, buffer_len: 114975\n",
      "Loss: 125.720581055\n",
      "[NOR] Episode: 1250, Length: 122, Avg Reward: -105.518306381, e: 0.264580333333, Learning Rate: 0.01, buffer_len: 116075\n",
      "Loss: -5.21907997131\n",
      "[NOR] Episode: 1260, Length: 124, Avg Reward: -119.660218843, e: 0.2630345, Learning Rate: 0.01, buffer_len: 117400\n",
      "Loss: -9.01974487305\n",
      "[NOR] Episode: 1270, Length: 108, Avg Reward: -111.359101661, e: 0.261629833333, Learning Rate: 0.01, buffer_len: 118604\n",
      "Loss: -8.07849311829\n",
      "[NOR] Episode: 1280, Length: 223, Avg Reward: -148.131440418, e: 0.260245, Learning Rate: 0.01, buffer_len: 119791\n",
      "Loss: 12.1823825836\n",
      "[NOR] Episode: 1290, Length: 112, Avg Reward: -146.30067653, e: 0.258590666667, Learning Rate: 0.01, buffer_len: 121209\n",
      "Loss: 92.9830322266\n",
      "[NOR] Episode: 1300, Length: 130, Avg Reward: -141.271274909, e: 0.257204666667, Learning Rate: 0.01, buffer_len: 122397\n",
      "Loss: -3.0084900856\n",
      "[NOR] Episode: 1310, Length: 144, Avg Reward: -134.302117117, e: 0.255706666667, Learning Rate: 0.01, buffer_len: 123681\n",
      "Loss: -5.45173835754\n",
      "[MAX] Episode: 1311, Length: 696, Reward: 150.944890219, buffer_len: 124377\n",
      "[NOR] Episode: 1320, Length: 135, Avg Reward: -86.6182783128, e: 0.253546, Learning Rate: 0.01, buffer_len: 125533\n",
      "Loss: 0.617362380028\n",
      "[NOR] Episode: 1330, Length: 152, Avg Reward: -147.248939723, e: 0.251619833333, Learning Rate: 0.01, buffer_len: 127184\n",
      "Loss: -6.74400043488\n",
      "[NOR] Episode: 1340, Length: 105, Avg Reward: -145.839394052, e: 0.250155666667, Learning Rate: 0.01, buffer_len: 128439\n",
      "Loss: -0.545631110668\n",
      "[NOR] Episode: 1350, Length: 120, Avg Reward: -157.849285561, e: 0.248483833333, Learning Rate: 0.01, buffer_len: 129872\n",
      "Loss: 46.5348968506\n",
      "[NOR] Episode: 1360, Length: 95, Avg Reward: -125.180790235, e: 0.247164333333, Learning Rate: 0.01, buffer_len: 131003\n",
      "Loss: 160.653808594\n",
      "[NOR] Episode: 1370, Length: 81, Avg Reward: -119.274366007, e: 0.245713, Learning Rate: 0.01, buffer_len: 132247\n",
      "Loss: 2.31489229202\n",
      "[NOR] Episode: 1380, Length: 81, Avg Reward: -89.9814905018, e: 0.244398166667, Learning Rate: 0.01, buffer_len: 133374\n",
      "Loss: -7.73173427582\n",
      "[NOR] Episode: 1390, Length: 106, Avg Reward: -115.619389234, e: 0.2431265, Learning Rate: 0.01, buffer_len: 134464\n",
      "Loss: 120.985801697\n",
      "[NOR] Episode: 1400, Length: 80, Avg Reward: -108.920756912, e: 0.241723, Learning Rate: 0.01, buffer_len: 135667\n",
      "Loss: 5.62670183182\n",
      "[NOR] Episode: 1410, Length: 74, Avg Reward: -158.274205569, e: 0.240335833333, Learning Rate: 0.01, buffer_len: 136856\n",
      "Loss: 32.6135215759\n",
      "[NOR] Episode: 1420, Length: 133, Avg Reward: -125.026520936, e: 0.2379675, Learning Rate: 0.01, buffer_len: 138886\n",
      "Loss: -10.6357250214\n",
      "[NOR] Episode: 1430, Length: 80, Avg Reward: -110.261972982, e: 0.23669, Learning Rate: 0.01, buffer_len: 139981\n",
      "Loss: -4.97547340393\n",
      "[NOR] Episode: 1440, Length: 102, Avg Reward: -130.381337907, e: 0.235178, Learning Rate: 0.01, buffer_len: 141277\n",
      "Loss: 9.1306848526\n",
      "[NOR] Episode: 1450, Length: 82, Avg Reward: -108.720576657, e: 0.2338095, Learning Rate: 0.01, buffer_len: 142450\n",
      "Loss: -10.629948616\n",
      "[NOR] Episode: 1460, Length: 125, Avg Reward: -126.632489124, e: 0.23263, Learning Rate: 0.01, buffer_len: 143461\n",
      "Loss: -2.51723766327\n",
      "[NOR] Episode: 1470, Length: 88, Avg Reward: -108.473038466, e: 0.231218333333, Learning Rate: 0.01, buffer_len: 144671\n",
      "Loss: -0.475150346756\n",
      "[NOR] Episode: 1480, Length: 115, Avg Reward: -101.210901651, e: 0.229883666667, Learning Rate: 0.01, buffer_len: 145815\n",
      "Loss: -5.29219865799\n",
      "[NOR] Episode: 1490, Length: 105, Avg Reward: -135.021746098, e: 0.228490666667, Learning Rate: 0.01, buffer_len: 147009\n",
      "Loss: 4.96708488464\n",
      "[NOR] Episode: 1500, Length: 144, Avg Reward: -112.264861617, e: 0.227244666667, Learning Rate: 0.01, buffer_len: 148077\n",
      "Loss: 26.8377056122\n",
      "[NOR] Episode: 1510, Length: 97, Avg Reward: -103.376688864, e: 0.226002166667, Learning Rate: 0.01, buffer_len: 149142\n",
      "Loss: 64.3979949951\n",
      "[NOR] Episode: 1520, Length: 80, Avg Reward: -96.4371514848, e: 0.2247935, Learning Rate: 0.01, buffer_len: 150178\n",
      "Loss: 11.6797637939\n",
      "[NOR] Episode: 1530, Length: 156, Avg Reward: -100.890447922, e: 0.223549833333, Learning Rate: 0.01, buffer_len: 151244\n",
      "Loss: 45.5006561279\n",
      "[NOR] Episode: 1540, Length: 111, Avg Reward: -119.248311565, e: 0.222191833333, Learning Rate: 0.01, buffer_len: 152408\n",
      "Loss: 244.648666382\n",
      "[NOR] Episode: 1550, Length: 90, Avg Reward: -116.173684066, e: 0.220793, Learning Rate: 0.01, buffer_len: 153607\n",
      "Loss: 7.00472736359\n",
      "[NOR] Episode: 1560, Length: 189, Avg Reward: -105.879963743, e: 0.219269333333, Learning Rate: 0.01, buffer_len: 154913\n",
      "Loss: 6.69745826721\n",
      "[NOR] Episode: 1570, Length: 104, Avg Reward: -117.970101582, e: 0.217805166667, Learning Rate: 0.01, buffer_len: 156168\n",
      "Loss: 135.739700317\n",
      "[NOR] Episode: 1580, Length: 144, Avg Reward: -103.926795739, e: 0.216352666667, Learning Rate: 0.01, buffer_len: 157413\n",
      "Loss: 17.4684715271\n",
      "[NOR] Episode: 1590, Length: 97, Avg Reward: -119.199512175, e: 0.215111333333, Learning Rate: 0.01, buffer_len: 158477\n",
      "Loss: -8.46730041504\n",
      "[NOR] Episode: 1600, Length: 96, Avg Reward: -134.610215239, e: 0.213716, Learning Rate: 0.01, buffer_len: 159673\n",
      "Loss: 365.954406738\n",
      "[NOR] Episode: 1610, Length: 98, Avg Reward: -112.419071099, e: 0.212482833333, Learning Rate: 0.01, buffer_len: 160730\n",
      "Loss: 6.40180969238\n",
      "[NOR] Episode: 1620, Length: 171, Avg Reward: -117.728476047, e: 0.2112765, Learning Rate: 0.01, buffer_len: 161764\n",
      "Loss: 41.0425682068\n",
      "[NOR] Episode: 1630, Length: 141, Avg Reward: -120.267454625, e: 0.209948833333, Learning Rate: 0.01, buffer_len: 162902\n",
      "Loss: -5.41009712219\n",
      "[NOR] Episode: 1640, Length: 84, Avg Reward: -103.427309571, e: 0.208431, Learning Rate: 0.01, buffer_len: 164203\n",
      "Loss: 0.0892705917358\n",
      "[NOR] Episode: 1650, Length: 129, Avg Reward: -93.3312204921, e: 0.2070345, Learning Rate: 0.01, buffer_len: 165400\n",
      "Loss: -13.7815494537\n",
      "[NOR] Episode: 1660, Length: 130, Avg Reward: -111.78101424, e: 0.205659, Learning Rate: 0.01, buffer_len: 166579\n",
      "Loss: -6.62260246277\n",
      "[NOR] Episode: 1670, Length: 115, Avg Reward: -116.294245591, e: 0.204155166667, Learning Rate: 0.01, buffer_len: 167868\n",
      "Loss: -5.45687913895\n",
      "[NOR] Episode: 1680, Length: 112, Avg Reward: -112.046921779, e: 0.202838, Learning Rate: 0.01, buffer_len: 168997\n",
      "Loss: 41.143661499\n",
      "[NOR] Episode: 1690, Length: 108, Avg Reward: -142.412064004, e: 0.201450833333, Learning Rate: 0.01, buffer_len: 170186\n",
      "Loss: 27.3537521362\n",
      "[NOR] Episode: 1700, Length: 123, Avg Reward: -115.249533566, e: 0.2000695, Learning Rate: 0.01, buffer_len: 171370\n",
      "Loss: -0.06192445755\n",
      "[NOR] Episode: 1710, Length: 90, Avg Reward: -129.368825058, e: 0.198891166667, Learning Rate: 0.01, buffer_len: 172380\n",
      "Loss: 4.22615480423\n",
      "[NOR] Episode: 1720, Length: 97, Avg Reward: -132.773417505, e: 0.197511, Learning Rate: 0.01, buffer_len: 173563\n",
      "Loss: 94.5124359131\n",
      "[NOR] Episode: 1730, Length: 126, Avg Reward: -118.024679485, e: 0.195996666667, Learning Rate: 0.01, buffer_len: 174861\n",
      "Loss: 5.41203689575\n",
      "[NOR] Episode: 1740, Length: 79, Avg Reward: -106.540236133, e: 0.194487, Learning Rate: 0.01, buffer_len: 176155\n",
      "Loss: -5.15868616104\n",
      "[NOR] Episode: 1750, Length: 105, Avg Reward: -151.205630247, e: 0.192969166667, Learning Rate: 0.01, buffer_len: 177456\n",
      "Loss: 3.70246076584\n",
      "[NOR] Episode: 1760, Length: 88, Avg Reward: -106.362082222, e: 0.191737166667, Learning Rate: 0.01, buffer_len: 178512\n",
      "Loss: 26.2350654602\n",
      "[NOR] Episode: 1770, Length: 114, Avg Reward: -99.2523666907, e: 0.190498166667, Learning Rate: 0.01, buffer_len: 179574\n",
      "Loss: 48.3656158447\n",
      "[NOR] Episode: 1780, Length: 150, Avg Reward: -81.4473647715, e: 0.189224166667, Learning Rate: 0.01, buffer_len: 180666\n",
      "Loss: 141.660430908\n",
      "[NOR] Episode: 1790, Length: 98, Avg Reward: -130.885140193, e: 0.187858, Learning Rate: 0.01, buffer_len: 181837\n",
      "Loss: 3.10955715179\n",
      "[NOR] Episode: 1800, Length: 115, Avg Reward: -92.6533740774, e: 0.186596833333, Learning Rate: 0.01, buffer_len: 182918\n",
      "Loss: 254.397033691\n",
      "[NOR] Episode: 1810, Length: 118, Avg Reward: -127.043017448, e: 0.185241166667, Learning Rate: 0.01, buffer_len: 184080\n",
      "Loss: 13.6893501282\n",
      "[NOR] Episode: 1820, Length: 105, Avg Reward: -103.613742368, e: 0.183879666667, Learning Rate: 0.01, buffer_len: 185247\n",
      "Loss: 22.4211483002\n",
      "[NOR] Episode: 1830, Length: 130, Avg Reward: -103.393512036, e: 0.182314, Learning Rate: 0.01, buffer_len: 186589\n",
      "Loss: 24.1347408295\n",
      "[MAX] Episode: 1837, Length: 885, Reward: 207.074770402, buffer_len: 188216\n",
      "[NOR] Episode: 1840, Length: 63, Avg Reward: -77.036017662, e: 0.1800635, Learning Rate: 0.01, buffer_len: 188518\n",
      "Loss: 8.70064258575\n",
      "[NOR] Episode: 1850, Length: 163, Avg Reward: -98.3510301115, e: 0.178368333333, Learning Rate: 0.01, buffer_len: 189971\n",
      "Loss: -2.8387196064\n",
      "[NOR] Episode: 1860, Length: 95, Avg Reward: -131.496107719, e: 0.177002166667, Learning Rate: 0.01, buffer_len: 191142\n",
      "Loss: -10.9954509735\n",
      "[NOR] Episode: 1870, Length: 139, Avg Reward: -25.4849018469, e: 0.174894, Learning Rate: 0.01, buffer_len: 192949\n",
      "Loss: -9.33451938629\n",
      "[NOR] Episode: 1880, Length: 145, Avg Reward: -84.8794166043, e: 0.172771833333, Learning Rate: 0.01, buffer_len: 194768\n",
      "Loss: 406.336730957\n",
      "[NOR] Episode: 1890, Length: 207, Avg Reward: -104.36240751, e: 0.171364833333, Learning Rate: 0.01, buffer_len: 195974\n",
      "Loss: 4.48365831375\n",
      "[NOR] Episode: 1900, Length: 98, Avg Reward: -118.004346272, e: 0.169953166667, Learning Rate: 0.01, buffer_len: 197184\n",
      "Loss: 220.030334473\n",
      "[NOR] Episode: 1910, Length: 156, Avg Reward: -92.3045069351, e: 0.168269666667, Learning Rate: 0.01, buffer_len: 198627\n",
      "Loss: 15.9127388\n",
      "[NOR] Episode: 1920, Length: 181, Avg Reward: -72.7045776152, e: 0.166523166667, Learning Rate: 0.01, buffer_len: 200124\n",
      "Loss: 6.62156248093\n",
      "[NOR] Episode: 1930, Length: 92, Avg Reward: -99.0716018981, e: 0.165204833333, Learning Rate: 0.01, buffer_len: 201254\n",
      "Loss: 2.97999644279\n",
      "[NOR] Episode: 1940, Length: 137, Avg Reward: -110.349841, e: 0.163916833333, Learning Rate: 0.01, buffer_len: 202358\n",
      "Loss: -3.08981442451\n",
      "[NOR] Episode: 1950, Length: 112, Avg Reward: -106.05369408, e: 0.1623885, Learning Rate: 0.01, buffer_len: 203668\n",
      "Loss: -13.2117614746\n",
      "[NOR] Episode: 1960, Length: 141, Avg Reward: -76.5321802843, e: 0.160720166667, Learning Rate: 0.01, buffer_len: 205098\n",
      "Loss: -14.0891151428\n",
      "[NOR] Episode: 1970, Length: 122, Avg Reward: -127.46255769, e: 0.159396, Learning Rate: 0.01, buffer_len: 206233\n",
      "Loss: 29.0085716248\n",
      "[NOR] Episode: 1980, Length: 115, Avg Reward: -109.484354704, e: 0.157486166667, Learning Rate: 0.01, buffer_len: 207870\n",
      "Loss: 3.90858411789\n",
      "[NOR] Episode: 1990, Length: 123, Avg Reward: -87.9103408737, e: 0.155589166667, Learning Rate: 0.01, buffer_len: 209496\n",
      "Loss: -2.10220980644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:28:48,886] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video002000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 2000, Length: 197, Avg Reward: -112.836800558, e: 0.15311, Learning Rate: 0.01, buffer_len: 211621\n",
      "Loss: 0.234939098358\n",
      "[NOR] Episode: 2010, Length: 159, Avg Reward: -115.722907391, e: 0.151397333333, Learning Rate: 0.01, buffer_len: 213089\n",
      "Loss: -2.44032526016\n",
      "[NOR] Episode: 2020, Length: 122, Avg Reward: -170.180717637, e: 0.149838666667, Learning Rate: 0.01, buffer_len: 214425\n",
      "Loss: 70.0360717773\n",
      "[NOR] Episode: 2030, Length: 103, Avg Reward: -134.570330067, e: 0.147857666667, Learning Rate: 0.01, buffer_len: 216123\n",
      "Loss: -8.46421527863\n",
      "[NOR] Episode: 2040, Length: 176, Avg Reward: -107.569839786, e: 0.146019, Learning Rate: 0.01, buffer_len: 217699\n",
      "Loss: 22.7128372192\n",
      "[NOR] Episode: 2050, Length: 191, Avg Reward: -87.3195128075, e: 0.144388, Learning Rate: 0.01, buffer_len: 219097\n",
      "Loss: 54.0369186401\n",
      "[NOR] Episode: 2060, Length: 118, Avg Reward: -145.445248933, e: 0.142621666667, Learning Rate: 0.01, buffer_len: 220611\n",
      "Loss: -13.1034641266\n",
      "[NOR] Episode: 2070, Length: 136, Avg Reward: -113.712617621, e: 0.140636, Learning Rate: 0.01, buffer_len: 222313\n",
      "Loss: 36.1228103638\n",
      "[NOR] Episode: 2080, Length: 163, Avg Reward: -79.5330564321, e: 0.138429833333, Learning Rate: 0.01, buffer_len: 224204\n",
      "Loss: 22.4699020386\n",
      "[NOR] Episode: 2090, Length: 134, Avg Reward: -124.111978153, e: 0.136755666667, Learning Rate: 0.01, buffer_len: 225639\n",
      "Loss: 29.1091899872\n",
      "[NOR] Episode: 2100, Length: 119, Avg Reward: -101.349938278, e: 0.1349975, Learning Rate: 0.01, buffer_len: 227146\n",
      "Loss: 4.56237316132\n",
      "[NOR] Episode: 2110, Length: 184, Avg Reward: -150.446223703, e: 0.133346666667, Learning Rate: 0.01, buffer_len: 228561\n",
      "Loss: 4.0724606514\n",
      "[NOR] Episode: 2120, Length: 129, Avg Reward: -112.180603958, e: 0.131662, Learning Rate: 0.01, buffer_len: 230005\n",
      "Loss: 1.8971247673\n",
      "[NOR] Episode: 2130, Length: 106, Avg Reward: -116.789800729, e: 0.130074166667, Learning Rate: 0.01, buffer_len: 231366\n",
      "Loss: -9.73839950562\n",
      "[NOR] Episode: 2140, Length: 102, Avg Reward: -66.943425246, e: 0.1285785, Learning Rate: 0.01, buffer_len: 232648\n",
      "Loss: 187.986572266\n",
      "[NOR] Episode: 2150, Length: 173, Avg Reward: -114.280473607, e: 0.126886833333, Learning Rate: 0.01, buffer_len: 234098\n",
      "Loss: 4.96361017227\n",
      "[NOR] Episode: 2160, Length: 128, Avg Reward: -126.586062748, e: 0.125566166667, Learning Rate: 0.01, buffer_len: 235230\n",
      "Loss: 209.559906006\n",
      "[NOR] Episode: 2170, Length: 119, Avg Reward: -97.7583640038, e: 0.124090333333, Learning Rate: 0.01, buffer_len: 236495\n",
      "Loss: 0.270303726196\n",
      "[NOR] Episode: 2180, Length: 109, Avg Reward: -93.5348308842, e: 0.122466333333, Learning Rate: 0.01, buffer_len: 237887\n",
      "Loss: 136.921630859\n",
      "[NOR] Episode: 2190, Length: 134, Avg Reward: -90.319893818, e: 0.120831833333, Learning Rate: 0.01, buffer_len: 239288\n",
      "Loss: 7.5053691864\n",
      "[NOR] Episode: 2200, Length: 97, Avg Reward: -87.0598417372, e: 0.1193875, Learning Rate: 0.01, buffer_len: 240526\n",
      "Loss: -0.70472496748\n",
      "[NOR] Episode: 2210, Length: 181, Avg Reward: -101.78865297, e: 0.117814833333, Learning Rate: 0.01, buffer_len: 241874\n",
      "Loss: 124.419021606\n",
      "[NOR] Episode: 2220, Length: 117, Avg Reward: -79.4130785294, e: 0.116138333333, Learning Rate: 0.01, buffer_len: 243311\n",
      "Loss: 8.60613250732\n",
      "[NOR] Episode: 2230, Length: 132, Avg Reward: -103.118851686, e: 0.114787333333, Learning Rate: 0.01, buffer_len: 244469\n",
      "Loss: 14.7502889633\n",
      "[NOR] Episode: 2240, Length: 118, Avg Reward: -122.188568922, e: 0.113112, Learning Rate: 0.01, buffer_len: 245905\n",
      "Loss: 2.0263967514\n",
      "[NOR] Episode: 2250, Length: 114, Avg Reward: -106.727723262, e: 0.111339833333, Learning Rate: 0.01, buffer_len: 247424\n",
      "Loss: 8.35230827332\n",
      "[NOR] Episode: 2260, Length: 150, Avg Reward: -89.0828885351, e: 0.109467333333, Learning Rate: 0.01, buffer_len: 249029\n",
      "Loss: 117.912986755\n",
      "[NOR] Episode: 2270, Length: 122, Avg Reward: -89.1826648223, e: 0.108146666667, Learning Rate: 0.01, buffer_len: 250161\n",
      "Loss: 205.995178223\n",
      "[NOR] Episode: 2280, Length: 151, Avg Reward: -71.0667269321, e: 0.106282333333, Learning Rate: 0.01, buffer_len: 251759\n",
      "Loss: 4.79406833649\n",
      "[NOR] Episode: 2290, Length: 155, Avg Reward: -80.1552806898, e: 0.104748166667, Learning Rate: 0.01, buffer_len: 253074\n",
      "Loss: 244.264984131\n",
      "[NOR] Episode: 2300, Length: 145, Avg Reward: -68.0910220661, e: 0.103222166667, Learning Rate: 0.01, buffer_len: 254382\n",
      "Loss: -11.4866886139\n",
      "[NOR] Episode: 2310, Length: 163, Avg Reward: -96.2887761651, e: 0.101646, Learning Rate: 0.01, buffer_len: 255733\n",
      "Loss: -11.3390674591\n",
      "[NOR] Episode: 2320, Length: 145, Avg Reward: -106.192223935, e: 0.100191166667, Learning Rate: 0.01, buffer_len: 256980\n",
      "Loss: 21.3494548798\n",
      "[NOR] Episode: 2330, Length: 161, Avg Reward: -60.3619546444, e: 0.0987643333333, Learning Rate: 0.01, buffer_len: 258203\n",
      "Loss: 4.5535364151\n",
      "[NOR] Episode: 2340, Length: 98, Avg Reward: -92.9222570359, e: 0.0973036666667, Learning Rate: 0.01, buffer_len: 259455\n",
      "Loss: -14.1824789047\n",
      "[NOR] Episode: 2350, Length: 143, Avg Reward: -61.8026981685, e: 0.095514, Learning Rate: 0.01, buffer_len: 260989\n",
      "Loss: -0.0240120887756\n",
      "[NOR] Episode: 2360, Length: 141, Avg Reward: -107.04131848, e: 0.0938946666667, Learning Rate: 0.01, buffer_len: 262377\n",
      "Loss: 1.74960768223\n",
      "[NOR] Episode: 2370, Length: 153, Avg Reward: -64.7978404907, e: 0.0918063333333, Learning Rate: 0.01, buffer_len: 264167\n",
      "Loss: 17.3203048706\n",
      "[NOR] Episode: 2380, Length: 141, Avg Reward: -83.5054501374, e: 0.089837, Learning Rate: 0.01, buffer_len: 265855\n",
      "Loss: 7.04654026031\n",
      "[NOR] Episode: 2390, Length: 176, Avg Reward: -72.818500672, e: 0.0877346666667, Learning Rate: 0.01, buffer_len: 267657\n",
      "Loss: -12.062374115\n",
      "[NOR] Episode: 2400, Length: 307, Avg Reward: -92.4921346517, e: 0.08598, Learning Rate: 0.01, buffer_len: 269161\n",
      "Loss: 233.623748779\n",
      "[NOR] Episode: 2410, Length: 150, Avg Reward: -72.5115744875, e: 0.0839313333333, Learning Rate: 0.01, buffer_len: 270917\n",
      "Loss: 0.896182537079\n",
      "[NOR] Episode: 2420, Length: 219, Avg Reward: -106.425759775, e: 0.0822746666667, Learning Rate: 0.01, buffer_len: 272337\n",
      "Loss: 62.1455535889\n",
      "[NOR] Episode: 2430, Length: 156, Avg Reward: -123.470711781, e: 0.0802621666667, Learning Rate: 0.01, buffer_len: 274062\n",
      "Loss: 171.900268555\n",
      "[NOR] Episode: 2440, Length: 107, Avg Reward: -100.436330622, e: 0.078539, Learning Rate: 0.01, buffer_len: 275539\n",
      "Loss: 219.481262207\n",
      "[NOR] Episode: 2450, Length: 139, Avg Reward: -100.895129801, e: 0.0766536666667, Learning Rate: 0.01, buffer_len: 277155\n",
      "Loss: -10.4124231339\n",
      "[NOR] Episode: 2460, Length: 131, Avg Reward: -79.963544278, e: 0.074647, Learning Rate: 0.01, buffer_len: 278875\n",
      "Loss: 94.5408554077\n",
      "[NOR] Episode: 2470, Length: 138, Avg Reward: -68.8781367251, e: 0.0727581666667, Learning Rate: 0.01, buffer_len: 280494\n",
      "Loss: 0.509002447128\n",
      "[NOR] Episode: 2480, Length: 175, Avg Reward: -114.721862972, e: 0.070965, Learning Rate: 0.01, buffer_len: 282031\n",
      "Loss: -4.62609243393\n",
      "[NOR] Episode: 2490, Length: 163, Avg Reward: -90.6154310025, e: 0.0692255, Learning Rate: 0.01, buffer_len: 283522\n",
      "Loss: -9.13931083679\n",
      "[NOR] Episode: 2500, Length: 168, Avg Reward: -94.4311846852, e: 0.067437, Learning Rate: 0.01, buffer_len: 285055\n",
      "Loss: -8.96828269958\n",
      "[NOR] Episode: 2510, Length: 371, Avg Reward: -92.7214349449, e: 0.0653941666667, Learning Rate: 0.01, buffer_len: 286806\n",
      "Loss: -2.36014938354\n",
      "[NOR] Episode: 2520, Length: 197, Avg Reward: -78.0231753578, e: 0.0634026666667, Learning Rate: 0.01, buffer_len: 288513\n",
      "Loss: -5.26069545746\n",
      "[NOR] Episode: 2530, Length: 113, Avg Reward: -143.863548574, e: 0.0612536666667, Learning Rate: 0.01, buffer_len: 290355\n",
      "Loss: 1.89893043041\n",
      "[NOR] Episode: 2540, Length: 453, Avg Reward: -74.4881968063, e: 0.0591186666667, Learning Rate: 0.01, buffer_len: 292185\n",
      "Loss: 109.146118164\n",
      "[NOR] Episode: 2550, Length: 157, Avg Reward: -94.5203709923, e: 0.0570735, Learning Rate: 0.01, buffer_len: 293938\n",
      "Loss: 3.53119587898\n",
      "[NOR] Episode: 2560, Length: 181, Avg Reward: -101.581390659, e: 0.0552068333333, Learning Rate: 0.01, buffer_len: 295538\n",
      "Loss: 325.728027344\n",
      "[NOR] Episode: 2570, Length: 206, Avg Reward: -46.1660062612, e: 0.0526973333333, Learning Rate: 0.01, buffer_len: 297689\n",
      "Loss: 226.791702271\n",
      "[NOR] Episode: 2580, Length: 113, Avg Reward: -125.429128821, e: 0.0509333333333, Learning Rate: 0.01, buffer_len: 299201\n",
      "Loss: 15.6749105453\n",
      "[NOR] Episode: 2590, Length: 137, Avg Reward: -133.09400288, e: 0.05, Learning Rate: 0.01, buffer_len: 300626\n",
      "Loss: 0.461466729641\n",
      "[NOR] Episode: 2600, Length: 204, Avg Reward: -105.494559943, e: 0.05, Learning Rate: 0.01, buffer_len: 302749\n",
      "Loss: 1.60709500313\n",
      "[NOR] Episode: 2610, Length: 238, Avg Reward: -136.897636572, e: 0.05, Learning Rate: 0.01, buffer_len: 304389\n",
      "Loss: 107.077095032\n",
      "[NOR] Episode: 2620, Length: 172, Avg Reward: -97.3557974838, e: 0.05, Learning Rate: 0.01, buffer_len: 306564\n",
      "Loss: 28.242609024\n",
      "[NOR] Episode: 2630, Length: 122, Avg Reward: -120.79474804, e: 0.05, Learning Rate: 0.01, buffer_len: 308389\n",
      "Loss: -3.87297916412\n",
      "[NOR] Episode: 2640, Length: 95, Avg Reward: -113.018066356, e: 0.05, Learning Rate: 0.01, buffer_len: 310365\n",
      "Loss: 197.787200928\n",
      "[NOR] Episode: 2650, Length: 180, Avg Reward: -65.4545775877, e: 0.05, Learning Rate: 0.01, buffer_len: 311902\n",
      "Loss: 239.40411377\n",
      "[NOR] Episode: 2660, Length: 199, Avg Reward: -123.935431733, e: 0.05, Learning Rate: 0.01, buffer_len: 313438\n",
      "Loss: -9.70386314392\n",
      "[NOR] Episode: 2670, Length: 171, Avg Reward: -116.568623803, e: 0.05, Learning Rate: 0.01, buffer_len: 315158\n",
      "Loss: 78.5907516479\n",
      "[NOR] Episode: 2680, Length: 156, Avg Reward: -79.5145325387, e: 0.05, Learning Rate: 0.01, buffer_len: 317096\n",
      "Loss: -0.393683433533\n",
      "[NOR] Episode: 2690, Length: 283, Avg Reward: -130.774864322, e: 0.05, Learning Rate: 0.01, buffer_len: 318597\n",
      "Loss: 56.1405715942\n",
      "[NOR] Episode: 2700, Length: 156, Avg Reward: -97.2121700902, e: 0.05, Learning Rate: 0.01, buffer_len: 320198\n",
      "Loss: 45.7492103577\n",
      "[NOR] Episode: 2710, Length: 189, Avg Reward: -116.86802013, e: 0.05, Learning Rate: 0.01, buffer_len: 322078\n",
      "Loss: -1.08994305134\n",
      "[NOR] Episode: 2720, Length: 179, Avg Reward: -143.082918491, e: 0.05, Learning Rate: 0.01, buffer_len: 323862\n",
      "Loss: 5.68554401398\n",
      "[NOR] Episode: 2730, Length: 172, Avg Reward: -93.9607570946, e: 0.05, Learning Rate: 0.01, buffer_len: 325677\n",
      "Loss: 102.62424469\n",
      "[NOR] Episode: 2740, Length: 217, Avg Reward: -114.376325661, e: 0.05, Learning Rate: 0.01, buffer_len: 327662\n",
      "Loss: 32.6919784546\n",
      "[NOR] Episode: 2750, Length: 231, Avg Reward: -78.2996297479, e: 0.05, Learning Rate: 0.01, buffer_len: 329613\n",
      "Loss: 8.63233280182\n",
      "[NOR] Episode: 2760, Length: 209, Avg Reward: -89.5171909853, e: 0.05, Learning Rate: 0.01, buffer_len: 331741\n",
      "Loss: 132.409851074\n",
      "[NOR] Episode: 2770, Length: 200, Avg Reward: -80.3061805523, e: 0.05, Learning Rate: 0.01, buffer_len: 333661\n",
      "Loss: 12.2014741898\n",
      "[NOR] Episode: 2780, Length: 129, Avg Reward: -68.5108980452, e: 0.05, Learning Rate: 0.01, buffer_len: 335317\n",
      "Loss: 15.3431367874\n",
      "[NOR] Episode: 2790, Length: 214, Avg Reward: -89.238151306, e: 0.05, Learning Rate: 0.01, buffer_len: 337224\n",
      "Loss: 231.199920654\n",
      "[NOR] Episode: 2800, Length: 93, Avg Reward: -100.747082015, e: 0.05, Learning Rate: 0.01, buffer_len: 338574\n",
      "Loss: 180.947860718\n",
      "[NOR] Episode: 2810, Length: 140, Avg Reward: -110.738766763, e: 0.05, Learning Rate: 0.01, buffer_len: 340283\n",
      "Loss: 27.7463645935\n",
      "[NOR] Episode: 2820, Length: 197, Avg Reward: -95.4562012794, e: 0.05, Learning Rate: 0.01, buffer_len: 341915\n",
      "Loss: -0.745771765709\n",
      "[NOR] Episode: 2830, Length: 181, Avg Reward: -111.312476203, e: 0.05, Learning Rate: 0.01, buffer_len: 343751\n",
      "Loss: -16.1875419617\n",
      "[NOR] Episode: 2840, Length: 198, Avg Reward: -29.0033324216, e: 0.05, Learning Rate: 0.01, buffer_len: 345946\n",
      "Loss: 112.29813385\n",
      "[NOR] Episode: 2850, Length: 121, Avg Reward: -46.829556704, e: 0.05, Learning Rate: 0.01, buffer_len: 348288\n",
      "Loss: 4.00636148453\n",
      "[NOR] Episode: 2860, Length: 238, Avg Reward: -82.5673176391, e: 0.05, Learning Rate: 0.01, buffer_len: 349882\n",
      "Loss: 0.956808328629\n",
      "[MAX] Episode: 2862, Length: 256, Reward: 208.320750909, buffer_len: 350245\n",
      "[NOR] Episode: 2870, Length: 126, Avg Reward: -56.4897676303, e: 0.05, Learning Rate: 0.01, buffer_len: 351782\n",
      "Loss: -1.9775929451\n",
      "[NOR] Episode: 2880, Length: 183, Avg Reward: -74.3336041685, e: 0.05, Learning Rate: 0.01, buffer_len: 353626\n",
      "Loss: 4.79256916046\n",
      "[NOR] Episode: 2890, Length: 199, Avg Reward: -85.6899736118, e: 0.05, Learning Rate: 0.01, buffer_len: 355447\n",
      "Loss: 3.16285514832\n",
      "[NOR] Episode: 2900, Length: 191, Avg Reward: -74.2043734536, e: 0.05, Learning Rate: 0.01, buffer_len: 357142\n",
      "Loss: -9.36493015289\n",
      "[NOR] Episode: 2910, Length: 162, Avg Reward: -134.36884016, e: 0.05, Learning Rate: 0.01, buffer_len: 358603\n",
      "Loss: 26.2032394409\n",
      "[NOR] Episode: 2920, Length: 167, Avg Reward: -51.2689732002, e: 0.05, Learning Rate: 0.01, buffer_len: 360358\n",
      "Loss: -11.6898956299\n",
      "[NOR] Episode: 2930, Length: 139, Avg Reward: -109.466856469, e: 0.05, Learning Rate: 0.01, buffer_len: 361883\n",
      "Loss: 268.547241211\n",
      "[NOR] Episode: 2940, Length: 167, Avg Reward: -86.8569738407, e: 0.05, Learning Rate: 0.01, buffer_len: 363395\n",
      "Loss: -4.99562883377\n",
      "[NOR] Episode: 2950, Length: 150, Avg Reward: -89.163398666, e: 0.05, Learning Rate: 0.01, buffer_len: 364641\n",
      "Loss: 3.02943682671\n",
      "[NOR] Episode: 2960, Length: 116, Avg Reward: -82.1844130515, e: 0.05, Learning Rate: 0.01, buffer_len: 365993\n",
      "Loss: 6.29960584641\n",
      "[NOR] Episode: 2970, Length: 117, Avg Reward: -128.44333733, e: 0.05, Learning Rate: 0.01, buffer_len: 367759\n",
      "Loss: 44.3487510681\n",
      "[NOR] Episode: 2980, Length: 173, Avg Reward: -100.121901665, e: 0.05, Learning Rate: 0.01, buffer_len: 369302\n",
      "Loss: 0.924232363701\n",
      "[MAX] Episode: 2989, Length: 334, Reward: 221.39012774, buffer_len: 370902\n",
      "[NOR] Episode: 2990, Length: 153, Avg Reward: -36.4714948018, e: 0.05, Learning Rate: 0.01, buffer_len: 371055\n",
      "Loss: 97.9281463623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:37:09,998] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video003000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 3000, Length: 179, Avg Reward: -76.514339351, e: 0.05, Learning Rate: 0.01, buffer_len: 372834\n",
      "Loss: 44.7933883667\n",
      "[NOR] Episode: 3010, Length: 95, Avg Reward: -94.655375831, e: 0.05, Learning Rate: 0.01, buffer_len: 374549\n",
      "Loss: 33.2605018616\n",
      "[NOR] Episode: 3020, Length: 108, Avg Reward: -85.1086048062, e: 0.05, Learning Rate: 0.01, buffer_len: 376218\n",
      "Loss: 38.0312919617\n",
      "[NOR] Episode: 3030, Length: 228, Avg Reward: -35.9945651176, e: 0.05, Learning Rate: 0.01, buffer_len: 378499\n",
      "Loss: -3.90676641464\n",
      "[NOR] Episode: 3040, Length: 324, Avg Reward: -54.039189141, e: 0.05, Learning Rate: 0.01, buffer_len: 380613\n",
      "Loss: 97.0826263428\n",
      "[NOR] Episode: 3050, Length: 172, Avg Reward: -109.289001641, e: 0.05, Learning Rate: 0.01, buffer_len: 381970\n",
      "Loss: 252.605163574\n",
      "[NOR] Episode: 3060, Length: 140, Avg Reward: -97.6435901454, e: 0.05, Learning Rate: 0.01, buffer_len: 383516\n",
      "Loss: 24.87591362\n",
      "[NOR] Episode: 3070, Length: 85, Avg Reward: -113.735369555, e: 0.05, Learning Rate: 0.01, buffer_len: 384901\n",
      "Loss: 1.22585260868\n",
      "[NOR] Episode: 3080, Length: 201, Avg Reward: -102.674596766, e: 0.05, Learning Rate: 0.01, buffer_len: 386514\n",
      "Loss: 0.347709476948\n",
      "[NOR] Episode: 3090, Length: 171, Avg Reward: -122.767060895, e: 0.05, Learning Rate: 0.01, buffer_len: 387856\n",
      "Loss: 25.9893913269\n",
      "[NOR] Episode: 3100, Length: 110, Avg Reward: -83.9440225863, e: 0.05, Learning Rate: 0.01, buffer_len: 389685\n",
      "Loss: 21.1066017151\n",
      "[NOR] Episode: 3110, Length: 100, Avg Reward: -129.093537758, e: 0.05, Learning Rate: 0.01, buffer_len: 391354\n",
      "Loss: -3.93781232834\n",
      "[NOR] Episode: 3120, Length: 316, Avg Reward: -121.895275953, e: 0.05, Learning Rate: 0.01, buffer_len: 392891\n",
      "Loss: 41.5985488892\n",
      "[NOR] Episode: 3130, Length: 246, Avg Reward: -90.8587821758, e: 0.05, Learning Rate: 0.01, buffer_len: 394938\n",
      "Loss: 4.98277378082\n",
      "[NOR] Episode: 3140, Length: 168, Avg Reward: -124.45112582, e: 0.05, Learning Rate: 0.01, buffer_len: 396520\n",
      "Loss: 184.807769775\n",
      "[NOR] Episode: 3150, Length: 143, Avg Reward: -88.5159321201, e: 0.05, Learning Rate: 0.01, buffer_len: 398176\n",
      "Loss: 254.128173828\n",
      "[NOR] Episode: 3160, Length: 182, Avg Reward: -128.561533077, e: 0.05, Learning Rate: 0.01, buffer_len: 399924\n",
      "Loss: 72.0561523438\n",
      "[NOR] Episode: 3170, Length: 211, Avg Reward: -126.854894013, e: 0.05, Learning Rate: 0.01, buffer_len: 401395\n",
      "Loss: 11.8897399902\n",
      "[NOR] Episode: 3180, Length: 83, Avg Reward: -124.610476215, e: 0.05, Learning Rate: 0.01, buffer_len: 402979\n",
      "Loss: 250.97958374\n",
      "[NOR] Episode: 3190, Length: 175, Avg Reward: -124.9769561, e: 0.05, Learning Rate: 0.01, buffer_len: 404474\n",
      "Loss: 15.15508461\n",
      "[NOR] Episode: 3200, Length: 142, Avg Reward: -106.139633486, e: 0.05, Learning Rate: 0.01, buffer_len: 405945\n",
      "Loss: 1.78029990196\n",
      "[NOR] Episode: 3210, Length: 195, Avg Reward: -170.733342128, e: 0.05, Learning Rate: 0.01, buffer_len: 407595\n",
      "Loss: 11.1681480408\n",
      "[NOR] Episode: 3220, Length: 201, Avg Reward: -125.089199751, e: 0.05, Learning Rate: 0.01, buffer_len: 409434\n",
      "Loss: 122.95136261\n",
      "[NOR] Episode: 3230, Length: 239, Avg Reward: -146.179267671, e: 0.05, Learning Rate: 0.01, buffer_len: 411419\n",
      "Loss: 4.95242977142\n",
      "[NOR] Episode: 3240, Length: 153, Avg Reward: -150.956838229, e: 0.05, Learning Rate: 0.01, buffer_len: 413243\n",
      "Loss: -3.58582139015\n",
      "[NOR] Episode: 3250, Length: 194, Avg Reward: -141.111654039, e: 0.05, Learning Rate: 0.01, buffer_len: 414934\n",
      "Loss: 2.89862155914\n",
      "[NOR] Episode: 3260, Length: 121, Avg Reward: -135.800875544, e: 0.05, Learning Rate: 0.01, buffer_len: 416455\n",
      "Loss: 15.0273361206\n",
      "[NOR] Episode: 3270, Length: 167, Avg Reward: -150.541650509, e: 0.05, Learning Rate: 0.01, buffer_len: 418537\n",
      "Loss: 60.2165107727\n",
      "[NOR] Episode: 3280, Length: 99, Avg Reward: -172.022112721, e: 0.05, Learning Rate: 0.01, buffer_len: 419955\n",
      "Loss: 9.06544971466\n",
      "[NOR] Episode: 3290, Length: 131, Avg Reward: -170.247364113, e: 0.05, Learning Rate: 0.01, buffer_len: 421327\n",
      "Loss: 1.33971834183\n",
      "[NOR] Episode: 3300, Length: 136, Avg Reward: -120.421439261, e: 0.05, Learning Rate: 0.01, buffer_len: 423319\n",
      "Loss: -6.10518121719\n",
      "[NOR] Episode: 3310, Length: 350, Avg Reward: -35.9265942156, e: 0.05, Learning Rate: 0.01, buffer_len: 426037\n",
      "Loss: 3.48213195801\n",
      "[NOR] Episode: 3320, Length: 103, Avg Reward: -161.699677146, e: 0.05, Learning Rate: 0.01, buffer_len: 427631\n",
      "Loss: 20.2896270752\n",
      "[NOR] Episode: 3330, Length: 116, Avg Reward: -128.39951473, e: 0.05, Learning Rate: 0.01, buffer_len: 429521\n",
      "Loss: -10.382891655\n",
      "[NOR] Episode: 3340, Length: 107, Avg Reward: -154.554205252, e: 0.05, Learning Rate: 0.01, buffer_len: 430944\n",
      "Loss: -0.212295293808\n",
      "[NOR] Episode: 3350, Length: 341, Avg Reward: -106.848414323, e: 0.05, Learning Rate: 0.01, buffer_len: 433613\n",
      "Loss: 3.60082769394\n",
      "[NOR] Episode: 3360, Length: 125, Avg Reward: -166.366533765, e: 0.05, Learning Rate: 0.01, buffer_len: 436082\n",
      "Loss: 2.14306426048\n",
      "[NOR] Episode: 3370, Length: 112, Avg Reward: -78.96353658, e: 0.05, Learning Rate: 0.01, buffer_len: 438384\n",
      "Loss: 6.66589736938\n",
      "[NOR] Episode: 3380, Length: 171, Avg Reward: -55.1769240315, e: 0.05, Learning Rate: 0.01, buffer_len: 440771\n",
      "Loss: 0.492505550385\n",
      "[NOR] Episode: 3390, Length: 220, Avg Reward: -162.131632843, e: 0.05, Learning Rate: 0.01, buffer_len: 442503\n",
      "Loss: 46.9555206299\n",
      "[NOR] Episode: 3400, Length: 110, Avg Reward: -113.167979011, e: 0.05, Learning Rate: 0.01, buffer_len: 444958\n",
      "Loss: 96.8633270264\n",
      "[NOR] Episode: 3410, Length: 982, Avg Reward: -41.8191522423, e: 0.05, Learning Rate: 0.01, buffer_len: 448325\n",
      "Loss: 0.935697197914\n",
      "[NOR] Episode: 3420, Length: 243, Avg Reward: -146.320117872, e: 0.05, Learning Rate: 0.01, buffer_len: 450478\n",
      "Loss: 226.944000244\n",
      "[NOR] Episode: 3430, Length: 295, Avg Reward: -105.407141262, e: 0.05, Learning Rate: 0.01, buffer_len: 452928\n",
      "Loss: -8.29261016846\n",
      "[NOR] Episode: 3440, Length: 421, Avg Reward: -84.8427060501, e: 0.05, Learning Rate: 0.01, buffer_len: 455737\n",
      "Loss: -6.98609638214\n",
      "[NOR] Episode: 3450, Length: 208, Avg Reward: -17.5416015552, e: 0.05, Learning Rate: 0.01, buffer_len: 458797\n",
      "Loss: 237.403198242\n",
      "[NOR] Episode: 3460, Length: 178, Avg Reward: -100.563943419, e: 0.05, Learning Rate: 0.01, buffer_len: 460513\n",
      "Loss: 1.08253002167\n",
      "[NOR] Episode: 3470, Length: 269, Avg Reward: -114.903654314, e: 0.05, Learning Rate: 0.01, buffer_len: 462308\n",
      "Loss: 176.334091187\n",
      "[NOR] Episode: 3480, Length: 137, Avg Reward: -117.374044478, e: 0.05, Learning Rate: 0.01, buffer_len: 464161\n",
      "Loss: 40.7860908508\n",
      "[NOR] Episode: 3490, Length: 232, Avg Reward: -56.8609257257, e: 0.05, Learning Rate: 0.01, buffer_len: 466149\n",
      "Loss: 26.6152572632\n",
      "[NOR] Episode: 3500, Length: 569, Avg Reward: -109.826246438, e: 0.05, Learning Rate: 0.01, buffer_len: 468462\n",
      "Loss: 0.502684533596\n",
      "[NOR] Episode: 3510, Length: 261, Avg Reward: 49.7464113405, e: 0.05, Learning Rate: 0.01, buffer_len: 471806\n",
      "Loss: 19.6341438293\n",
      "[NOR] Episode: 3520, Length: 412, Avg Reward: -52.6823215008, e: 0.05, Learning Rate: 0.01, buffer_len: 474679\n",
      "Loss: 1.64263963699\n",
      "[NOR] Episode: 3530, Length: 155, Avg Reward: -103.654886048, e: 0.05, Learning Rate: 0.01, buffer_len: 477338\n",
      "Loss: -2.91882634163\n",
      "[NOR] Episode: 3540, Length: 217, Avg Reward: -89.613090128, e: 0.05, Learning Rate: 0.01, buffer_len: 480838\n",
      "Loss: -3.65966367722\n",
      "[NOR] Episode: 3550, Length: 286, Avg Reward: -38.2128961708, e: 0.05, Learning Rate: 0.01, buffer_len: 483730\n",
      "Loss: 8.58959388733\n",
      "[NOR] Episode: 3560, Length: 199, Avg Reward: -126.127059684, e: 0.05, Learning Rate: 0.01, buffer_len: 485493\n",
      "Loss: 8.12848472595\n",
      "[NOR] Episode: 3570, Length: 220, Avg Reward: -136.221703113, e: 0.05, Learning Rate: 0.01, buffer_len: 488099\n",
      "Loss: -2.12442660332\n",
      "[NOR] Episode: 3580, Length: 136, Avg Reward: -142.515323217, e: 0.05, Learning Rate: 0.01, buffer_len: 490026\n",
      "Loss: 132.632949829\n",
      "[NOR] Episode: 3590, Length: 217, Avg Reward: -167.451363442, e: 0.05, Learning Rate: 0.01, buffer_len: 492167\n",
      "Loss: 18.6315135956\n",
      "[NOR] Episode: 3600, Length: 226, Avg Reward: -156.72790356, e: 0.05, Learning Rate: 0.01, buffer_len: 494045\n",
      "Loss: 109.044021606\n",
      "[NOR] Episode: 3610, Length: 121, Avg Reward: -65.1676474373, e: 0.05, Learning Rate: 0.01, buffer_len: 497044\n",
      "Loss: 232.733215332\n",
      "[NOR] Episode: 3620, Length: 348, Avg Reward: -106.895278775, e: 0.05, Learning Rate: 0.01, buffer_len: 499308\n",
      "Loss: 179.015106201\n",
      "[NOR] Episode: 3630, Length: 260, Avg Reward: -179.494458955, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 31.0714702606\n",
      "[NOR] Episode: 3640, Length: 163, Avg Reward: -120.151236128, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.6300535202\n",
      "[NOR] Episode: 3650, Length: 245, Avg Reward: -112.741222829, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.50538063049\n",
      "[NOR] Episode: 3660, Length: 86, Avg Reward: -86.3272039013, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 24.1000232697\n",
      "[NOR] Episode: 3670, Length: 259, Avg Reward: -19.7997184678, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 35.8787727356\n",
      "[NOR] Episode: 3680, Length: 261, Avg Reward: -54.2612133491, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 247.115661621\n",
      "[NOR] Episode: 3690, Length: 205, Avg Reward: -121.087215594, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 39.8766021729\n",
      "[NOR] Episode: 3700, Length: 493, Avg Reward: 68.7615116167, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.780845403671\n",
      "[MAX] Episode: 3706, Length: 559, Reward: 240.622509239, buffer_len: 500000\n",
      "[NOR] Episode: 3710, Length: 391, Avg Reward: -0.612691164322, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.2049026489\n",
      "[NOR] Episode: 3720, Length: 223, Avg Reward: -60.9099110367, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 75.9990692139\n",
      "[NOR] Episode: 3730, Length: 194, Avg Reward: -30.1285091682, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 148.790649414\n",
      "[NOR] Episode: 3740, Length: 353, Avg Reward: -50.8874703001, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.43146634102\n",
      "[NOR] Episode: 3750, Length: 372, Avg Reward: -72.45855201, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.942428588867\n",
      "[NOR] Episode: 3760, Length: 206, Avg Reward: -13.102877912, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.629209697247\n",
      "[NOR] Episode: 3770, Length: 202, Avg Reward: -65.9316567105, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.779859602451\n",
      "[NOR] Episode: 3780, Length: 106, Avg Reward: -110.672247036, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.5099143982\n",
      "[NOR] Episode: 3790, Length: 153, Avg Reward: 2.41947579228, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.22612380981\n",
      "[NOR] Episode: 3800, Length: 481, Avg Reward: -11.2787461757, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 159.188995361\n",
      "[NOR] Episode: 3810, Length: 410, Avg Reward: 84.6137671651, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 115.444503784\n",
      "[NOR] Episode: 3820, Length: 213, Avg Reward: 22.0739889688, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.62668299675\n",
      "[NOR] Episode: 3830, Length: 468, Avg Reward: 59.9806226991, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.20019102097\n",
      "[NOR] Episode: 3840, Length: 647, Avg Reward: 89.5780593023, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.107620596886\n",
      "[NOR] Episode: 3850, Length: 476, Avg Reward: 129.25674453, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.203567326069\n",
      "[NOR] Episode: 3860, Length: 998, Avg Reward: 148.787415506, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.55412960052\n",
      "[NOR] Episode: 3870, Length: 451, Avg Reward: 104.385650921, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0462638735771\n",
      "[NOR] Episode: 3880, Length: 1000, Avg Reward: 34.1929983224, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.55071735382\n",
      "[NOR] Episode: 3890, Length: 336, Avg Reward: -21.9451523436, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.52406597137\n",
      "[NOR] Episode: 3900, Length: 1000, Avg Reward: -29.3403679715, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 244.204605103\n",
      "[NOR] Episode: 3910, Length: 1000, Avg Reward: -20.48059594, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.14601612091\n",
      "[NOR] Episode: 3920, Length: 1000, Avg Reward: -54.5072875492, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.37950897217\n",
      "[NOR] Episode: 3930, Length: 632, Avg Reward: -25.0169603799, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 30.5274085999\n",
      "[NOR] Episode: 3940, Length: 1000, Avg Reward: -6.78474838066, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.82996940613\n",
      "[NOR] Episode: 3950, Length: 1000, Avg Reward: -61.6124028896, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 447.091125488\n",
      "[NOR] Episode: 3960, Length: 1000, Avg Reward: -78.9632563249, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 80.7901306152\n",
      "[NOR] Episode: 3970, Length: 448, Avg Reward: 68.7843384534, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.77772712708\n",
      "[NOR] Episode: 3980, Length: 681, Avg Reward: 22.018946792, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 156.912445068\n",
      "[NOR] Episode: 3990, Length: 750, Avg Reward: 39.6792068072, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.119377374649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 01:58:06,255] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video004000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 4000, Length: 354, Avg Reward: 34.8354789684, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.5269603729\n",
      "[NOR] Episode: 4010, Length: 584, Avg Reward: 144.176711748, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.5749435425\n",
      "[NOR] Episode: 4020, Length: 158, Avg Reward: 29.0559845897, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.396255254745\n",
      "[NOR] Episode: 4030, Length: 1000, Avg Reward: 32.3354739618, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.95072841644\n",
      "[NOR] Episode: 4040, Length: 1000, Avg Reward: 45.7770173819, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.765479207039\n",
      "[MAX] Episode: 4046, Length: 459, Reward: 251.67485821, buffer_len: 500000\n",
      "[NOR] Episode: 4050, Length: 356, Avg Reward: 95.0302586711, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 250.539978027\n",
      "[NOR] Episode: 4060, Length: 293, Avg Reward: 131.969412482, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.1859617233\n",
      "[NOR] Episode: 4070, Length: 310, Avg Reward: 134.56742091, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.765785455704\n",
      "[NOR] Episode: 4080, Length: 245, Avg Reward: 146.814185878, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 216.618270874\n",
      "[NOR] Episode: 4090, Length: 207, Avg Reward: 101.320007622, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 250.745941162\n",
      "[NOR] Episode: 4100, Length: 275, Avg Reward: 200.881975569, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.1855125427\n",
      "[NOR] Episode: 4110, Length: 489, Avg Reward: 170.654759327, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.20081949234\n",
      "[NOR] Episode: 4120, Length: 382, Avg Reward: 110.772029203, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.62059879303\n",
      "[NOR] Episode: 4130, Length: 1000, Avg Reward: 79.9643113293, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 152.792816162\n",
      "[NOR] Episode: 4140, Length: 560, Avg Reward: 148.468563309, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 185.845428467\n",
      "[NOR] Episode: 4150, Length: 268, Avg Reward: 161.747795375, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.82058334351\n",
      "[NOR] Episode: 4160, Length: 266, Avg Reward: 180.429105978, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.28557825089\n",
      "[NOR] Episode: 4170, Length: 292, Avg Reward: 159.212574158, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.92041921616\n",
      "[NOR] Episode: 4180, Length: 523, Avg Reward: 129.489110242, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 32.1257553101\n",
      "[NOR] Episode: 4190, Length: 180, Avg Reward: 150.11755779, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.14377856255\n",
      "[NOR] Episode: 4200, Length: 368, Avg Reward: 167.435835772, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.70007133484\n",
      "[NOR] Episode: 4210, Length: 1000, Avg Reward: 111.510994159, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.78638958931\n",
      "[NOR] Episode: 4220, Length: 1000, Avg Reward: 11.1656091297, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.223046302795\n",
      "[NOR] Episode: 4230, Length: 1000, Avg Reward: 38.4514687911, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.20605683327\n",
      "[NOR] Episode: 4240, Length: 563, Avg Reward: 52.3779135968, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.17427539825\n",
      "[NOR] Episode: 4250, Length: 234, Avg Reward: 15.0182108567, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.81925439835\n",
      "[NOR] Episode: 4260, Length: 1000, Avg Reward: 108.737872341, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.79258441925\n",
      "[NOR] Episode: 4270, Length: 336, Avg Reward: 48.1848749205, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0547770261765\n",
      "[NOR] Episode: 4280, Length: 127, Avg Reward: 109.673601561, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.663615942\n",
      "[NOR] Episode: 4290, Length: 690, Avg Reward: 91.5713915204, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.41035914421\n",
      "[NOR] Episode: 4300, Length: 130, Avg Reward: 57.6161467643, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.78229427338\n",
      "[NOR] Episode: 4310, Length: 983, Avg Reward: 45.43678893, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.47204065323\n",
      "[NOR] Episode: 4320, Length: 318, Avg Reward: 123.346483662, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.80293083191\n",
      "[NOR] Episode: 4330, Length: 281, Avg Reward: 148.038222426, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.18731832504\n",
      "[NOR] Episode: 4340, Length: 100, Avg Reward: 68.8533068911, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.5807952881\n",
      "[NOR] Episode: 4350, Length: 195, Avg Reward: 97.2514365102, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.9421882629\n",
      "[NOR] Episode: 4360, Length: 493, Avg Reward: 83.4516162344, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.85882043839\n",
      "[NOR] Episode: 4370, Length: 112, Avg Reward: 56.2318298174, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.12075185776\n",
      "[NOR] Episode: 4380, Length: 279, Avg Reward: 35.8803832203, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.34156107903\n",
      "[MAX] Episode: 4389, Length: 313, Reward: 258.422769257, buffer_len: 500000\n",
      "[NOR] Episode: 4390, Length: 186, Avg Reward: 136.775698292, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.50394153595\n",
      "[NOR] Episode: 4400, Length: 153, Avg Reward: 3.58568569927, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.71930217743\n",
      "[NOR] Episode: 4410, Length: 83, Avg Reward: -6.11856984347, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.31630039215\n",
      "[NOR] Episode: 4420, Length: 255, Avg Reward: 121.936234072, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.05207943916\n",
      "[NOR] Episode: 4430, Length: 102, Avg Reward: -53.8279518073, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.23164749146\n",
      "[NOR] Episode: 4440, Length: 449, Avg Reward: 150.245476923, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.85378623009\n",
      "[NOR] Episode: 4450, Length: 282, Avg Reward: 153.603151167, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.988927841187\n",
      "[NOR] Episode: 4460, Length: 139, Avg Reward: 133.331015128, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.90043282509\n",
      "[MAX] Episode: 4464, Length: 327, Reward: 258.794762935, buffer_len: 500000\n",
      "[NOR] Episode: 4470, Length: 173, Avg Reward: 153.713894693, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.70719850063\n",
      "[NOR] Episode: 4480, Length: 881, Avg Reward: 113.249502151, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.19828391075\n",
      "[NOR] Episode: 4490, Length: 170, Avg Reward: 124.750362891, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.18678331375\n",
      "[NOR] Episode: 4500, Length: 200, Avg Reward: 77.19032676, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.84756422043\n",
      "[NOR] Episode: 4510, Length: 81, Avg Reward: 101.460746189, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.659159779549\n",
      "[NOR] Episode: 4520, Length: 136, Avg Reward: 98.4324306556, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 257.208129883\n",
      "[NOR] Episode: 4530, Length: 274, Avg Reward: 179.020860967, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.3558652401\n",
      "[NOR] Episode: 4540, Length: 233, Avg Reward: 150.990488084, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.15616965294\n",
      "[NOR] Episode: 4550, Length: 206, Avg Reward: 154.056963991, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.5129795074\n",
      "[NOR] Episode: 4560, Length: 252, Avg Reward: 131.15399793, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0272250175476\n",
      "[NOR] Episode: 4570, Length: 174, Avg Reward: 87.8853379085, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.202420353889\n",
      "[NOR] Episode: 4580, Length: 328, Avg Reward: 38.4978538474, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.95330822468\n",
      "[NOR] Episode: 4590, Length: 196, Avg Reward: 92.9648647706, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.00171661377\n",
      "[NOR] Episode: 4600, Length: 256, Avg Reward: 17.5050451352, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.88025808334\n",
      "[NOR] Episode: 4610, Length: 336, Avg Reward: -0.718970903358, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.37247753143\n",
      "[NOR] Episode: 4620, Length: 110, Avg Reward: 61.8154481315, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.54272413254\n",
      "[NOR] Episode: 4630, Length: 61, Avg Reward: 17.9605248118, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.74827575684\n",
      "[NOR] Episode: 4640, Length: 389, Avg Reward: 173.483054327, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.97920799255\n",
      "[NOR] Episode: 4650, Length: 544, Avg Reward: 129.583934272, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.1585931778\n",
      "[NOR] Episode: 4660, Length: 177, Avg Reward: 115.726955729, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.5411157608\n",
      "[NOR] Episode: 4670, Length: 245, Avg Reward: 133.764460322, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.5277309418\n",
      "[NOR] Episode: 4680, Length: 233, Avg Reward: 84.6126984689, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.83868408203\n",
      "[NOR] Episode: 4690, Length: 167, Avg Reward: 153.782431931, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.86546814442\n",
      "[NOR] Episode: 4700, Length: 130, Avg Reward: 128.094665375, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.72016620636\n",
      "[NOR] Episode: 4710, Length: 896, Avg Reward: 113.902637087, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.9053153992\n",
      "[NOR] Episode: 4720, Length: 97, Avg Reward: 119.996069773, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.07912111282\n",
      "[NOR] Episode: 4730, Length: 108, Avg Reward: 16.895356875, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.320010602474\n",
      "[NOR] Episode: 4740, Length: 316, Avg Reward: 81.6350113013, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.705572605133\n",
      "[NOR] Episode: 4750, Length: 166, Avg Reward: 130.632255171, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.165559053421\n",
      "[NOR] Episode: 4760, Length: 239, Avg Reward: 116.601316606, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.37470245361\n",
      "[NOR] Episode: 4770, Length: 257, Avg Reward: 97.140469902, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.31632804871\n",
      "[NOR] Episode: 4780, Length: 212, Avg Reward: 106.3919494, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.49351096153\n",
      "[NOR] Episode: 4790, Length: 367, Avg Reward: 153.27005475, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.740737318993\n",
      "[NOR] Episode: 4800, Length: 489, Avg Reward: 84.7501909752, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.40091896057\n",
      "[NOR] Episode: 4810, Length: 380, Avg Reward: 124.128496148, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.62637615204\n",
      "[NOR] Episode: 4820, Length: 312, Avg Reward: 48.0141102218, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.3200359344\n",
      "[NOR] Episode: 4830, Length: 569, Avg Reward: 109.785632863, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.82490205765\n",
      "[NOR] Episode: 4840, Length: 328, Avg Reward: 175.714132631, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.74768865108\n",
      "[MAX] Episode: 4842, Length: 343, Reward: 271.348091664, buffer_len: 500000\n",
      "[NOR] Episode: 4850, Length: 229, Avg Reward: 200.658034532, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.21692347527\n",
      "[NOR] Episode: 4860, Length: 280, Avg Reward: 163.073555922, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 210.404144287\n",
      "[NOR] Episode: 4870, Length: 306, Avg Reward: 190.311657779, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.246406793594\n",
      "[NOR] Episode: 4880, Length: 455, Avg Reward: 169.00658159, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.344249367714\n",
      "[NOR] Episode: 4890, Length: 430, Avg Reward: 214.083913193, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.580719828606\n",
      "[NOR] Episode: 4900, Length: 621, Avg Reward: 151.405591135, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.65102005005\n",
      "[NOR] Episode: 4910, Length: 405, Avg Reward: 170.082524319, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.34887051582\n",
      "[NOR] Episode: 4920, Length: 630, Avg Reward: 148.030262478, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.13954162598\n",
      "[NOR] Episode: 4930, Length: 372, Avg Reward: 118.051910264, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.870470047\n",
      "[NOR] Episode: 4940, Length: 444, Avg Reward: 185.349899992, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.82745885849\n",
      "[NOR] Episode: 4950, Length: 141, Avg Reward: 103.249824505, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.18101167679\n",
      "[NOR] Episode: 4960, Length: 439, Avg Reward: 192.097801611, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.21476697922\n",
      "[NOR] Episode: 4970, Length: 1000, Avg Reward: 96.8093452301, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.15402603149\n",
      "[NOR] Episode: 4980, Length: 193, Avg Reward: 161.693546038, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.21909022331\n",
      "[NOR] Episode: 4990, Length: 180, Avg Reward: 160.233911377, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.753918886185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 02:18:48,853] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video005000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 5000, Length: 276, Avg Reward: 186.637388962, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.263635873795\n",
      "[NOR] Episode: 5010, Length: 263, Avg Reward: 164.670708126, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.86114168167\n",
      "[NOR] Episode: 5020, Length: 260, Avg Reward: 192.041873727, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.168060556054\n",
      "[NOR] Episode: 5030, Length: 179, Avg Reward: 142.305056885, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.33479547501\n",
      "[MAX] Episode: 5035, Length: 272, Reward: 272.313376154, buffer_len: 500000\n",
      "[NOR] Episode: 5040, Length: 164, Avg Reward: 152.350507441, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.6625919342\n",
      "[NOR] Episode: 5050, Length: 199, Avg Reward: 200.374845384, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.09260511398\n",
      "[NOR] Episode: 5060, Length: 347, Avg Reward: 162.967224168, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.52090072632\n",
      "[NOR] Episode: 5070, Length: 526, Avg Reward: 160.549567977, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.226534247398\n",
      "[MAX] Episode: 5080, Length: 313, Reward: 272.507571049, buffer_len: 500000\n",
      "[NOR] Episode: 5080, Length: 313, Avg Reward: 154.336627393, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 249.863327026\n",
      "[NOR] Episode: 5090, Length: 138, Avg Reward: 165.541227395, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.103962063789\n",
      "[NOR] Episode: 5100, Length: 286, Avg Reward: 157.049899934, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.2041931152\n",
      "[NOR] Episode: 5110, Length: 237, Avg Reward: 210.111502439, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.3720998764\n",
      "[NOR] Episode: 5120, Length: 235, Avg Reward: 229.307056739, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.93314874172\n",
      "[NOR] Episode: 5130, Length: 337, Avg Reward: 223.156894396, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.81700372696\n",
      "[NOR] Episode: 5140, Length: 348, Avg Reward: 194.899459748, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.56981468201\n",
      "[NOR] Episode: 5150, Length: 225, Avg Reward: 158.063770215, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.67768573761\n",
      "[NOR] Episode: 5160, Length: 335, Avg Reward: 160.780415066, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 254.890228271\n",
      "[NOR] Episode: 5170, Length: 433, Avg Reward: 167.257058478, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.27253293991\n",
      "[NOR] Episode: 5180, Length: 302, Avg Reward: 226.664967227, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.44904136658\n",
      "[NOR] Episode: 5190, Length: 399, Avg Reward: 190.562688632, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.7643828392\n",
      "[NOR] Episode: 5200, Length: 328, Avg Reward: 169.869719294, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.138900756836\n",
      "[NOR] Episode: 5210, Length: 272, Avg Reward: 219.561846122, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.49091494083\n",
      "[NOR] Episode: 5220, Length: 243, Avg Reward: 226.322679369, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.01341533661\n",
      "[MAX] Episode: 5229, Length: 342, Reward: 272.691146069, buffer_len: 500000\n",
      "[NOR] Episode: 5230, Length: 200, Avg Reward: 224.592339746, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0804960727692\n",
      "[NOR] Episode: 5240, Length: 231, Avg Reward: 187.996495222, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.24591445923\n",
      "[NOR] Episode: 5250, Length: 244, Avg Reward: 184.79627039, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.1318778992\n",
      "[NOR] Episode: 5260, Length: 902, Avg Reward: 220.156201397, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.31067967415\n",
      "[NOR] Episode: 5270, Length: 318, Avg Reward: 239.253264169, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.546580433846\n",
      "[NOR] Episode: 5280, Length: 200, Avg Reward: 177.428800079, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.77062129974\n",
      "[NOR] Episode: 5290, Length: 310, Avg Reward: 191.899015682, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.81886339188\n",
      "[NOR] Episode: 5300, Length: 244, Avg Reward: 209.391817619, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.72735261917\n",
      "[NOR] Episode: 5310, Length: 267, Avg Reward: 213.285413185, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.42571997643\n",
      "[NOR] Episode: 5320, Length: 293, Avg Reward: 222.791222991, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.93782615662\n",
      "[NOR] Episode: 5330, Length: 289, Avg Reward: 161.267195843, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.61452007294\n",
      "[NOR] Episode: 5340, Length: 415, Avg Reward: 209.371689347, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.39373826981\n",
      "[NOR] Episode: 5350, Length: 348, Avg Reward: 174.509745444, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.5735616684\n",
      "[NOR] Episode: 5360, Length: 349, Avg Reward: 194.158104439, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.871640920639\n",
      "[NOR] Episode: 5370, Length: 233, Avg Reward: 170.48420636, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.57031440735\n",
      "[NOR] Episode: 5380, Length: 326, Avg Reward: 220.321752951, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.74609899521\n",
      "[NOR] Episode: 5390, Length: 131, Avg Reward: 164.280730514, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.81588983536\n",
      "[NOR] Episode: 5400, Length: 283, Avg Reward: 165.82276695, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0767269134521\n",
      "[NOR] Episode: 5410, Length: 203, Avg Reward: 158.018534141, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.512078762054\n",
      "[NOR] Episode: 5420, Length: 325, Avg Reward: 114.379259898, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.0528409481\n",
      "[NOR] Episode: 5430, Length: 599, Avg Reward: 199.454048368, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.29118871689\n",
      "[NOR] Episode: 5440, Length: 539, Avg Reward: -8.17786128881, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.20223128796\n",
      "[NOR] Episode: 5450, Length: 642, Avg Reward: 160.158266383, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 246.606414795\n",
      "[NOR] Episode: 5460, Length: 359, Avg Reward: 42.3724211068, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.11999630928\n",
      "[NOR] Episode: 5470, Length: 341, Avg Reward: 173.777734936, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.69544100761\n",
      "[NOR] Episode: 5480, Length: 162, Avg Reward: 130.055331505, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.3478851318\n",
      "[NOR] Episode: 5490, Length: 442, Avg Reward: 189.316278396, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.95302295685\n",
      "[NOR] Episode: 5500, Length: 310, Avg Reward: 146.690724996, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.41265559196\n",
      "[NOR] Episode: 5510, Length: 703, Avg Reward: 90.8882311563, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.52276492119\n",
      "[NOR] Episode: 5520, Length: 1000, Avg Reward: -58.5376811215, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 258.148223877\n",
      "[NOR] Episode: 5530, Length: 1000, Avg Reward: -41.5197814241, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.08998656273\n",
      "[NOR] Episode: 5540, Length: 1000, Avg Reward: -182.637241322, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.7994947433\n",
      "[NOR] Episode: 5550, Length: 417, Avg Reward: -47.7615503603, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.38626480103\n",
      "[NOR] Episode: 5560, Length: 1000, Avg Reward: 103.396815281, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.23028182983\n",
      "[NOR] Episode: 5570, Length: 512, Avg Reward: 133.674614422, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.5137193203\n",
      "[NOR] Episode: 5580, Length: 625, Avg Reward: 166.281151732, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 272.323852539\n",
      "[NOR] Episode: 5590, Length: 621, Avg Reward: 120.823739914, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.0426187515\n",
      "[NOR] Episode: 5600, Length: 740, Avg Reward: -1.52604041379, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.327439785\n",
      "[NOR] Episode: 5610, Length: 1000, Avg Reward: 128.10802259, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.26554393768\n",
      "[NOR] Episode: 5620, Length: 1000, Avg Reward: 108.559671799, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.36840963364\n",
      "[NOR] Episode: 5630, Length: 715, Avg Reward: -26.0475954467, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.9003343582\n",
      "[NOR] Episode: 5640, Length: 957, Avg Reward: 55.149010081, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.87315034866\n",
      "[NOR] Episode: 5650, Length: 624, Avg Reward: 89.9359712487, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.14747929573\n",
      "[NOR] Episode: 5660, Length: 1000, Avg Reward: 8.59128790065, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.69566011429\n",
      "[NOR] Episode: 5670, Length: 1000, Avg Reward: -3.44669230426, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.41622757912\n",
      "[NOR] Episode: 5680, Length: 1000, Avg Reward: -43.4556242903, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.187301576138\n",
      "[NOR] Episode: 5690, Length: 1000, Avg Reward: -53.7547817997, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.48127102852\n",
      "[NOR] Episode: 5700, Length: 1000, Avg Reward: -6.18401017292, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.13496291637\n",
      "[NOR] Episode: 5710, Length: 1000, Avg Reward: 26.3317375465, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.156906694174\n",
      "[NOR] Episode: 5720, Length: 1000, Avg Reward: -19.0641196063, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.968645751476\n",
      "[NOR] Episode: 5730, Length: 1000, Avg Reward: 42.2939367412, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.67832803726\n",
      "[NOR] Episode: 5740, Length: 647, Avg Reward: 102.324274797, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.21589231491\n",
      "[NOR] Episode: 5750, Length: 1000, Avg Reward: 125.991772212, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.6238193512\n",
      "[NOR] Episode: 5760, Length: 342, Avg Reward: 148.676734905, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.2874183655\n",
      "[NOR] Episode: 5770, Length: 1000, Avg Reward: 129.120427136, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.30949580669\n",
      "[NOR] Episode: 5780, Length: 365, Avg Reward: 76.2102018382, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.01180315018\n",
      "[NOR] Episode: 5790, Length: 430, Avg Reward: 193.878294431, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.80195140839\n",
      "[NOR] Episode: 5800, Length: 392, Avg Reward: 193.000568104, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.667634010315\n",
      "[NOR] Episode: 5810, Length: 280, Avg Reward: 188.499016835, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.69656419754\n",
      "[NOR] Episode: 5820, Length: 320, Avg Reward: 206.043855088, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.278625696898\n",
      "[NOR] Episode: 5830, Length: 623, Avg Reward: 174.344612184, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.5361161232\n",
      "[NOR] Episode: 5840, Length: 273, Avg Reward: 162.736782435, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 31.7479476929\n",
      "[NOR] Episode: 5850, Length: 336, Avg Reward: 210.211692704, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.106876373291\n",
      "[NOR] Episode: 5860, Length: 334, Avg Reward: 200.93461245, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.37368392944\n",
      "[NOR] Episode: 5870, Length: 359, Avg Reward: 210.349846805, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 250.60508728\n",
      "[NOR] Episode: 5880, Length: 279, Avg Reward: 139.115887962, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.70537924767\n",
      "[NOR] Episode: 5890, Length: 236, Avg Reward: 208.799128361, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.28270065784\n",
      "[NOR] Episode: 5900, Length: 400, Avg Reward: 226.606293864, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.965879559517\n",
      "[NOR] Episode: 5910, Length: 346, Avg Reward: 181.22681282, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.3411273956\n",
      "[NOR] Episode: 5920, Length: 328, Avg Reward: 216.608446551, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.74207496643\n",
      "[NOR] Episode: 5930, Length: 301, Avg Reward: 208.461511718, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.10712122917\n",
      "[NOR] Episode: 5940, Length: 283, Avg Reward: 183.465972019, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 248.094360352\n",
      "[NOR] Episode: 5950, Length: 364, Avg Reward: 196.422744407, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.87828636169\n",
      "[NOR] Episode: 5960, Length: 223, Avg Reward: 135.497013817, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.93391799927\n",
      "[NOR] Episode: 5970, Length: 712, Avg Reward: 153.494219353, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.1668014526\n",
      "[NOR] Episode: 5980, Length: 779, Avg Reward: 170.005402295, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.95902729034\n",
      "[NOR] Episode: 5990, Length: 301, Avg Reward: 145.669463161, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.77233624458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 02:51:50,506] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video006000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 6000, Length: 404, Avg Reward: 141.458835911, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.37481451035\n",
      "[NOR] Episode: 6010, Length: 259, Avg Reward: 178.278681712, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.2872142792\n",
      "[NOR] Episode: 6020, Length: 262, Avg Reward: 219.188850307, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.57602977753\n",
      "[NOR] Episode: 6030, Length: 368, Avg Reward: 195.906281212, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.63027715683\n",
      "[NOR] Episode: 6040, Length: 437, Avg Reward: 131.445349963, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.906276643276\n",
      "[NOR] Episode: 6050, Length: 401, Avg Reward: 128.622338099, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.152612090111\n",
      "[NOR] Episode: 6060, Length: 318, Avg Reward: 179.052564037, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.04090619087\n",
      "[NOR] Episode: 6070, Length: 365, Avg Reward: 148.592077315, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.715093970299\n",
      "[NOR] Episode: 6080, Length: 384, Avg Reward: 168.378969267, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.30085754395\n",
      "[NOR] Episode: 6090, Length: 450, Avg Reward: 200.18408428, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.42500138283\n",
      "[NOR] Episode: 6100, Length: 415, Avg Reward: 203.854471354, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 248.768768311\n",
      "[NOR] Episode: 6110, Length: 458, Avg Reward: 211.40422135, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.19363415241\n",
      "[NOR] Episode: 6120, Length: 304, Avg Reward: 204.073939649, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 28.8247699738\n",
      "[NOR] Episode: 6130, Length: 660, Avg Reward: 145.178831161, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.24360704422\n",
      "[NOR] Episode: 6140, Length: 407, Avg Reward: 212.720362806, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.573209643364\n",
      "[NOR] Episode: 6150, Length: 1000, Avg Reward: -9.60360038202, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.4471111298\n",
      "[NOR] Episode: 6160, Length: 337, Avg Reward: 159.205116109, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.577709436417\n",
      "[NOR] Episode: 6170, Length: 315, Avg Reward: 173.437364056, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.129863798618\n",
      "[NOR] Episode: 6180, Length: 361, Avg Reward: 164.222870486, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.38914895058\n",
      "[NOR] Episode: 6190, Length: 885, Avg Reward: 140.788271899, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.06317186356\n",
      "[NOR] Episode: 6200, Length: 576, Avg Reward: 182.284620949, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.19177961349\n",
      "[NOR] Episode: 6210, Length: 500, Avg Reward: 152.616655234, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.382626861334\n",
      "[NOR] Episode: 6220, Length: 612, Avg Reward: 188.753890897, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.13159561157\n",
      "[NOR] Episode: 6230, Length: 358, Avg Reward: 199.118711484, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.23416376114\n",
      "[NOR] Episode: 6240, Length: 461, Avg Reward: 204.913548802, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.4046134949\n",
      "[NOR] Episode: 6250, Length: 380, Avg Reward: 207.832528687, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.71626806259\n",
      "[NOR] Episode: 6260, Length: 276, Avg Reward: 151.876420893, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.08619618416\n",
      "[NOR] Episode: 6270, Length: 302, Avg Reward: 225.375501227, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.63990306854\n",
      "[NOR] Episode: 6280, Length: 347, Avg Reward: 218.015952233, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.96841049194\n",
      "[NOR] Episode: 6290, Length: 268, Avg Reward: 194.360187545, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.618250727654\n",
      "[NOR] Episode: 6300, Length: 346, Avg Reward: 215.232079064, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.67842388153\n",
      "[NOR] Episode: 6310, Length: 279, Avg Reward: 231.981149963, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.33206796646\n",
      "[NOR] Episode: 6320, Length: 628, Avg Reward: 171.951838315, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.17271900177\n",
      "[NOR] Episode: 6330, Length: 355, Avg Reward: 214.081439445, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0989764630795\n",
      "[NOR] Episode: 6340, Length: 524, Avg Reward: 168.066866317, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.15942525864\n",
      "[NOR] Episode: 6350, Length: 282, Avg Reward: 177.151197366, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.6819763184\n",
      "[NOR] Episode: 6360, Length: 294, Avg Reward: 216.106224841, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.64038085938\n",
      "[NOR] Episode: 6370, Length: 332, Avg Reward: 187.666135545, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 252.401779175\n",
      "[NOR] Episode: 6380, Length: 350, Avg Reward: 122.95469218, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.85348272324\n",
      "[NOR] Episode: 6390, Length: 233, Avg Reward: 147.707294259, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.823034763336\n",
      "[NOR] Episode: 6400, Length: 184, Avg Reward: 154.467158053, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.1383099556\n",
      "[NOR] Episode: 6410, Length: 358, Avg Reward: 114.508095207, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.1059532166\n",
      "[NOR] Episode: 6420, Length: 358, Avg Reward: 183.243782685, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.14947605133\n",
      "[NOR] Episode: 6430, Length: 265, Avg Reward: 159.30767923, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.73871350288\n",
      "[NOR] Episode: 6440, Length: 327, Avg Reward: 132.523255209, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.7366876602\n",
      "[NOR] Episode: 6450, Length: 275, Avg Reward: 111.01755896, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.752746582\n",
      "[NOR] Episode: 6460, Length: 277, Avg Reward: 109.350638416, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.53549098969\n",
      "[NOR] Episode: 6470, Length: 225, Avg Reward: 116.095972135, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.93921756744\n",
      "[NOR] Episode: 6480, Length: 1000, Avg Reward: 116.84300406, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.1878547668\n",
      "[NOR] Episode: 6490, Length: 159, Avg Reward: -19.2832569548, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.74206256866\n",
      "[NOR] Episode: 6500, Length: 403, Avg Reward: -8.6184652497, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.56305789948\n",
      "[NOR] Episode: 6510, Length: 374, Avg Reward: 94.8958676956, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.08345031738\n",
      "[NOR] Episode: 6520, Length: 1000, Avg Reward: 61.0109181043, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.74279499054\n",
      "[NOR] Episode: 6530, Length: 199, Avg Reward: -62.6915118933, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.27467131615\n",
      "[NOR] Episode: 6540, Length: 391, Avg Reward: -65.2801959548, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.47109031677\n",
      "[NOR] Episode: 6550, Length: 131, Avg Reward: 6.2036392895, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.933931529522\n",
      "[NOR] Episode: 6560, Length: 164, Avg Reward: -16.6433147106, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.476799696684\n",
      "[NOR] Episode: 6570, Length: 1000, Avg Reward: -10.628641469, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.18720734119\n",
      "[NOR] Episode: 6580, Length: 485, Avg Reward: 126.618404595, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.95092201233\n",
      "[NOR] Episode: 6590, Length: 294, Avg Reward: 122.8761777, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.64896059036\n",
      "[NOR] Episode: 6600, Length: 404, Avg Reward: 140.816809126, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 255.62890625\n",
      "[NOR] Episode: 6610, Length: 242, Avg Reward: 180.856554926, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.7483291626\n",
      "[NOR] Episode: 6620, Length: 570, Avg Reward: 82.5642881524, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.57747030258\n",
      "[NOR] Episode: 6630, Length: 295, Avg Reward: 190.987221997, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.43477058411\n",
      "[NOR] Episode: 6640, Length: 385, Avg Reward: 75.3171173601, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.04326677322\n",
      "[NOR] Episode: 6650, Length: 184, Avg Reward: 27.808533374, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.81942272186\n",
      "[NOR] Episode: 6660, Length: 209, Avg Reward: 76.5354473892, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.32256746292\n",
      "[NOR] Episode: 6670, Length: 1000, Avg Reward: 43.2531139772, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.394926786423\n",
      "[NOR] Episode: 6680, Length: 163, Avg Reward: 87.3449405322, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.21770477295\n",
      "[NOR] Episode: 6690, Length: 538, Avg Reward: 99.2242938317, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.49593305588\n",
      "[NOR] Episode: 6700, Length: 1000, Avg Reward: 30.8242206285, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.57788801193\n",
      "[NOR] Episode: 6710, Length: 220, Avg Reward: -159.989236401, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.19288873672\n",
      "[NOR] Episode: 6720, Length: 197, Avg Reward: -68.4563051215, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.4992370605\n",
      "[NOR] Episode: 6730, Length: 246, Avg Reward: 69.3880415031, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.7021808624\n",
      "[NOR] Episode: 6740, Length: 946, Avg Reward: 122.151959439, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.39301419258\n",
      "[NOR] Episode: 6750, Length: 585, Avg Reward: 150.076543693, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.24294614792\n",
      "[NOR] Episode: 6760, Length: 753, Avg Reward: 147.682798768, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.028422832489\n",
      "[NOR] Episode: 6770, Length: 469, Avg Reward: 96.464010906, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 246.68196106\n",
      "[NOR] Episode: 6780, Length: 170, Avg Reward: 122.989667129, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.24782133102\n",
      "[NOR] Episode: 6790, Length: 126, Avg Reward: 71.1811228643, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.493884712458\n",
      "[NOR] Episode: 6800, Length: 310, Avg Reward: 93.477780647, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.51831150055\n",
      "[NOR] Episode: 6810, Length: 605, Avg Reward: 116.624139498, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 242.483032227\n",
      "[NOR] Episode: 6820, Length: 316, Avg Reward: 1.16793578608, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.23746490479\n",
      "[NOR] Episode: 6830, Length: 246, Avg Reward: -19.0533185819, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.6889705658\n",
      "[NOR] Episode: 6840, Length: 590, Avg Reward: 151.463126314, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.09062719345\n",
      "[NOR] Episode: 6850, Length: 674, Avg Reward: 126.511635923, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.03664064407\n",
      "[NOR] Episode: 6860, Length: 426, Avg Reward: 121.40381688, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.06384944916\n",
      "[NOR] Episode: 6870, Length: 442, Avg Reward: 113.357925619, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.685750365257\n",
      "[NOR] Episode: 6880, Length: 258, Avg Reward: 37.1039153293, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.699162125587\n",
      "[NOR] Episode: 6890, Length: 512, Avg Reward: 163.235426383, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.69343090057\n",
      "[NOR] Episode: 6900, Length: 419, Avg Reward: 91.8805784657, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 252.876876831\n",
      "[NOR] Episode: 6910, Length: 512, Avg Reward: 140.966462785, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.55694913864\n",
      "[NOR] Episode: 6920, Length: 1000, Avg Reward: 81.5367434092, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.90675258636\n",
      "[NOR] Episode: 6930, Length: 1000, Avg Reward: -30.9318076946, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.7291221619\n",
      "[NOR] Episode: 6940, Length: 1000, Avg Reward: -59.8491340522, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.1125369072\n",
      "[NOR] Episode: 6950, Length: 1000, Avg Reward: -151.499005591, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.26434135437\n",
      "[NOR] Episode: 6960, Length: 360, Avg Reward: -6.38445035605, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.06369161606\n",
      "[NOR] Episode: 6970, Length: 1000, Avg Reward: -148.48067694, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.6805801392\n",
      "[NOR] Episode: 6980, Length: 158, Avg Reward: 43.4038873731, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.5044984818\n",
      "[NOR] Episode: 6990, Length: 503, Avg Reward: 137.919796446, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.88888525963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 03:20:50,991] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video007000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 7000, Length: 614, Avg Reward: 163.712977436, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.16698074341\n",
      "[NOR] Episode: 7010, Length: 525, Avg Reward: 92.5157296155, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.11277580261\n",
      "[NOR] Episode: 7020, Length: 369, Avg Reward: 176.913693831, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 118.501083374\n",
      "[NOR] Episode: 7030, Length: 341, Avg Reward: 137.964282944, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.39794445038\n",
      "[NOR] Episode: 7040, Length: 339, Avg Reward: 161.868529432, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.05223846436\n",
      "[NOR] Episode: 7050, Length: 500, Avg Reward: 158.144279745, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.75638961792\n",
      "[NOR] Episode: 7060, Length: 508, Avg Reward: 107.488955296, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.50390458107\n",
      "[NOR] Episode: 7070, Length: 684, Avg Reward: 29.0875185194, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.04037237167\n",
      "[NOR] Episode: 7080, Length: 1000, Avg Reward: -49.9476444709, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.43134307861\n",
      "[NOR] Episode: 7090, Length: 903, Avg Reward: -40.4389554111, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.74969816208\n",
      "[NOR] Episode: 7100, Length: 748, Avg Reward: 118.613354202, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 263.711669922\n",
      "[NOR] Episode: 7110, Length: 367, Avg Reward: 183.445985265, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.34105491638\n",
      "[NOR] Episode: 7120, Length: 1000, Avg Reward: 105.712759177, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.32913422585\n",
      "[NOR] Episode: 7130, Length: 715, Avg Reward: 102.197185664, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.38284063339\n",
      "[NOR] Episode: 7140, Length: 1000, Avg Reward: 14.8261545702, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.42076921463\n",
      "[NOR] Episode: 7150, Length: 1000, Avg Reward: 71.087382544, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.129967927933\n",
      "[NOR] Episode: 7160, Length: 1000, Avg Reward: -60.9725053875, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.57204580307\n",
      "[NOR] Episode: 7170, Length: 1000, Avg Reward: 13.2321870352, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.03522777557\n",
      "[NOR] Episode: 7180, Length: 1000, Avg Reward: -69.2451071632, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.38928318024\n",
      "[NOR] Episode: 7190, Length: 1000, Avg Reward: -86.4015918842, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.690460801125\n",
      "[NOR] Episode: 7200, Length: 356, Avg Reward: 33.8460116757, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0146738290787\n",
      "[NOR] Episode: 7210, Length: 639, Avg Reward: 114.596346988, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.17795181274\n",
      "[NOR] Episode: 7220, Length: 1000, Avg Reward: 16.4373449115, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.71423578262\n",
      "[NOR] Episode: 7230, Length: 423, Avg Reward: 180.076013763, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.65686988831\n",
      "[NOR] Episode: 7240, Length: 678, Avg Reward: 90.3710012607, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.21080636978\n",
      "[NOR] Episode: 7250, Length: 1000, Avg Reward: 90.6892866297, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.38232135773\n",
      "[NOR] Episode: 7260, Length: 1000, Avg Reward: 139.60008276, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.8452625275\n",
      "[NOR] Episode: 7270, Length: 269, Avg Reward: 120.2997479, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.79412055016\n",
      "[NOR] Episode: 7280, Length: 384, Avg Reward: 113.874420653, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.46762371063\n",
      "[NOR] Episode: 7290, Length: 288, Avg Reward: -0.982289601453, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.46607589722\n",
      "[NOR] Episode: 7300, Length: 642, Avg Reward: 89.3951982776, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.50720691681\n",
      "[NOR] Episode: 7310, Length: 796, Avg Reward: 49.4738803492, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 251.16619873\n",
      "[NOR] Episode: 7320, Length: 1000, Avg Reward: -18.949884588, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.43044710159\n",
      "[NOR] Episode: 7330, Length: 169, Avg Reward: 47.5348211661, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.54509878159\n",
      "[NOR] Episode: 7340, Length: 551, Avg Reward: 30.4788155078, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.39712524414\n",
      "[NOR] Episode: 7350, Length: 803, Avg Reward: 76.2466123507, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.86094558239\n",
      "[NOR] Episode: 7360, Length: 1000, Avg Reward: 99.8080396526, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0167534351349\n",
      "[NOR] Episode: 7370, Length: 846, Avg Reward: 64.8326965289, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.10965240002\n",
      "[NOR] Episode: 7380, Length: 723, Avg Reward: 51.5685073426, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.73591804504\n",
      "[NOR] Episode: 7390, Length: 502, Avg Reward: 121.580252343, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.23422837257\n",
      "[NOR] Episode: 7400, Length: 1000, Avg Reward: 151.963975476, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.6277484894\n",
      "[NOR] Episode: 7410, Length: 269, Avg Reward: 1.07931436172, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.34392547607\n",
      "[NOR] Episode: 7420, Length: 647, Avg Reward: 73.6603970079, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.511250257492\n",
      "[NOR] Episode: 7430, Length: 546, Avg Reward: 79.0701612881, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.59333074093\n",
      "[NOR] Episode: 7440, Length: 845, Avg Reward: 133.400875212, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.01451158524\n",
      "[NOR] Episode: 7450, Length: 1000, Avg Reward: 60.5251155812, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.759171843529\n",
      "[NOR] Episode: 7460, Length: 582, Avg Reward: 146.11483102, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.7542309761\n",
      "[NOR] Episode: 7470, Length: 718, Avg Reward: 164.087797513, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.67906498909\n",
      "[NOR] Episode: 7480, Length: 560, Avg Reward: 176.71328502, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.85430526733\n",
      "[NOR] Episode: 7490, Length: 731, Avg Reward: 144.985380331, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.975717067719\n",
      "[NOR] Episode: 7500, Length: 884, Avg Reward: 29.9709031902, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.805419623852\n",
      "[NOR] Episode: 7510, Length: 1000, Avg Reward: -21.2875283129, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 257.413970947\n",
      "[NOR] Episode: 7520, Length: 1000, Avg Reward: -69.6714900915, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.45972275734\n",
      "[NOR] Episode: 7530, Length: 841, Avg Reward: -46.5355470681, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.44948327541\n",
      "[NOR] Episode: 7540, Length: 1000, Avg Reward: -68.5162554962, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.98996186256\n",
      "[NOR] Episode: 7550, Length: 1000, Avg Reward: -78.612971919, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.47683644295\n",
      "[NOR] Episode: 7560, Length: 1000, Avg Reward: -88.1991565544, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.91168928146\n",
      "[NOR] Episode: 7570, Length: 1000, Avg Reward: -62.9871114547, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.5317668915\n",
      "[NOR] Episode: 7580, Length: 1000, Avg Reward: -60.2453803282, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.88040781021\n",
      "[NOR] Episode: 7590, Length: 967, Avg Reward: -96.1987913892, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.28516244888\n",
      "[NOR] Episode: 7600, Length: 853, Avg Reward: 8.22971444916, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 261.497711182\n",
      "[NOR] Episode: 7610, Length: 931, Avg Reward: 120.808123992, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.275606393814\n",
      "[NOR] Episode: 7620, Length: 593, Avg Reward: 162.440022903, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.38439297676\n",
      "[NOR] Episode: 7630, Length: 590, Avg Reward: 162.645144638, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.89421081543\n",
      "[NOR] Episode: 7640, Length: 659, Avg Reward: 76.82941191, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.116758823395\n",
      "[NOR] Episode: 7650, Length: 815, Avg Reward: -38.6300015053, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.61245822906\n",
      "[NOR] Episode: 7660, Length: 255, Avg Reward: -124.685494978, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.56653404236\n",
      "[NOR] Episode: 7670, Length: 135, Avg Reward: -149.837923562, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.71577239037\n",
      "[NOR] Episode: 7680, Length: 144, Avg Reward: -109.896422562, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.71502232552\n",
      "[NOR] Episode: 7690, Length: 194, Avg Reward: -140.712330371, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.37826871872\n",
      "[NOR] Episode: 7700, Length: 174, Avg Reward: -164.056372904, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.88958835602\n",
      "[NOR] Episode: 7710, Length: 121, Avg Reward: -158.971012, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.223257958889\n",
      "[NOR] Episode: 7720, Length: 137, Avg Reward: -195.094131635, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.9796898365\n",
      "[NOR] Episode: 7730, Length: 135, Avg Reward: -170.230818608, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.342744350433\n",
      "[NOR] Episode: 7740, Length: 85, Avg Reward: -179.240493347, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.05353546143\n",
      "[NOR] Episode: 7750, Length: 120, Avg Reward: -231.158992129, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.2614979744\n",
      "[NOR] Episode: 7760, Length: 145, Avg Reward: -242.530503777, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.39100933075\n",
      "[NOR] Episode: 7770, Length: 130, Avg Reward: -298.201415961, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.93781471252\n",
      "[NOR] Episode: 7780, Length: 136, Avg Reward: -354.652014359, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.48440265656\n",
      "[NOR] Episode: 7790, Length: 121, Avg Reward: -360.647903605, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.37437987328\n",
      "[NOR] Episode: 7800, Length: 130, Avg Reward: -366.745492288, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.7216939926\n",
      "[NOR] Episode: 7810, Length: 126, Avg Reward: -302.481125225, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.6818847656\n",
      "[NOR] Episode: 7820, Length: 110, Avg Reward: -167.585709104, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.08527135849\n",
      "[NOR] Episode: 7830, Length: 193, Avg Reward: -149.908098578, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.46111297607\n",
      "[NOR] Episode: 7840, Length: 130, Avg Reward: -115.361445837, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.89967441559\n",
      "[NOR] Episode: 7850, Length: 123, Avg Reward: -111.914293598, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.12752485275\n",
      "[NOR] Episode: 7860, Length: 110, Avg Reward: -114.652685736, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.82226490974\n",
      "[NOR] Episode: 7870, Length: 170, Avg Reward: -124.054601788, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.55666816235\n",
      "[NOR] Episode: 7880, Length: 249, Avg Reward: -164.566792314, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.75041389465\n",
      "[NOR] Episode: 7890, Length: 113, Avg Reward: -179.402608277, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.45414209366\n",
      "[NOR] Episode: 7900, Length: 207, Avg Reward: -158.372364893, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.56236743927\n",
      "[NOR] Episode: 7910, Length: 89, Avg Reward: -111.999220682, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.74429321289\n",
      "[NOR] Episode: 7920, Length: 75, Avg Reward: -155.516936736, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.6974506378\n",
      "[NOR] Episode: 7930, Length: 51, Avg Reward: -106.752332896, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.15560519695\n",
      "[NOR] Episode: 7940, Length: 62, Avg Reward: -153.471747821, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.698335289955\n",
      "[NOR] Episode: 7950, Length: 76, Avg Reward: -140.209500929, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.18198537827\n",
      "[NOR] Episode: 7960, Length: 68, Avg Reward: -115.840665069, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.716197013855\n",
      "[NOR] Episode: 7970, Length: 195, Avg Reward: -120.224370916, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.466779112816\n",
      "[NOR] Episode: 7980, Length: 149, Avg Reward: -124.759750183, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.9685764313\n",
      "[NOR] Episode: 7990, Length: 186, Avg Reward: -135.74390547, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.15163469315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 04:00:26,010] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video008000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 8000, Length: 75, Avg Reward: -148.893921962, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.87208688259\n",
      "[NOR] Episode: 8010, Length: 57, Avg Reward: -150.331803647, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.426493763924\n",
      "[NOR] Episode: 8020, Length: 66, Avg Reward: -172.298576514, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.79467582703\n",
      "[NOR] Episode: 8030, Length: 161, Avg Reward: -181.49937127, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.43273162842\n",
      "[NOR] Episode: 8040, Length: 205, Avg Reward: -131.099696753, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.40911912918\n",
      "[NOR] Episode: 8050, Length: 172, Avg Reward: -134.981910076, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.30971503258\n",
      "[NOR] Episode: 8060, Length: 105, Avg Reward: -149.109276344, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.25017976761\n",
      "[NOR] Episode: 8070, Length: 145, Avg Reward: -130.773580008, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.21218919754\n",
      "[NOR] Episode: 8080, Length: 90, Avg Reward: -199.560645222, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.32485294342\n",
      "[NOR] Episode: 8090, Length: 215, Avg Reward: -151.422997174, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.85081911087\n",
      "[NOR] Episode: 8100, Length: 228, Avg Reward: -252.469050265, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.7468624115\n",
      "[NOR] Episode: 8110, Length: 127, Avg Reward: -410.603921688, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.19470000267\n",
      "[NOR] Episode: 8120, Length: 137, Avg Reward: -479.330226997, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.38788080215\n",
      "[NOR] Episode: 8130, Length: 119, Avg Reward: -488.124177555, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.57552552223\n",
      "[NOR] Episode: 8140, Length: 109, Avg Reward: -568.87745539, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.06327009201\n",
      "[NOR] Episode: 8150, Length: 103, Avg Reward: -527.342909684, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.31816613674\n",
      "[NOR] Episode: 8160, Length: 112, Avg Reward: -520.921444914, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 264.97769165\n",
      "[NOR] Episode: 8170, Length: 87, Avg Reward: -518.513528017, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.42768859863\n",
      "[NOR] Episode: 8180, Length: 129, Avg Reward: -459.096608098, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.63958692551\n",
      "[NOR] Episode: 8190, Length: 117, Avg Reward: -443.972210898, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.05766487122\n",
      "[NOR] Episode: 8200, Length: 144, Avg Reward: -450.788177331, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.29574251175\n",
      "[NOR] Episode: 8210, Length: 157, Avg Reward: -401.631878924, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.8970594406\n",
      "[NOR] Episode: 8220, Length: 171, Avg Reward: -256.02426217, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.1925163269\n",
      "[NOR] Episode: 8230, Length: 95, Avg Reward: -322.731291972, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.96460831165\n",
      "[NOR] Episode: 8240, Length: 210, Avg Reward: -227.952351889, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.50435352325\n",
      "[NOR] Episode: 8250, Length: 136, Avg Reward: -228.507079745, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.63790416718\n",
      "[NOR] Episode: 8260, Length: 154, Avg Reward: -278.832434841, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.49960803986\n",
      "[NOR] Episode: 8270, Length: 127, Avg Reward: -207.356416191, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.84736180305\n",
      "[NOR] Episode: 8280, Length: 170, Avg Reward: -258.667622204, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 234.998352051\n",
      "[NOR] Episode: 8290, Length: 106, Avg Reward: -336.892308397, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.3549919128\n",
      "[NOR] Episode: 8300, Length: 152, Avg Reward: -351.203916428, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.28220939636\n",
      "[NOR] Episode: 8310, Length: 136, Avg Reward: -385.292512401, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.81496858597\n",
      "[NOR] Episode: 8320, Length: 109, Avg Reward: -367.531275485, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.6236953735\n",
      "[NOR] Episode: 8330, Length: 139, Avg Reward: -268.971708261, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.82025337219\n",
      "[NOR] Episode: 8340, Length: 96, Avg Reward: -295.748088895, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -23.2184448242\n",
      "[NOR] Episode: 8350, Length: 130, Avg Reward: -307.686771984, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.69112324715\n",
      "[NOR] Episode: 8360, Length: 88, Avg Reward: -370.614356808, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.43054389954\n",
      "[NOR] Episode: 8370, Length: 139, Avg Reward: -378.922675744, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.70118904114\n",
      "[NOR] Episode: 8380, Length: 130, Avg Reward: -374.0999488, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.01555371284\n",
      "[NOR] Episode: 8390, Length: 311, Avg Reward: -311.929348055, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.752576828\n",
      "[NOR] Episode: 8400, Length: 158, Avg Reward: -327.820330336, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.76061153412\n",
      "[NOR] Episode: 8410, Length: 159, Avg Reward: -334.228197482, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.5150175095\n",
      "[NOR] Episode: 8420, Length: 233, Avg Reward: -359.359098446, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.14474177361\n",
      "[NOR] Episode: 8430, Length: 176, Avg Reward: -253.609219738, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.42595911026\n",
      "[NOR] Episode: 8440, Length: 259, Avg Reward: -312.331127425, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 217.78793335\n",
      "[NOR] Episode: 8450, Length: 137, Avg Reward: -305.507749336, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 56.6873779297\n",
      "[NOR] Episode: 8460, Length: 105, Avg Reward: -325.349210336, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.71421289444\n",
      "[NOR] Episode: 8470, Length: 192, Avg Reward: -301.875235815, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.15847158432\n",
      "[NOR] Episode: 8480, Length: 138, Avg Reward: -277.44947001, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.83130717278\n",
      "[NOR] Episode: 8490, Length: 142, Avg Reward: -226.556383334, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.0933208466\n",
      "[NOR] Episode: 8500, Length: 281, Avg Reward: -186.999173828, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.82338309288\n",
      "[NOR] Episode: 8510, Length: 199, Avg Reward: -197.009578137, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.8879852295\n",
      "[NOR] Episode: 8520, Length: 281, Avg Reward: -199.619084156, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0672163367271\n",
      "[NOR] Episode: 8530, Length: 250, Avg Reward: -175.849975283, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.60461521149\n",
      "[NOR] Episode: 8540, Length: 185, Avg Reward: -235.731136988, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.48007559776\n",
      "[NOR] Episode: 8550, Length: 235, Avg Reward: -229.405997953, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.00986433029\n",
      "[NOR] Episode: 8560, Length: 74, Avg Reward: -164.556343105, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.24668884277\n",
      "[NOR] Episode: 8570, Length: 85, Avg Reward: -239.84723092, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.433932423592\n",
      "[NOR] Episode: 8580, Length: 120, Avg Reward: -296.230807722, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.448765158653\n",
      "[NOR] Episode: 8590, Length: 96, Avg Reward: -184.375980756, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.17344474792\n",
      "[NOR] Episode: 8600, Length: 89, Avg Reward: -129.652214728, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.3572807312\n",
      "[NOR] Episode: 8610, Length: 83, Avg Reward: -153.62823901, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.14684152603\n",
      "[NOR] Episode: 8620, Length: 70, Avg Reward: -172.260525438, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.654995441437\n",
      "[NOR] Episode: 8630, Length: 73, Avg Reward: -180.982655424, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.16098690033\n",
      "[NOR] Episode: 8640, Length: 131, Avg Reward: -146.909525449, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 253.356658936\n",
      "[NOR] Episode: 8650, Length: 70, Avg Reward: -173.498469889, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.48289179802\n",
      "[NOR] Episode: 8660, Length: 129, Avg Reward: -181.504938493, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 33.3789291382\n",
      "[NOR] Episode: 8670, Length: 69, Avg Reward: -170.164748028, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.18777370453\n",
      "[NOR] Episode: 8680, Length: 182, Avg Reward: -133.544031493, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.30706322193\n",
      "[NOR] Episode: 8690, Length: 90, Avg Reward: -162.672479229, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 77.3526306152\n",
      "[NOR] Episode: 8700, Length: 60, Avg Reward: -155.590291042, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -15.4426422119\n",
      "[NOR] Episode: 8710, Length: 77, Avg Reward: -147.386659831, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.62405204773\n",
      "[NOR] Episode: 8720, Length: 80, Avg Reward: -150.483413066, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.23015117645\n",
      "[NOR] Episode: 8730, Length: 80, Avg Reward: -149.531209392, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.5251057148\n",
      "[NOR] Episode: 8740, Length: 72, Avg Reward: -149.926715515, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 251.950378418\n",
      "[NOR] Episode: 8750, Length: 70, Avg Reward: -155.834552947, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.13056087494\n",
      "[NOR] Episode: 8760, Length: 90, Avg Reward: -139.003099188, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.6910171509\n",
      "[NOR] Episode: 8770, Length: 87, Avg Reward: -124.098999259, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.04072141647\n",
      "[NOR] Episode: 8780, Length: 72, Avg Reward: -146.195915639, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0428474545479\n",
      "[NOR] Episode: 8790, Length: 74, Avg Reward: -162.562607727, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.32539844513\n",
      "[NOR] Episode: 8800, Length: 86, Avg Reward: -160.826941024, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.5410349369\n",
      "[NOR] Episode: 8810, Length: 100, Avg Reward: -136.169574879, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.38929128647\n",
      "[NOR] Episode: 8820, Length: 79, Avg Reward: -149.566174007, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.3069896698\n",
      "[NOR] Episode: 8830, Length: 81, Avg Reward: -157.112654399, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 28.3416957855\n",
      "[NOR] Episode: 8840, Length: 136, Avg Reward: -94.9538668817, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 98.8458557129\n",
      "[NOR] Episode: 8850, Length: 320, Avg Reward: -120.406701993, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.16002833843\n",
      "[NOR] Episode: 8860, Length: 120, Avg Reward: -33.9034466362, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.30707347393\n",
      "[NOR] Episode: 8870, Length: 376, Avg Reward: -108.76897319, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.4774227142\n",
      "[NOR] Episode: 8880, Length: 68, Avg Reward: -78.5136811915, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.08919143677\n",
      "[NOR] Episode: 8890, Length: 76, Avg Reward: -124.254351666, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.34689974785\n",
      "[NOR] Episode: 8900, Length: 169, Avg Reward: -118.164073149, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.63333368301\n",
      "[NOR] Episode: 8910, Length: 431, Avg Reward: -51.6945358981, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.77295732498\n",
      "[NOR] Episode: 8920, Length: 92, Avg Reward: -144.146285578, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.90560531616\n",
      "[NOR] Episode: 8930, Length: 90, Avg Reward: -126.410141165, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.86111927032\n",
      "[NOR] Episode: 8940, Length: 91, Avg Reward: -106.660850804, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.69270086288\n",
      "[MAX] Episode: 8946, Length: 269, Reward: 274.274703819, buffer_len: 500000\n",
      "[NOR] Episode: 8950, Length: 100, Avg Reward: -10.1611637771, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.01254463196\n",
      "[NOR] Episode: 8960, Length: 86, Avg Reward: -74.3422137522, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.08865308762\n",
      "[NOR] Episode: 8970, Length: 106, Avg Reward: -97.3138833351, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.10676956177\n",
      "[NOR] Episode: 8980, Length: 273, Avg Reward: -109.931086802, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.20614612103\n",
      "[NOR] Episode: 8990, Length: 136, Avg Reward: -114.988214819, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.56240081787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 04:08:44,771] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video009000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 9000, Length: 121, Avg Reward: -104.058864091, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.72410917282\n",
      "[NOR] Episode: 9010, Length: 179, Avg Reward: -145.385684244, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.505770087242\n",
      "[NOR] Episode: 9020, Length: 147, Avg Reward: -158.295038119, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.6981868744\n",
      "[NOR] Episode: 9030, Length: 84, Avg Reward: -166.812078675, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.61475133896\n",
      "[NOR] Episode: 9040, Length: 160, Avg Reward: -119.432769409, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.9980096817\n",
      "[NOR] Episode: 9050, Length: 119, Avg Reward: -159.771080501, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.22212696075\n",
      "[NOR] Episode: 9060, Length: 154, Avg Reward: -83.135790131, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.1989402771\n",
      "[NOR] Episode: 9070, Length: 125, Avg Reward: -149.18856122, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.037277802825\n",
      "[NOR] Episode: 9080, Length: 96, Avg Reward: -99.4998850771, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.294946551323\n",
      "[NOR] Episode: 9090, Length: 152, Avg Reward: -117.281737447, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.07238936424\n",
      "[NOR] Episode: 9100, Length: 133, Avg Reward: -141.879067589, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.52074432373\n",
      "[NOR] Episode: 9110, Length: 104, Avg Reward: -183.317968825, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.41538906097\n",
      "[NOR] Episode: 9120, Length: 113, Avg Reward: -165.815131503, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.84609079361\n",
      "[NOR] Episode: 9130, Length: 88, Avg Reward: -164.8772839, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.47417259216\n",
      "[NOR] Episode: 9140, Length: 97, Avg Reward: -88.1711311846, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.3838319778\n",
      "[NOR] Episode: 9150, Length: 70, Avg Reward: -144.830865172, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.25811767578\n",
      "[NOR] Episode: 9160, Length: 194, Avg Reward: -135.270612605, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.60998296738\n",
      "[NOR] Episode: 9170, Length: 207, Avg Reward: -177.688309593, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 140.641906738\n",
      "[NOR] Episode: 9180, Length: 159, Avg Reward: -150.512491291, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.081138134\n",
      "[NOR] Episode: 9190, Length: 320, Avg Reward: -101.659678756, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.66069698334\n",
      "[NOR] Episode: 9200, Length: 312, Avg Reward: -107.26637403, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.92706680298\n",
      "[NOR] Episode: 9210, Length: 105, Avg Reward: -106.512966233, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.56385469437\n",
      "[NOR] Episode: 9220, Length: 105, Avg Reward: -172.429148965, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.4593811035\n",
      "[NOR] Episode: 9230, Length: 119, Avg Reward: -144.754664581, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.2071237564\n",
      "[NOR] Episode: 9240, Length: 433, Avg Reward: -164.568925936, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.725247621536\n",
      "[NOR] Episode: 9250, Length: 89, Avg Reward: -167.686483473, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 38.3418998718\n",
      "[NOR] Episode: 9260, Length: 195, Avg Reward: -119.626758443, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.16479635239\n",
      "[NOR] Episode: 9270, Length: 92, Avg Reward: -126.389927863, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.55874919891\n",
      "[NOR] Episode: 9280, Length: 115, Avg Reward: -102.740110125, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.56702327728\n",
      "[NOR] Episode: 9290, Length: 361, Avg Reward: -100.564413543, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.0043077469\n",
      "[NOR] Episode: 9300, Length: 94, Avg Reward: -119.275007266, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.772949337959\n",
      "[NOR] Episode: 9310, Length: 84, Avg Reward: -144.725520233, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.281046867371\n",
      "[NOR] Episode: 9320, Length: 141, Avg Reward: -92.4929049448, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.97590351105\n",
      "[NOR] Episode: 9330, Length: 458, Avg Reward: -117.354350222, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.30782365799\n",
      "[NOR] Episode: 9340, Length: 96, Avg Reward: -148.032238031, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.77496278286\n",
      "[NOR] Episode: 9350, Length: 932, Avg Reward: -67.3937916211, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.46405220032\n",
      "[NOR] Episode: 9360, Length: 1000, Avg Reward: -57.4325459269, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.17095851898\n",
      "[NOR] Episode: 9370, Length: 163, Avg Reward: -20.0488676833, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.5622196198\n",
      "[NOR] Episode: 9380, Length: 120, Avg Reward: -54.8009319174, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 62.4960365295\n",
      "[NOR] Episode: 9390, Length: 63, Avg Reward: -126.538176683, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.0165596008\n",
      "[NOR] Episode: 9400, Length: 151, Avg Reward: -149.602699298, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.25110721588\n",
      "[NOR] Episode: 9410, Length: 129, Avg Reward: -90.4584630644, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.49997544289\n",
      "[NOR] Episode: 9420, Length: 553, Avg Reward: -56.4752207672, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 212.321258545\n",
      "[NOR] Episode: 9430, Length: 1000, Avg Reward: -126.561082, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.88293743134\n",
      "[NOR] Episode: 9440, Length: 1000, Avg Reward: -98.6837867418, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.59438705444\n",
      "[NOR] Episode: 9450, Length: 439, Avg Reward: -47.998829888, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.852195501328\n",
      "[NOR] Episode: 9460, Length: 116, Avg Reward: -106.21967397, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.28625392914\n",
      "[NOR] Episode: 9470, Length: 1000, Avg Reward: -44.828842559, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.24185848236\n",
      "[NOR] Episode: 9480, Length: 84, Avg Reward: -126.797471392, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.33852577209\n",
      "[NOR] Episode: 9490, Length: 323, Avg Reward: -77.8705759524, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.82841587067\n",
      "[NOR] Episode: 9500, Length: 563, Avg Reward: 3.36543068151, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -18.714679718\n",
      "[NOR] Episode: 9510, Length: 1000, Avg Reward: -8.53735651779, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.584803104401\n",
      "[NOR] Episode: 9520, Length: 76, Avg Reward: -16.5481632478, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.398996353149\n",
      "[NOR] Episode: 9530, Length: 72, Avg Reward: -87.1519778007, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.26297783852\n",
      "[NOR] Episode: 9540, Length: 404, Avg Reward: -103.157119725, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.00975465775\n",
      "[NOR] Episode: 9550, Length: 77, Avg Reward: 31.3198124385, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.22700691223\n",
      "[NOR] Episode: 9560, Length: 1000, Avg Reward: -57.7808383759, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.8523826599\n",
      "[NOR] Episode: 9570, Length: 136, Avg Reward: -21.4174333761, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.28230857849\n",
      "[NOR] Episode: 9580, Length: 106, Avg Reward: -91.9404807116, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.57485389709\n",
      "[NOR] Episode: 9590, Length: 91, Avg Reward: -126.868219351, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 241.190155029\n",
      "[NOR] Episode: 9600, Length: 375, Avg Reward: -45.2399561408, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.8495445251\n",
      "[NOR] Episode: 9610, Length: 116, Avg Reward: -66.9518433395, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.4448785782\n",
      "[NOR] Episode: 9620, Length: 603, Avg Reward: -116.426524607, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.446172893047\n",
      "[NOR] Episode: 9630, Length: 82, Avg Reward: -73.7702581775, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.63557481766\n",
      "[NOR] Episode: 9640, Length: 187, Avg Reward: -72.1168933397, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.672310352325\n",
      "[NOR] Episode: 9650, Length: 1000, Avg Reward: -53.2289161357, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.3924036026\n",
      "[NOR] Episode: 9660, Length: 138, Avg Reward: -56.049547087, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.00838899612\n",
      "[NOR] Episode: 9670, Length: 351, Avg Reward: -39.0337109136, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.36754131317\n",
      "[NOR] Episode: 9680, Length: 61, Avg Reward: -114.006618646, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.57512187958\n",
      "[NOR] Episode: 9690, Length: 250, Avg Reward: -119.29250347, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.9692568779\n",
      "[NOR] Episode: 9700, Length: 261, Avg Reward: -99.1808515248, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.8595924377\n",
      "[NOR] Episode: 9710, Length: 344, Avg Reward: -88.4739383212, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.39813029766\n",
      "[NOR] Episode: 9720, Length: 76, Avg Reward: -128.736924082, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.35457706451\n",
      "[NOR] Episode: 9730, Length: 66, Avg Reward: -69.2025998307, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.63227665424\n",
      "[NOR] Episode: 9740, Length: 97, Avg Reward: -84.0243438479, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.139731884\n",
      "[NOR] Episode: 9750, Length: 62, Avg Reward: -74.3386959666, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.03842687607\n",
      "[NOR] Episode: 9760, Length: 64, Avg Reward: -91.1219095947, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.506875753403\n",
      "[NOR] Episode: 9770, Length: 66, Avg Reward: -50.2224861002, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.21258115768\n",
      "[NOR] Episode: 9780, Length: 67, Avg Reward: -105.183252632, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 207.337341309\n",
      "[NOR] Episode: 9790, Length: 109, Avg Reward: -105.320862515, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 57.9437179565\n",
      "[NOR] Episode: 9800, Length: 80, Avg Reward: -39.1398998363, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 58.7998809814\n",
      "[NOR] Episode: 9810, Length: 102, Avg Reward: -70.7867268193, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.828125\n",
      "[NOR] Episode: 9820, Length: 112, Avg Reward: -44.4157021979, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.17465996742\n",
      "[NOR] Episode: 9830, Length: 78, Avg Reward: -88.7727763322, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.6525259018\n",
      "[NOR] Episode: 9840, Length: 99, Avg Reward: -77.8285367947, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.666386842728\n",
      "[NOR] Episode: 9850, Length: 81, Avg Reward: -60.4571022393, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.32834243774\n",
      "[NOR] Episode: 9860, Length: 87, Avg Reward: -72.4531251713, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.7403469086\n",
      "[NOR] Episode: 9870, Length: 90, Avg Reward: -42.4228595946, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.99574279785\n",
      "[NOR] Episode: 9880, Length: 105, Avg Reward: -49.5615205946, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.729917645454\n",
      "[NOR] Episode: 9890, Length: 87, Avg Reward: -46.9481307601, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.71077275276\n",
      "[NOR] Episode: 9900, Length: 187, Avg Reward: -39.5393594389, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.6428070068\n",
      "[NOR] Episode: 9910, Length: 276, Avg Reward: -88.7883304238, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 210.641159058\n",
      "[NOR] Episode: 9920, Length: 153, Avg Reward: -32.2007173473, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.05328583717\n",
      "[NOR] Episode: 9930, Length: 155, Avg Reward: -51.2755135603, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.59060168266\n",
      "[NOR] Episode: 9940, Length: 81, Avg Reward: -41.2121439945, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.73641490936\n",
      "[NOR] Episode: 9950, Length: 282, Avg Reward: -59.6490926538, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.391955375671\n",
      "[NOR] Episode: 9960, Length: 68, Avg Reward: -80.1201629638, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 192.26864624\n",
      "[NOR] Episode: 9970, Length: 111, Avg Reward: -64.1376168432, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 253.247283936\n",
      "[NOR] Episode: 9980, Length: 85, Avg Reward: -83.3574333691, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 101.755844116\n",
      "[NOR] Episode: 9990, Length: 163, Avg Reward: -107.558590733, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.49540328979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 04:24:31,644] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video010000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 10000, Length: 80, Avg Reward: -91.1117474553, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.8566236496\n",
      "[NOR] Episode: 10010, Length: 100, Avg Reward: -103.489104915, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.208065271378\n",
      "[NOR] Episode: 10020, Length: 76, Avg Reward: -95.3220536984, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 428.243469238\n",
      "[NOR] Episode: 10030, Length: 86, Avg Reward: -72.9324111892, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.78845214844\n",
      "[NOR] Episode: 10040, Length: 110, Avg Reward: -73.936028299, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.47647380829\n",
      "[NOR] Episode: 10050, Length: 98, Avg Reward: -54.6643591228, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.4853515625\n",
      "[NOR] Episode: 10060, Length: 127, Avg Reward: -48.4353401342, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.807794332504\n",
      "[NOR] Episode: 10070, Length: 87, Avg Reward: -90.1245910157, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.14870452881\n",
      "[NOR] Episode: 10080, Length: 118, Avg Reward: -60.0887092949, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.97246193886\n",
      "[NOR] Episode: 10090, Length: 81, Avg Reward: -87.6174846983, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.07468175888\n",
      "[NOR] Episode: 10100, Length: 100, Avg Reward: -69.2366726885, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.19450521469\n",
      "[NOR] Episode: 10110, Length: 147, Avg Reward: -27.9291243164, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.98996829987\n",
      "[NOR] Episode: 10120, Length: 105, Avg Reward: -34.6700749366, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.13855886459\n",
      "[NOR] Episode: 10130, Length: 101, Avg Reward: -19.8979949484, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.03723430634\n",
      "[NOR] Episode: 10140, Length: 82, Avg Reward: -115.101120901, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.1748008728\n",
      "[NOR] Episode: 10150, Length: 132, Avg Reward: -100.500722045, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.8335971832\n",
      "[NOR] Episode: 10160, Length: 117, Avg Reward: -67.5692008147, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.469660758972\n",
      "[NOR] Episode: 10170, Length: 163, Avg Reward: -34.8044996359, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.689453601837\n",
      "[NOR] Episode: 10180, Length: 85, Avg Reward: -70.4784776353, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.58532845974\n",
      "[NOR] Episode: 10190, Length: 84, Avg Reward: -97.4433905216, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.0193901062\n",
      "[NOR] Episode: 10200, Length: 116, Avg Reward: -116.03759519, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.36936044693\n",
      "[NOR] Episode: 10210, Length: 134, Avg Reward: -9.92578636705, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.8070659637\n",
      "[NOR] Episode: 10220, Length: 121, Avg Reward: -12.5308550941, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.042283535\n",
      "[NOR] Episode: 10230, Length: 129, Avg Reward: -50.4179220535, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.6587305069\n",
      "[NOR] Episode: 10240, Length: 141, Avg Reward: -24.9626517772, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.49035167694\n",
      "[NOR] Episode: 10250, Length: 99, Avg Reward: 37.9626616179, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.632689476013\n",
      "[NOR] Episode: 10260, Length: 118, Avg Reward: -63.3836183181, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.4118289948\n",
      "[NOR] Episode: 10270, Length: 102, Avg Reward: -97.5229861404, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.76146221161\n",
      "[NOR] Episode: 10280, Length: 96, Avg Reward: -66.8400172073, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.27920913696\n",
      "[NOR] Episode: 10290, Length: 120, Avg Reward: -67.4891317552, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.67181050777\n",
      "[NOR] Episode: 10300, Length: 151, Avg Reward: -67.0174590191, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 194.791351318\n",
      "[NOR] Episode: 10310, Length: 1000, Avg Reward: -50.0218128566, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.81970965862\n",
      "[NOR] Episode: 10320, Length: 102, Avg Reward: -69.6955677035, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.52340888977\n",
      "[NOR] Episode: 10330, Length: 98, Avg Reward: -7.65252804473, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -12.2602539062\n",
      "[NOR] Episode: 10340, Length: 129, Avg Reward: -57.4475313465, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.0084667206\n",
      "[NOR] Episode: 10350, Length: 71, Avg Reward: -45.9840461304, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.938961863518\n",
      "[NOR] Episode: 10360, Length: 99, Avg Reward: -67.24723777, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.16246986389\n",
      "[NOR] Episode: 10370, Length: 122, Avg Reward: -77.817666236, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.17541503906\n",
      "[NOR] Episode: 10380, Length: 171, Avg Reward: -51.024635507, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.15088176727\n",
      "[NOR] Episode: 10390, Length: 125, Avg Reward: -71.4887300123, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.74039268494\n",
      "[NOR] Episode: 10400, Length: 136, Avg Reward: -57.7800797189, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.6698846817\n",
      "[NOR] Episode: 10410, Length: 1000, Avg Reward: -12.8913178325, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.29590415955\n",
      "[NOR] Episode: 10420, Length: 89, Avg Reward: -25.3695098422, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.21874046326\n",
      "[NOR] Episode: 10430, Length: 142, Avg Reward: -48.3612888757, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 200.898529053\n",
      "[NOR] Episode: 10440, Length: 110, Avg Reward: -59.9452681088, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.75595343113\n",
      "[NOR] Episode: 10450, Length: 138, Avg Reward: -64.7824478196, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.62535715103\n",
      "[NOR] Episode: 10460, Length: 165, Avg Reward: -23.0643434417, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.24360227585\n",
      "[NOR] Episode: 10470, Length: 176, Avg Reward: -66.5594112947, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.401950120926\n",
      "[NOR] Episode: 10480, Length: 119, Avg Reward: -58.1919581877, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.51324439049\n",
      "[NOR] Episode: 10490, Length: 152, Avg Reward: -7.53383847028, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.5645656586\n",
      "[NOR] Episode: 10500, Length: 112, Avg Reward: -51.2857166664, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.72871935368\n",
      "[NOR] Episode: 10510, Length: 425, Avg Reward: 10.1282939104, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.762383461\n",
      "[NOR] Episode: 10520, Length: 148, Avg Reward: -25.7728424846, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.57375001907\n",
      "[NOR] Episode: 10530, Length: 132, Avg Reward: -69.6428251182, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.4136447906\n",
      "[NOR] Episode: 10540, Length: 93, Avg Reward: -73.3892489714, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.264188468456\n",
      "[NOR] Episode: 10550, Length: 111, Avg Reward: -56.4497270726, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.92471170425\n",
      "[NOR] Episode: 10560, Length: 128, Avg Reward: -63.76650743, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.98262929916\n",
      "[NOR] Episode: 10570, Length: 164, Avg Reward: -59.5270577245, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.826176524162\n",
      "[NOR] Episode: 10580, Length: 135, Avg Reward: -70.6436361044, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.53388834\n",
      "[NOR] Episode: 10590, Length: 102, Avg Reward: -60.0600418741, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 72.0595703125\n",
      "[NOR] Episode: 10600, Length: 115, Avg Reward: -64.9787838735, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.75829792023\n",
      "[NOR] Episode: 10610, Length: 161, Avg Reward: -54.2631612939, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.38887166977\n",
      "[NOR] Episode: 10620, Length: 136, Avg Reward: -74.8255260397, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.96934223175\n",
      "[NOR] Episode: 10630, Length: 145, Avg Reward: -69.4808265636, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.86401510239\n",
      "[NOR] Episode: 10640, Length: 124, Avg Reward: -54.6251899252, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.82233810425\n",
      "[NOR] Episode: 10650, Length: 130, Avg Reward: -68.1564719883, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.80998110771\n",
      "[NOR] Episode: 10660, Length: 117, Avg Reward: -50.7523479361, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 250.100769043\n",
      "[NOR] Episode: 10670, Length: 153, Avg Reward: -52.7840921272, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 244.214172363\n",
      "[NOR] Episode: 10680, Length: 103, Avg Reward: -67.4923371178, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.64198732376\n",
      "[NOR] Episode: 10690, Length: 96, Avg Reward: -63.7870674928, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.12409687042\n",
      "[NOR] Episode: 10700, Length: 85, Avg Reward: -79.6736405896, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 55.8886947632\n",
      "[NOR] Episode: 10710, Length: 116, Avg Reward: -72.0117914685, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 110.179275513\n",
      "[NOR] Episode: 10720, Length: 102, Avg Reward: -63.0317769266, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.7897143364\n",
      "[NOR] Episode: 10730, Length: 81, Avg Reward: -41.484123739, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 127.462417603\n",
      "[NOR] Episode: 10740, Length: 125, Avg Reward: -67.9856023776, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.3347425461\n",
      "[NOR] Episode: 10750, Length: 164, Avg Reward: -60.8224623088, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.63665771484\n",
      "[NOR] Episode: 10760, Length: 131, Avg Reward: -53.6307169637, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.0485935211\n",
      "[NOR] Episode: 10770, Length: 131, Avg Reward: -59.2701661363, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 104.550384521\n",
      "[NOR] Episode: 10780, Length: 120, Avg Reward: -70.0220684053, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.43033432961\n",
      "[NOR] Episode: 10790, Length: 138, Avg Reward: -79.4858284061, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 200.973297119\n",
      "[NOR] Episode: 10800, Length: 115, Avg Reward: -74.7429078445, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 144.671020508\n",
      "[NOR] Episode: 10810, Length: 113, Avg Reward: -88.0179042164, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.86499762535\n",
      "[NOR] Episode: 10820, Length: 148, Avg Reward: -56.4648097862, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.41096115112\n",
      "[NOR] Episode: 10830, Length: 97, Avg Reward: -55.1526386302, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.1651477814\n",
      "[NOR] Episode: 10840, Length: 133, Avg Reward: -91.6150635874, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 31.252872467\n",
      "[NOR] Episode: 10850, Length: 118, Avg Reward: -97.6896818271, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.14700770378\n",
      "[NOR] Episode: 10860, Length: 97, Avg Reward: -113.810495168, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 252.060852051\n",
      "[NOR] Episode: 10870, Length: 100, Avg Reward: -83.4559097749, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 25.4298744202\n",
      "[NOR] Episode: 10880, Length: 88, Avg Reward: -85.6059267782, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 248.768295288\n",
      "[NOR] Episode: 10890, Length: 119, Avg Reward: -63.0125493597, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 252.959381104\n",
      "[NOR] Episode: 10900, Length: 86, Avg Reward: -76.6805273341, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.79595732689\n",
      "[NOR] Episode: 10910, Length: 109, Avg Reward: -66.0682139444, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.829881191254\n",
      "[NOR] Episode: 10920, Length: 110, Avg Reward: -65.517799254, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.29457950592\n",
      "[NOR] Episode: 10930, Length: 119, Avg Reward: -56.8206260952, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.603679656982\n",
      "[NOR] Episode: 10940, Length: 73, Avg Reward: -72.3740782194, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.3499145508\n",
      "[NOR] Episode: 10950, Length: 108, Avg Reward: -83.1703290369, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.33195781708\n",
      "[NOR] Episode: 10960, Length: 91, Avg Reward: -70.6094166298, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.0410785675\n",
      "[NOR] Episode: 10970, Length: 134, Avg Reward: -64.0292569397, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.25908470154\n",
      "[NOR] Episode: 10980, Length: 97, Avg Reward: -56.5315117992, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 275.005554199\n",
      "[NOR] Episode: 10990, Length: 111, Avg Reward: -48.3110241686, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.50564718246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 04:33:02,867] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video011000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 11000, Length: 126, Avg Reward: -30.4837881249, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.64176571369\n",
      "[NOR] Episode: 11010, Length: 135, Avg Reward: -56.2182954742, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 256.802246094\n",
      "[NOR] Episode: 11020, Length: 152, Avg Reward: -66.5297731242, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.7146186829\n",
      "[NOR] Episode: 11030, Length: 123, Avg Reward: -66.0994642878, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 36.2937927246\n",
      "[NOR] Episode: 11040, Length: 160, Avg Reward: -64.3764595632, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 451.261810303\n",
      "[NOR] Episode: 11050, Length: 183, Avg Reward: -77.334538514, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.01342797279\n",
      "[NOR] Episode: 11060, Length: 127, Avg Reward: -56.9695715041, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.10449314117\n",
      "[NOR] Episode: 11070, Length: 135, Avg Reward: -92.1308036943, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.7278470993\n",
      "[NOR] Episode: 11080, Length: 136, Avg Reward: -80.9936851872, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 244.346237183\n",
      "[NOR] Episode: 11090, Length: 139, Avg Reward: -113.964687751, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 207.525299072\n",
      "[NOR] Episode: 11100, Length: 119, Avg Reward: -75.3271125121, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 187.825164795\n",
      "[NOR] Episode: 11110, Length: 118, Avg Reward: -71.392465497, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.29153990746\n",
      "[NOR] Episode: 11120, Length: 117, Avg Reward: -82.5630891524, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.43129587173\n",
      "[NOR] Episode: 11130, Length: 128, Avg Reward: -58.9057035148, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.44507312775\n",
      "[NOR] Episode: 11140, Length: 134, Avg Reward: -80.0556873629, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.918956041336\n",
      "[NOR] Episode: 11150, Length: 140, Avg Reward: -38.7848366123, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.956368207932\n",
      "[NOR] Episode: 11160, Length: 104, Avg Reward: -49.8762640129, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 66.4748535156\n",
      "[NOR] Episode: 11170, Length: 123, Avg Reward: -83.9162603022, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.69787693024\n",
      "[NOR] Episode: 11180, Length: 102, Avg Reward: -67.5812199545, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.4014568329\n",
      "[NOR] Episode: 11190, Length: 174, Avg Reward: -73.6619947861, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.21208262444\n",
      "[NOR] Episode: 11200, Length: 121, Avg Reward: -46.9244578731, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 94.0429534912\n",
      "[NOR] Episode: 11210, Length: 159, Avg Reward: -63.9575139396, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.657289505\n",
      "[NOR] Episode: 11220, Length: 136, Avg Reward: -27.4299239575, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.9422249794\n",
      "[NOR] Episode: 11230, Length: 116, Avg Reward: -8.37274350167, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.35520935059\n",
      "[NOR] Episode: 11240, Length: 126, Avg Reward: -48.5436140875, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.48848843575\n",
      "[NOR] Episode: 11250, Length: 131, Avg Reward: -54.7187344327, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.87220478058\n",
      "[NOR] Episode: 11260, Length: 419, Avg Reward: -27.3720442679, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 46.9971313477\n",
      "[NOR] Episode: 11270, Length: 1000, Avg Reward: -16.3079823205, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.5108585358\n",
      "[NOR] Episode: 11280, Length: 154, Avg Reward: -60.0682351095, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.1688022614\n",
      "[NOR] Episode: 11290, Length: 170, Avg Reward: -54.3215894588, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.25813937187\n",
      "[NOR] Episode: 11300, Length: 193, Avg Reward: -45.6203205179, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 251.216888428\n",
      "[NOR] Episode: 11310, Length: 145, Avg Reward: -53.7988037151, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.79284071922\n",
      "[NOR] Episode: 11320, Length: 232, Avg Reward: -48.6333851433, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.5036010742\n",
      "[NOR] Episode: 11330, Length: 144, Avg Reward: -53.2081013447, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.882139384747\n",
      "[NOR] Episode: 11340, Length: 167, Avg Reward: -53.2111286544, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.74814605713\n",
      "[NOR] Episode: 11350, Length: 161, Avg Reward: -13.0917107411, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.07538509369\n",
      "[NOR] Episode: 11360, Length: 508, Avg Reward: -26.4406914754, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.13669157028\n",
      "[NOR] Episode: 11370, Length: 122, Avg Reward: -93.0568301818, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.2477531433\n",
      "[NOR] Episode: 11380, Length: 1000, Avg Reward: -45.1988716105, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 74.9772796631\n",
      "[NOR] Episode: 11390, Length: 1000, Avg Reward: -83.5489652239, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.3457603455\n",
      "[NOR] Episode: 11400, Length: 209, Avg Reward: -81.3718498546, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.97412586212\n",
      "[NOR] Episode: 11410, Length: 1000, Avg Reward: -62.684838712, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 128.398483276\n",
      "[NOR] Episode: 11420, Length: 632, Avg Reward: -34.8605647021, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.76233530045\n",
      "[NOR] Episode: 11430, Length: 1000, Avg Reward: -75.9079907578, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.12194824219\n",
      "[NOR] Episode: 11440, Length: 335, Avg Reward: -50.945375387, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.0173406601\n",
      "[NOR] Episode: 11450, Length: 1000, Avg Reward: -90.7311832753, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.59883332253\n",
      "[NOR] Episode: 11460, Length: 717, Avg Reward: -28.6222427738, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.73827648163\n",
      "[NOR] Episode: 11470, Length: 536, Avg Reward: 27.9073531897, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.0960302353\n",
      "[NOR] Episode: 11480, Length: 217, Avg Reward: -37.5426663309, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.50592684746\n",
      "[NOR] Episode: 11490, Length: 256, Avg Reward: -30.377069149, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.9668197632\n",
      "[NOR] Episode: 11500, Length: 169, Avg Reward: -47.8065538654, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.8385028839\n",
      "[NOR] Episode: 11510, Length: 721, Avg Reward: -82.4953040801, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.52643692493\n",
      "[NOR] Episode: 11520, Length: 318, Avg Reward: -78.3157200034, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 250.079345703\n",
      "[NOR] Episode: 11530, Length: 315, Avg Reward: -106.386341378, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.02189683914\n",
      "[NOR] Episode: 11540, Length: 237, Avg Reward: -104.473960148, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.72586941719\n",
      "[NOR] Episode: 11550, Length: 1000, Avg Reward: -29.9577287115, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.29159832001\n",
      "[NOR] Episode: 11560, Length: 1000, Avg Reward: -65.9025073803, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.4226074219\n",
      "[NOR] Episode: 11570, Length: 345, Avg Reward: -132.045306418, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 32.7553405762\n",
      "[NOR] Episode: 11580, Length: 930, Avg Reward: -85.8392833569, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 27.4024047852\n",
      "[NOR] Episode: 11590, Length: 1000, Avg Reward: -100.729652938, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.70982933044\n",
      "[NOR] Episode: 11600, Length: 752, Avg Reward: -107.755311379, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.12505197525\n",
      "[NOR] Episode: 11610, Length: 522, Avg Reward: -150.818797424, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.37365245819\n",
      "[NOR] Episode: 11620, Length: 1000, Avg Reward: -146.323150649, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.58746767044\n",
      "[NOR] Episode: 11630, Length: 1000, Avg Reward: -101.954527292, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.32485294342\n",
      "[NOR] Episode: 11640, Length: 1000, Avg Reward: -106.173643071, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.71223497391\n",
      "[NOR] Episode: 11650, Length: 602, Avg Reward: -170.912038681, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 241.598739624\n",
      "[NOR] Episode: 11660, Length: 1000, Avg Reward: -111.677778889, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.09276199341\n",
      "[NOR] Episode: 11670, Length: 1000, Avg Reward: -109.494365531, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.11685657501\n",
      "[NOR] Episode: 11680, Length: 1000, Avg Reward: -37.3467467076, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.543446064\n",
      "[NOR] Episode: 11690, Length: 1000, Avg Reward: -138.075019796, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.16998195648\n",
      "[NOR] Episode: 11700, Length: 1000, Avg Reward: -90.3434929264, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.6498260498\n",
      "[NOR] Episode: 11710, Length: 934, Avg Reward: -141.91032189, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.46639728546\n",
      "[NOR] Episode: 11720, Length: 693, Avg Reward: -103.937614417, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.373751997948\n",
      "[NOR] Episode: 11730, Length: 1000, Avg Reward: -89.302441289, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0308678150177\n",
      "[NOR] Episode: 11740, Length: 1000, Avg Reward: -102.265261532, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.9649913311\n",
      "[NOR] Episode: 11750, Length: 1000, Avg Reward: -98.3706185923, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.24013948441\n",
      "[NOR] Episode: 11760, Length: 1000, Avg Reward: -83.2279282229, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.41742992401\n",
      "[NOR] Episode: 11770, Length: 1000, Avg Reward: -157.958899736, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.22844815254\n",
      "[NOR] Episode: 11780, Length: 1000, Avg Reward: -129.597511846, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.433394610882\n",
      "[NOR] Episode: 11790, Length: 674, Avg Reward: -192.570448659, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.40967726707\n",
      "[NOR] Episode: 11800, Length: 376, Avg Reward: -154.15138357, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.8411426544\n",
      "[NOR] Episode: 11810, Length: 227, Avg Reward: -162.587147863, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.850164294243\n",
      "[NOR] Episode: 11820, Length: 285, Avg Reward: -171.465052014, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.1564445496\n",
      "[NOR] Episode: 11830, Length: 260, Avg Reward: -164.653262361, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.43433952332\n",
      "[NOR] Episode: 11840, Length: 120, Avg Reward: -169.343529801, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.9991340637\n",
      "[NOR] Episode: 11850, Length: 343, Avg Reward: -130.222529484, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.69322538376\n",
      "[NOR] Episode: 11860, Length: 1000, Avg Reward: -162.860227595, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 256.172393799\n",
      "[NOR] Episode: 11870, Length: 355, Avg Reward: -151.397106488, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.19414329529\n",
      "[NOR] Episode: 11880, Length: 509, Avg Reward: -147.951440677, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.31442308426\n",
      "[NOR] Episode: 11890, Length: 229, Avg Reward: -146.775052829, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.609951972961\n",
      "[NOR] Episode: 11900, Length: 328, Avg Reward: -151.786606958, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.06133127213\n",
      "[NOR] Episode: 11910, Length: 305, Avg Reward: -159.261187015, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.44518470764\n",
      "[NOR] Episode: 11920, Length: 284, Avg Reward: -44.2375147025, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 215.226013184\n",
      "[NOR] Episode: 11930, Length: 360, Avg Reward: -92.5372999025, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.44219350815\n",
      "[NOR] Episode: 11940, Length: 151, Avg Reward: -12.4917202798, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.74604892731\n",
      "[NOR] Episode: 11950, Length: 281, Avg Reward: -40.0236612215, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.427407622337\n",
      "[NOR] Episode: 11960, Length: 695, Avg Reward: 24.2013331079, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.66627883911\n",
      "[NOR] Episode: 11970, Length: 345, Avg Reward: 27.3114022631, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.04293560982\n",
      "[NOR] Episode: 11980, Length: 368, Avg Reward: -50.2944330318, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 360.49786377\n",
      "[NOR] Episode: 11990, Length: 204, Avg Reward: 38.0900616275, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 219.101501465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 05:04:04,325] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video012000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 12000, Length: 178, Avg Reward: 71.2067680089, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.461243331432\n",
      "[NOR] Episode: 12010, Length: 355, Avg Reward: 48.6042117273, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.96863365173\n",
      "[NOR] Episode: 12020, Length: 334, Avg Reward: 53.6741748782, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.51088178158\n",
      "[NOR] Episode: 12030, Length: 442, Avg Reward: 63.4174714897, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.75840353966\n",
      "[NOR] Episode: 12040, Length: 485, Avg Reward: 50.3301336574, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.51859402657\n",
      "[NOR] Episode: 12050, Length: 560, Avg Reward: 43.6976344017, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.02654409409\n",
      "[NOR] Episode: 12060, Length: 1000, Avg Reward: -7.6061976676, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.1010808945\n",
      "[NOR] Episode: 12070, Length: 514, Avg Reward: 126.74896199, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.11027956009\n",
      "[NOR] Episode: 12080, Length: 942, Avg Reward: 86.390226438, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.21896576881\n",
      "[NOR] Episode: 12090, Length: 1000, Avg Reward: 122.052645532, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.917165100574\n",
      "[NOR] Episode: 12100, Length: 1000, Avg Reward: 26.2741346103, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.3997430801\n",
      "[NOR] Episode: 12110, Length: 575, Avg Reward: 87.112179814, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.50415229797\n",
      "[NOR] Episode: 12120, Length: 657, Avg Reward: 113.359899143, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.6315956116\n",
      "[NOR] Episode: 12130, Length: 520, Avg Reward: 148.095302185, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 253.284057617\n",
      "[NOR] Episode: 12140, Length: 1000, Avg Reward: 97.2280052601, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.55178570747\n",
      "[NOR] Episode: 12150, Length: 1000, Avg Reward: 132.991557574, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 271.906982422\n",
      "[NOR] Episode: 12160, Length: 279, Avg Reward: 152.980160315, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.9702854156\n",
      "[NOR] Episode: 12170, Length: 256, Avg Reward: 128.847370483, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.408230781555\n",
      "[NOR] Episode: 12180, Length: 1000, Avg Reward: 156.438629777, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.8553123474\n",
      "[NOR] Episode: 12190, Length: 538, Avg Reward: 111.105130746, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.24053502083\n",
      "[NOR] Episode: 12200, Length: 143, Avg Reward: 121.939242989, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.68085074425\n",
      "[NOR] Episode: 12210, Length: 78, Avg Reward: 132.895005839, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.222399950027\n",
      "[NOR] Episode: 12220, Length: 748, Avg Reward: 96.646644399, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 206.85975647\n",
      "[NOR] Episode: 12230, Length: 129, Avg Reward: 101.637997666, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.66598439217\n",
      "[NOR] Episode: 12240, Length: 211, Avg Reward: 160.532189024, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.3481760025\n",
      "[NOR] Episode: 12250, Length: 153, Avg Reward: 162.984030024, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.11731147766\n",
      "[NOR] Episode: 12260, Length: 241, Avg Reward: 175.875471758, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.856044650078\n",
      "[NOR] Episode: 12270, Length: 156, Avg Reward: 170.919054835, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.120174944401\n",
      "[NOR] Episode: 12280, Length: 282, Avg Reward: 85.3484318356, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.83179831505\n",
      "[NOR] Episode: 12290, Length: 163, Avg Reward: 29.4633538339, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.29671502113\n",
      "[NOR] Episode: 12300, Length: 152, Avg Reward: 42.8762289729, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.72586607933\n",
      "[NOR] Episode: 12310, Length: 139, Avg Reward: 147.596791576, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.66346263885\n",
      "[NOR] Episode: 12320, Length: 156, Avg Reward: -19.8248459376, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.5726890564\n",
      "[NOR] Episode: 12330, Length: 267, Avg Reward: 124.557346919, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.09281253815\n",
      "[NOR] Episode: 12340, Length: 152, Avg Reward: 130.873898254, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.6135559082\n",
      "[NOR] Episode: 12350, Length: 129, Avg Reward: 132.839569549, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.636138379574\n",
      "[NOR] Episode: 12360, Length: 268, Avg Reward: 147.173490866, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 253.668487549\n",
      "[NOR] Episode: 12370, Length: 278, Avg Reward: 187.420642904, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.88214015961\n",
      "[NOR] Episode: 12380, Length: 1000, Avg Reward: 94.6469364895, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.83082389832\n",
      "[NOR] Episode: 12390, Length: 561, Avg Reward: 91.6574121733, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.12448310852\n",
      "[NOR] Episode: 12400, Length: 113, Avg Reward: 102.602411001, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.21247398853\n",
      "[NOR] Episode: 12410, Length: 65, Avg Reward: 7.31948113212, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.37854766846\n",
      "[NOR] Episode: 12420, Length: 178, Avg Reward: 156.189258724, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.78768205643\n",
      "[NOR] Episode: 12430, Length: 99, Avg Reward: 7.92482759413, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.64068460464\n",
      "[NOR] Episode: 12440, Length: 126, Avg Reward: 62.849164599, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.89265537262\n",
      "[NOR] Episode: 12450, Length: 264, Avg Reward: 187.322305519, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.8836183548\n",
      "[NOR] Episode: 12460, Length: 224, Avg Reward: 190.213281985, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.12756443024\n",
      "[NOR] Episode: 12470, Length: 298, Avg Reward: 166.403333725, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.15291643143\n",
      "[NOR] Episode: 12480, Length: 173, Avg Reward: 110.930953492, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 240.715255737\n",
      "[NOR] Episode: 12490, Length: 269, Avg Reward: 209.1567422, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.01233959198\n",
      "[NOR] Episode: 12500, Length: 456, Avg Reward: 123.210293112, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.58627748489\n",
      "[NOR] Episode: 12510, Length: 398, Avg Reward: 148.376120162, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.52404165268\n",
      "[NOR] Episode: 12520, Length: 192, Avg Reward: 191.249145029, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.338968277\n",
      "[NOR] Episode: 12530, Length: 465, Avg Reward: 151.250889308, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.95799553394\n",
      "[NOR] Episode: 12540, Length: 761, Avg Reward: 123.233194572, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.652942180634\n",
      "[NOR] Episode: 12550, Length: 937, Avg Reward: 147.345289854, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.44799470901\n",
      "[NOR] Episode: 12560, Length: 1000, Avg Reward: 179.894245924, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.13550925255\n",
      "[NOR] Episode: 12570, Length: 286, Avg Reward: 104.104352161, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.666888237\n",
      "[NOR] Episode: 12580, Length: 167, Avg Reward: -60.8788776508, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.71894931793\n",
      "[NOR] Episode: 12590, Length: 339, Avg Reward: 114.501483057, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.84476494789\n",
      "[NOR] Episode: 12600, Length: 402, Avg Reward: 206.143475077, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.06855010986\n",
      "[NOR] Episode: 12610, Length: 531, Avg Reward: 156.026345528, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.19206893444\n",
      "[NOR] Episode: 12620, Length: 996, Avg Reward: 158.632274066, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.23130893707\n",
      "[NOR] Episode: 12630, Length: 230, Avg Reward: 148.837244042, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.97992682457\n",
      "[NOR] Episode: 12640, Length: 1000, Avg Reward: 99.4877823827, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.10678482056\n",
      "[NOR] Episode: 12650, Length: 305, Avg Reward: 102.41207011, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.4877603054\n",
      "[NOR] Episode: 12660, Length: 1000, Avg Reward: 121.351724185, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.6062245369\n",
      "[NOR] Episode: 12670, Length: 1000, Avg Reward: 52.3362601585, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.11585974693\n",
      "[NOR] Episode: 12680, Length: 141, Avg Reward: 66.6165123595, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.23782420158\n",
      "[NOR] Episode: 12690, Length: 176, Avg Reward: 181.026536462, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.90503120422\n",
      "[NOR] Episode: 12700, Length: 157, Avg Reward: 164.663876538, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.93218326569\n",
      "[NOR] Episode: 12710, Length: 477, Avg Reward: 61.924704527, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.21254730225\n",
      "[NOR] Episode: 12720, Length: 147, Avg Reward: 86.4109193779, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.912963151932\n",
      "[NOR] Episode: 12730, Length: 145, Avg Reward: 95.3732520541, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.35120916367\n",
      "[NOR] Episode: 12740, Length: 848, Avg Reward: 114.807934688, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.4247398376\n",
      "[NOR] Episode: 12750, Length: 195, Avg Reward: -16.9881070029, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.01591229439\n",
      "[NOR] Episode: 12760, Length: 83, Avg Reward: 45.4527525414, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.20017147064\n",
      "[NOR] Episode: 12770, Length: 138, Avg Reward: 69.8188801002, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.29624414444\n",
      "[NOR] Episode: 12780, Length: 163, Avg Reward: 46.6117458, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.01972293854\n",
      "[NOR] Episode: 12790, Length: 168, Avg Reward: 81.8980715687, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.00722885132\n",
      "[NOR] Episode: 12800, Length: 1000, Avg Reward: 74.1348341409, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.19617557526\n",
      "[NOR] Episode: 12810, Length: 822, Avg Reward: 58.6927942546, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.20327377319\n",
      "[NOR] Episode: 12820, Length: 532, Avg Reward: 52.6175735806, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.6529836655\n",
      "[NOR] Episode: 12830, Length: 326, Avg Reward: 127.263651702, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.931098938\n",
      "[NOR] Episode: 12840, Length: 402, Avg Reward: 169.713535934, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.29992675781\n",
      "[NOR] Episode: 12850, Length: 588, Avg Reward: 145.594401326, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.376132249832\n",
      "[NOR] Episode: 12860, Length: 1000, Avg Reward: 38.2320877378, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.8264799118\n",
      "[NOR] Episode: 12870, Length: 1000, Avg Reward: -97.0255373429, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 270.927062988\n",
      "[NOR] Episode: 12880, Length: 1000, Avg Reward: -28.3520993176, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.1314067841\n",
      "[NOR] Episode: 12890, Length: 1000, Avg Reward: 40.496981875, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.93703937531\n",
      "[NOR] Episode: 12900, Length: 518, Avg Reward: -10.2055051117, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.42658555508\n",
      "[NOR] Episode: 12910, Length: 1000, Avg Reward: 54.7444020625, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.47566366196\n",
      "[NOR] Episode: 12920, Length: 192, Avg Reward: -4.44904094156, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.79688358307\n",
      "[NOR] Episode: 12930, Length: 776, Avg Reward: 19.3091767467, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.54737186432\n",
      "[NOR] Episode: 12940, Length: 1000, Avg Reward: -31.74394468, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.6520023346\n",
      "[NOR] Episode: 12950, Length: 801, Avg Reward: -36.5789818031, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.11159753799\n",
      "[NOR] Episode: 12960, Length: 542, Avg Reward: 7.32946636868, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 251.725143433\n",
      "[NOR] Episode: 12970, Length: 714, Avg Reward: -51.5388105654, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.1289138794\n",
      "[NOR] Episode: 12980, Length: 516, Avg Reward: -72.7472544297, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.66815376282\n",
      "[NOR] Episode: 12990, Length: 110, Avg Reward: -42.0857635161, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.38326263428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 05:39:12,931] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video013000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 13000, Length: 102, Avg Reward: -351.289258453, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.76350831985\n",
      "[NOR] Episode: 13010, Length: 226, Avg Reward: -237.589084261, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.0078706741\n",
      "[NOR] Episode: 13020, Length: 536, Avg Reward: -169.912796816, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.7649769783\n",
      "[NOR] Episode: 13030, Length: 193, Avg Reward: -108.738520687, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.76503944397\n",
      "[NOR] Episode: 13040, Length: 516, Avg Reward: 68.7660534386, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.35069274902\n",
      "[NOR] Episode: 13050, Length: 217, Avg Reward: 79.6407458651, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.67413520813\n",
      "[NOR] Episode: 13060, Length: 226, Avg Reward: 19.7544040272, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.31791782379\n",
      "[NOR] Episode: 13070, Length: 153, Avg Reward: -136.205555481, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.1765213013\n",
      "[NOR] Episode: 13080, Length: 105, Avg Reward: -277.784681221, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.53602790833\n",
      "[NOR] Episode: 13090, Length: 107, Avg Reward: -325.81878264, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.94646596909\n",
      "[NOR] Episode: 13100, Length: 111, Avg Reward: -326.631609868, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.43698167801\n",
      "[NOR] Episode: 13110, Length: 148, Avg Reward: -396.209457303, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.6817340851\n",
      "[NOR] Episode: 13120, Length: 148, Avg Reward: -411.819748891, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.62873029709\n",
      "[NOR] Episode: 13130, Length: 163, Avg Reward: -277.34960991, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.84120178223\n",
      "[NOR] Episode: 13140, Length: 124, Avg Reward: -338.937496611, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.6024780273\n",
      "[NOR] Episode: 13150, Length: 230, Avg Reward: -280.330864674, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.782766819\n",
      "[NOR] Episode: 13160, Length: 135, Avg Reward: -158.791804336, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 235.204238892\n",
      "[NOR] Episode: 13170, Length: 100, Avg Reward: -270.337078005, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.70108890533\n",
      "[NOR] Episode: 13180, Length: 128, Avg Reward: -286.644838578, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.39918470383\n",
      "[NOR] Episode: 13190, Length: 108, Avg Reward: -475.324709675, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.88640403748\n",
      "[NOR] Episode: 13200, Length: 130, Avg Reward: -322.417839587, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.54132711887\n",
      "[NOR] Episode: 13210, Length: 117, Avg Reward: -352.085576051, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.34556531906\n",
      "[NOR] Episode: 13220, Length: 102, Avg Reward: -478.354599368, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.853574752808\n",
      "[NOR] Episode: 13230, Length: 136, Avg Reward: -445.554613213, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.2295370102\n",
      "[NOR] Episode: 13240, Length: 77, Avg Reward: -461.104637168, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 372.32913208\n",
      "[NOR] Episode: 13250, Length: 89, Avg Reward: -437.056082609, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.5034570694\n",
      "[NOR] Episode: 13260, Length: 112, Avg Reward: -435.007568386, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.21852302551\n",
      "[NOR] Episode: 13270, Length: 89, Avg Reward: -489.493900775, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.50469374657\n",
      "[NOR] Episode: 13280, Length: 109, Avg Reward: -481.782764758, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.146064758301\n",
      "[NOR] Episode: 13290, Length: 87, Avg Reward: -461.645320568, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.97106862068\n",
      "[NOR] Episode: 13300, Length: 132, Avg Reward: -479.798812319, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.3301429749\n",
      "[NOR] Episode: 13310, Length: 105, Avg Reward: -508.084113565, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.95720100403\n",
      "[NOR] Episode: 13320, Length: 77, Avg Reward: -462.271573302, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.32480621338\n",
      "[NOR] Episode: 13330, Length: 93, Avg Reward: -476.001701807, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.8075275421\n",
      "[NOR] Episode: 13340, Length: 152, Avg Reward: -454.114954587, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.53402090073\n",
      "[NOR] Episode: 13350, Length: 155, Avg Reward: -421.914715447, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.1736898422\n",
      "[NOR] Episode: 13360, Length: 101, Avg Reward: -419.126817814, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.12487816811\n",
      "[NOR] Episode: 13370, Length: 86, Avg Reward: -429.657753286, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.669519066811\n",
      "[NOR] Episode: 13380, Length: 103, Avg Reward: -442.410707431, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -150.056838989\n",
      "[NOR] Episode: 13390, Length: 120, Avg Reward: -464.59956682, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.94742584229\n",
      "[NOR] Episode: 13400, Length: 160, Avg Reward: -326.33421945, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.8848228455\n",
      "[NOR] Episode: 13410, Length: 100, Avg Reward: -429.923649959, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -15.2670059204\n",
      "[NOR] Episode: 13420, Length: 100, Avg Reward: -455.509539645, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 179.526428223\n",
      "[NOR] Episode: 13430, Length: 120, Avg Reward: -402.794098607, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.09603071213\n",
      "[NOR] Episode: 13440, Length: 110, Avg Reward: -465.861724582, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 243.239059448\n",
      "[NOR] Episode: 13450, Length: 83, Avg Reward: -387.236227226, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.694858551\n",
      "[NOR] Episode: 13460, Length: 125, Avg Reward: -284.098185506, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 212.57333374\n",
      "[NOR] Episode: 13470, Length: 87, Avg Reward: -360.592123397, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 220.266677856\n",
      "[NOR] Episode: 13480, Length: 100, Avg Reward: -246.38028669, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.9424419403\n",
      "[NOR] Episode: 13490, Length: 108, Avg Reward: -282.257513142, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.02968788147\n",
      "[NOR] Episode: 13500, Length: 252, Avg Reward: -291.813826079, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 25.0226135254\n",
      "[NOR] Episode: 13510, Length: 118, Avg Reward: -321.331913615, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.36264324188\n",
      "[NOR] Episode: 13520, Length: 81, Avg Reward: -359.993777255, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.48878669739\n",
      "[NOR] Episode: 13530, Length: 112, Avg Reward: -475.478210055, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.16255187988\n",
      "[NOR] Episode: 13540, Length: 79, Avg Reward: -460.474885796, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.74407577515\n",
      "[NOR] Episode: 13550, Length: 111, Avg Reward: -449.048230449, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 29.831867218\n",
      "[NOR] Episode: 13560, Length: 128, Avg Reward: -503.10458002, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.10982608795\n",
      "[NOR] Episode: 13570, Length: 103, Avg Reward: -551.802949937, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.92380619049\n",
      "[NOR] Episode: 13580, Length: 80, Avg Reward: -528.926202906, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.539932251\n",
      "[NOR] Episode: 13590, Length: 84, Avg Reward: -437.928449374, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.2317171097\n",
      "[NOR] Episode: 13600, Length: 133, Avg Reward: -441.55162931, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.43147182465\n",
      "[NOR] Episode: 13610, Length: 106, Avg Reward: -341.658555407, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.5533943176\n",
      "[NOR] Episode: 13620, Length: 149, Avg Reward: -390.235200303, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.340679883957\n",
      "[NOR] Episode: 13630, Length: 107, Avg Reward: -394.335558203, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.59418487549\n",
      "[NOR] Episode: 13640, Length: 116, Avg Reward: -244.798791952, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.730710983276\n",
      "[NOR] Episode: 13650, Length: 92, Avg Reward: -171.807763734, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.24465608597\n",
      "[NOR] Episode: 13660, Length: 94, Avg Reward: -191.887249378, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.447655677795\n",
      "[NOR] Episode: 13670, Length: 99, Avg Reward: -110.674797054, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 153.897613525\n",
      "[NOR] Episode: 13680, Length: 79, Avg Reward: -116.13600712, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.52642059326\n",
      "[NOR] Episode: 13690, Length: 92, Avg Reward: -105.702869133, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.5893754959\n",
      "[NOR] Episode: 13700, Length: 96, Avg Reward: -152.600883642, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 317.480895996\n",
      "[NOR] Episode: 13710, Length: 118, Avg Reward: -147.973806265, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.875430583954\n",
      "[NOR] Episode: 13720, Length: 70, Avg Reward: -213.391713672, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.60553121567\n",
      "[NOR] Episode: 13730, Length: 198, Avg Reward: -107.616734979, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.9780712128\n",
      "[NOR] Episode: 13740, Length: 107, Avg Reward: -81.3558765864, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.27498340607\n",
      "[NOR] Episode: 13750, Length: 165, Avg Reward: -101.265628099, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 81.2905273438\n",
      "[NOR] Episode: 13760, Length: 71, Avg Reward: -85.2493948849, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -16.3867950439\n",
      "[NOR] Episode: 13770, Length: 71, Avg Reward: -81.5039099701, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.61417174339\n",
      "[NOR] Episode: 13780, Length: 78, Avg Reward: -78.919686255, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.09801673889\n",
      "[NOR] Episode: 13790, Length: 71, Avg Reward: -90.4847878038, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.48726892471\n",
      "[NOR] Episode: 13800, Length: 1000, Avg Reward: -50.929545655, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.93054938316\n",
      "[NOR] Episode: 13810, Length: 1000, Avg Reward: -12.868970624, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.6846199036\n",
      "[NOR] Episode: 13820, Length: 94, Avg Reward: -25.2882815225, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.58592653275\n",
      "[NOR] Episode: 13830, Length: 503, Avg Reward: -17.8750781917, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.07948493958\n",
      "[NOR] Episode: 13840, Length: 1000, Avg Reward: -6.77213875216, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.84922933578\n",
      "[NOR] Episode: 13850, Length: 84, Avg Reward: -2.66039540874, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.49141979218\n",
      "[NOR] Episode: 13860, Length: 96, Avg Reward: -14.0004149491, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.18557584286\n",
      "[NOR] Episode: 13870, Length: 388, Avg Reward: 16.5241504103, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.09243285656\n",
      "[NOR] Episode: 13880, Length: 104, Avg Reward: -68.9676742444, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.56203317642\n",
      "[NOR] Episode: 13890, Length: 58, Avg Reward: -68.7015192859, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.30576753616\n",
      "[NOR] Episode: 13900, Length: 84, Avg Reward: -47.7172062225, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.4837751389\n",
      "[NOR] Episode: 13910, Length: 171, Avg Reward: 31.2423250684, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.03252887726\n",
      "[NOR] Episode: 13920, Length: 80, Avg Reward: -69.0131574256, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.2182922363\n",
      "[NOR] Episode: 13930, Length: 74, Avg Reward: -72.8786484848, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.91406726837\n",
      "[NOR] Episode: 13940, Length: 161, Avg Reward: -26.181987067, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.14322376251\n",
      "[NOR] Episode: 13950, Length: 72, Avg Reward: -46.4004057369, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.48436546326\n",
      "[NOR] Episode: 13960, Length: 90, Avg Reward: -81.6953777078, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.19615840912\n",
      "[NOR] Episode: 13970, Length: 61, Avg Reward: -87.4038377883, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.3916969299\n",
      "[NOR] Episode: 13980, Length: 91, Avg Reward: -21.7115178749, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.44013333321\n",
      "[NOR] Episode: 13990, Length: 75, Avg Reward: -74.131671029, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.341709137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 05:49:46,562] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video014000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 14000, Length: 113, Avg Reward: -69.24545915, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.7040708065\n",
      "[NOR] Episode: 14010, Length: 74, Avg Reward: -121.375436357, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.25177836418\n",
      "[NOR] Episode: 14020, Length: 70, Avg Reward: -122.445965987, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.419631958008\n",
      "[NOR] Episode: 14030, Length: 54, Avg Reward: -153.613832515, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.37948608398\n",
      "[NOR] Episode: 14040, Length: 80, Avg Reward: -114.876332458, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.264799118\n",
      "[NOR] Episode: 14050, Length: 89, Avg Reward: -128.030393949, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.453458786\n",
      "[NOR] Episode: 14060, Length: 82, Avg Reward: -147.958320455, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.72286748886\n",
      "[NOR] Episode: 14070, Length: 92, Avg Reward: -115.558520053, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -26.1873703003\n",
      "[NOR] Episode: 14080, Length: 79, Avg Reward: -154.296964277, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.5997409821\n",
      "[NOR] Episode: 14090, Length: 58, Avg Reward: -107.565401928, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 170.896392822\n",
      "[NOR] Episode: 14100, Length: 71, Avg Reward: -115.759318925, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.32298970222\n",
      "[NOR] Episode: 14110, Length: 90, Avg Reward: -94.2127628698, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.5691719055\n",
      "[NOR] Episode: 14120, Length: 81, Avg Reward: -119.934857164, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.22080230713\n",
      "[NOR] Episode: 14130, Length: 1000, Avg Reward: -101.629425466, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.77782511711\n",
      "[NOR] Episode: 14140, Length: 65, Avg Reward: -115.237408924, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 249.347290039\n",
      "[NOR] Episode: 14150, Length: 169, Avg Reward: -28.1926339833, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.78957509995\n",
      "[NOR] Episode: 14160, Length: 739, Avg Reward: -40.1080437124, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -12.9157304764\n",
      "[NOR] Episode: 14170, Length: 192, Avg Reward: -90.6391977108, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.57023859024\n",
      "[NOR] Episode: 14180, Length: 189, Avg Reward: -49.6361813834, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.43430423737\n",
      "[NOR] Episode: 14190, Length: 531, Avg Reward: -227.673781306, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.09078836441\n",
      "[NOR] Episode: 14200, Length: 477, Avg Reward: -273.269120717, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 26.481180191\n",
      "[NOR] Episode: 14210, Length: 972, Avg Reward: -90.127239677, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -13.9117078781\n",
      "[NOR] Episode: 14220, Length: 490, Avg Reward: -30.2388906786, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.96749639511\n",
      "[NOR] Episode: 14230, Length: 389, Avg Reward: -86.8944988851, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.74119949341\n",
      "[NOR] Episode: 14240, Length: 109, Avg Reward: -73.1947938781, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.53650283813\n",
      "[NOR] Episode: 14250, Length: 460, Avg Reward: -33.6855444273, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.383704543114\n",
      "[NOR] Episode: 14260, Length: 95, Avg Reward: -58.6812662945, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.07728385925\n",
      "[NOR] Episode: 14270, Length: 189, Avg Reward: 12.7087054778, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.55500650406\n",
      "[NOR] Episode: 14280, Length: 1000, Avg Reward: -71.6815191162, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.8491535187\n",
      "[NOR] Episode: 14290, Length: 122, Avg Reward: -87.3487269811, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 113.490524292\n",
      "[NOR] Episode: 14300, Length: 305, Avg Reward: -36.4796519701, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 125.425842285\n",
      "[NOR] Episode: 14310, Length: 301, Avg Reward: -83.7552529951, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.55819797516\n",
      "[NOR] Episode: 14320, Length: 103, Avg Reward: -155.455313299, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.97427129745\n",
      "[NOR] Episode: 14330, Length: 116, Avg Reward: -140.499456356, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.09507036209\n",
      "[NOR] Episode: 14340, Length: 107, Avg Reward: -38.7549336617, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -13.6201581955\n",
      "[NOR] Episode: 14350, Length: 110, Avg Reward: 12.4813741381, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -12.8747816086\n",
      "[NOR] Episode: 14360, Length: 114, Avg Reward: -77.4504680615, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.05892562866\n",
      "[NOR] Episode: 14370, Length: 112, Avg Reward: -27.0594832309, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.14856910706\n",
      "[NOR] Episode: 14380, Length: 244, Avg Reward: -27.5186980988, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -18.5199832916\n",
      "[NOR] Episode: 14390, Length: 99, Avg Reward: 9.0197899358, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.32103157043\n",
      "[NOR] Episode: 14400, Length: 270, Avg Reward: -59.4379192985, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.19136285782\n",
      "[NOR] Episode: 14410, Length: 268, Avg Reward: -11.9861655386, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.1255702972\n",
      "[NOR] Episode: 14420, Length: 101, Avg Reward: 32.9498101034, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.63691997528\n",
      "[NOR] Episode: 14430, Length: 104, Avg Reward: -12.7370528436, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 260.527008057\n",
      "[NOR] Episode: 14440, Length: 113, Avg Reward: -29.921380827, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.77464103699\n",
      "[NOR] Episode: 14450, Length: 103, Avg Reward: 15.6571138305, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.7287902832\n",
      "[NOR] Episode: 14460, Length: 105, Avg Reward: -20.6773968116, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.24450492859\n",
      "[NOR] Episode: 14470, Length: 80, Avg Reward: -31.1812012222, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.07015323639\n",
      "[NOR] Episode: 14480, Length: 125, Avg Reward: -32.3186256418, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.5771522522\n",
      "[NOR] Episode: 14490, Length: 753, Avg Reward: 20.3004754539, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.81063890457\n",
      "[NOR] Episode: 14500, Length: 99, Avg Reward: -25.5377182706, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.89475333691\n",
      "[NOR] Episode: 14510, Length: 110, Avg Reward: -33.9139755548, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.8627052307\n",
      "[NOR] Episode: 14520, Length: 122, Avg Reward: -1.84514162323, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.63646268845\n",
      "[NOR] Episode: 14530, Length: 500, Avg Reward: -3.58064799242, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.44861185551\n",
      "[NOR] Episode: 14540, Length: 172, Avg Reward: 3.12052843648, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.682097435\n",
      "[NOR] Episode: 14550, Length: 110, Avg Reward: -38.472847315, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.2803632021\n",
      "[NOR] Episode: 14560, Length: 118, Avg Reward: 17.3913202223, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.1543264389\n",
      "[NOR] Episode: 14570, Length: 113, Avg Reward: -64.8547848884, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.3260746002\n",
      "[NOR] Episode: 14580, Length: 135, Avg Reward: -36.1159944611, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.95246696472\n",
      "[NOR] Episode: 14590, Length: 133, Avg Reward: -19.6822686719, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.550469398499\n",
      "[NOR] Episode: 14600, Length: 470, Avg Reward: 98.0739757111, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.74045562744\n",
      "[NOR] Episode: 14610, Length: 308, Avg Reward: -19.5685467293, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.87864971161\n",
      "[NOR] Episode: 14620, Length: 337, Avg Reward: -11.937828108, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 26.363067627\n",
      "[NOR] Episode: 14630, Length: 87, Avg Reward: -35.7772982249, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 271.647613525\n",
      "[NOR] Episode: 14640, Length: 466, Avg Reward: 6.89401660917, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.79120635986\n",
      "[NOR] Episode: 14650, Length: 155, Avg Reward: -12.2892676569, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 103.079536438\n",
      "[NOR] Episode: 14660, Length: 59, Avg Reward: -71.3458260679, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.0941381454\n",
      "[NOR] Episode: 14670, Length: 287, Avg Reward: -90.3492293597, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.20408535004\n",
      "[NOR] Episode: 14680, Length: 761, Avg Reward: -15.5869364187, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.312564611435\n",
      "[NOR] Episode: 14690, Length: 103, Avg Reward: -18.6868127301, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 186.20413208\n",
      "[NOR] Episode: 14700, Length: 294, Avg Reward: 76.4284003667, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.8300008774\n",
      "[NOR] Episode: 14710, Length: 116, Avg Reward: 13.9928186134, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.0327672958\n",
      "[NOR] Episode: 14720, Length: 651, Avg Reward: -6.46412330739, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.71919059753\n",
      "[NOR] Episode: 14730, Length: 368, Avg Reward: -30.0404837003, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.54165935516\n",
      "[NOR] Episode: 14740, Length: 110, Avg Reward: -39.0951480892, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 128.057983398\n",
      "[NOR] Episode: 14750, Length: 69, Avg Reward: -103.071232069, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.89078497887\n",
      "[NOR] Episode: 14760, Length: 91, Avg Reward: -75.2065013184, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.91050434113\n",
      "[NOR] Episode: 14770, Length: 290, Avg Reward: -31.1525800017, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 147.482025146\n",
      "[NOR] Episode: 14780, Length: 276, Avg Reward: -34.121839296, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.62773799896\n",
      "[NOR] Episode: 14790, Length: 114, Avg Reward: -4.23151909928, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.44233036041\n",
      "[NOR] Episode: 14800, Length: 109, Avg Reward: 29.7008947802, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 219.727981567\n",
      "[NOR] Episode: 14810, Length: 90, Avg Reward: -1.10993167945, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.5303659439\n",
      "[NOR] Episode: 14820, Length: 112, Avg Reward: -5.25515392647, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.728821396828\n",
      "[NOR] Episode: 14830, Length: 591, Avg Reward: -22.5980855629, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.87937927246\n",
      "[NOR] Episode: 14840, Length: 245, Avg Reward: 18.8826237692, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.92442512512\n",
      "[NOR] Episode: 14850, Length: 900, Avg Reward: 23.2818387572, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0993790626526\n",
      "[NOR] Episode: 14860, Length: 485, Avg Reward: 43.6351120509, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.8085079193\n",
      "[NOR] Episode: 14870, Length: 499, Avg Reward: 143.381455255, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 180.484420776\n",
      "[NOR] Episode: 14880, Length: 230, Avg Reward: -13.1285765904, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.70730113983\n",
      "[NOR] Episode: 14890, Length: 113, Avg Reward: 36.1409380425, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -20.2748031616\n",
      "[NOR] Episode: 14900, Length: 343, Avg Reward: -61.5761669457, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.971605837345\n",
      "[NOR] Episode: 14910, Length: 342, Avg Reward: 80.3752738021, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.54548454285\n",
      "[NOR] Episode: 14920, Length: 817, Avg Reward: 11.2019290456, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.83195114136\n",
      "[NOR] Episode: 14930, Length: 311, Avg Reward: -1.31624385754, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 115.384407043\n",
      "[NOR] Episode: 14940, Length: 232, Avg Reward: 32.788073118, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 253.098556519\n",
      "[NOR] Episode: 14950, Length: 176, Avg Reward: -37.2197345465, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 83.2094955444\n",
      "[NOR] Episode: 14960, Length: 749, Avg Reward: 33.841072769, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.44323158264\n",
      "[NOR] Episode: 14970, Length: 209, Avg Reward: -20.7544374594, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 26.6171150208\n",
      "[NOR] Episode: 14980, Length: 120, Avg Reward: -77.5248933168, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 121.627784729\n",
      "[NOR] Episode: 14990, Length: 263, Avg Reward: -58.7038280743, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.2869300842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 06:05:22,054] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video015000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 15000, Length: 447, Avg Reward: -68.0079142443, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.10780143738\n",
      "[NOR] Episode: 15010, Length: 361, Avg Reward: -60.5065246693, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 156.744171143\n",
      "[NOR] Episode: 15020, Length: 120, Avg Reward: -37.5456468884, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 56.0564422607\n",
      "[NOR] Episode: 15030, Length: 695, Avg Reward: 11.2766972318, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.2840423584\n",
      "[NOR] Episode: 15040, Length: 182, Avg Reward: 85.4643101561, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.99030399323\n",
      "[NOR] Episode: 15050, Length: 195, Avg Reward: 10.5436326813, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -16.2266578674\n",
      "[NOR] Episode: 15060, Length: 101, Avg Reward: -85.6546729092, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.9728879929\n",
      "[NOR] Episode: 15070, Length: 111, Avg Reward: 16.6990327808, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.87307167053\n",
      "[NOR] Episode: 15080, Length: 123, Avg Reward: -10.4599938693, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 128.386260986\n",
      "[NOR] Episode: 15090, Length: 1000, Avg Reward: -27.0464889764, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.72745013237\n",
      "[NOR] Episode: 15100, Length: 128, Avg Reward: -19.9355809544, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.11768817902\n",
      "[NOR] Episode: 15110, Length: 109, Avg Reward: -66.5750549082, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.44658207893\n",
      "[NOR] Episode: 15120, Length: 113, Avg Reward: -68.2582038507, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.32456493378\n",
      "[NOR] Episode: 15130, Length: 102, Avg Reward: -70.7297377816, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 108.489723206\n",
      "[NOR] Episode: 15140, Length: 112, Avg Reward: -34.1409239546, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.35636997223\n",
      "[NOR] Episode: 15150, Length: 209, Avg Reward: -53.107297938, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.67540836334\n",
      "[NOR] Episode: 15160, Length: 178, Avg Reward: -36.0378802446, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.0087280273\n",
      "[NOR] Episode: 15170, Length: 168, Avg Reward: -37.7803301782, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.9949131012\n",
      "[NOR] Episode: 15180, Length: 302, Avg Reward: -95.8521332183, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.89138412476\n",
      "[NOR] Episode: 15190, Length: 186, Avg Reward: -3.76277842478, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.61771202087\n",
      "[NOR] Episode: 15200, Length: 331, Avg Reward: -125.931083931, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.09004688263\n",
      "[NOR] Episode: 15210, Length: 106, Avg Reward: -28.837012414, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.21285295486\n",
      "[NOR] Episode: 15220, Length: 152, Avg Reward: -77.2909116037, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 211.994598389\n",
      "[NOR] Episode: 15230, Length: 406, Avg Reward: 60.4302216631, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.9524102211\n",
      "[NOR] Episode: 15240, Length: 100, Avg Reward: -87.0073244387, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.20592212677\n",
      "[NOR] Episode: 15250, Length: 1000, Avg Reward: -21.5271878622, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.99596786499\n",
      "[NOR] Episode: 15260, Length: 129, Avg Reward: -25.0525487587, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.0618305206\n",
      "[NOR] Episode: 15270, Length: 321, Avg Reward: -24.8502316482, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 222.330352783\n",
      "[NOR] Episode: 15280, Length: 1000, Avg Reward: 20.8624996612, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.15960884094\n",
      "[NOR] Episode: 15290, Length: 1000, Avg Reward: 43.3449735935, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.994164466858\n",
      "[NOR] Episode: 15300, Length: 153, Avg Reward: -9.93587988851, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.56412601471\n",
      "[NOR] Episode: 15310, Length: 164, Avg Reward: -13.8181472446, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.79047572613\n",
      "[NOR] Episode: 15320, Length: 163, Avg Reward: -6.40396030834, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.438373327255\n",
      "[NOR] Episode: 15330, Length: 794, Avg Reward: 76.3299732278, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 155.203521729\n",
      "[NOR] Episode: 15340, Length: 752, Avg Reward: 20.3751025555, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 41.7594184875\n",
      "[NOR] Episode: 15350, Length: 216, Avg Reward: 109.4882852, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -17.0555057526\n",
      "[NOR] Episode: 15360, Length: 322, Avg Reward: 77.8949687722, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -44.3071670532\n",
      "[NOR] Episode: 15370, Length: 114, Avg Reward: 22.3838997576, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.66817498207\n",
      "[NOR] Episode: 15380, Length: 146, Avg Reward: -42.1025856973, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.85831260681\n",
      "[NOR] Episode: 15390, Length: 89, Avg Reward: -18.7869517604, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.1416835785\n",
      "[NOR] Episode: 15400, Length: 153, Avg Reward: -45.6320772147, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.35005760193\n",
      "[NOR] Episode: 15410, Length: 180, Avg Reward: 18.8476498637, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.45404911041\n",
      "[NOR] Episode: 15420, Length: 159, Avg Reward: -4.84769160337, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -12.3310117722\n",
      "[NOR] Episode: 15430, Length: 160, Avg Reward: -36.9057230529, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.8997344971\n",
      "[NOR] Episode: 15440, Length: 84, Avg Reward: -64.0550243168, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.09607219696\n",
      "[NOR] Episode: 15450, Length: 121, Avg Reward: -3.21701938629, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.86277484894\n",
      "[NOR] Episode: 15460, Length: 122, Avg Reward: -32.9128658399, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.1034469604\n",
      "[NOR] Episode: 15470, Length: 232, Avg Reward: 7.16348800713, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.1754560471\n",
      "[NOR] Episode: 15480, Length: 235, Avg Reward: 73.6782106589, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.24302959442\n",
      "[NOR] Episode: 15490, Length: 74, Avg Reward: -29.9382001728, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.169148206711\n",
      "[NOR] Episode: 15500, Length: 64, Avg Reward: -43.3020076109, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.11327219009\n",
      "[NOR] Episode: 15510, Length: 72, Avg Reward: -72.2392186139, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 38.4923324585\n",
      "[NOR] Episode: 15520, Length: 75, Avg Reward: -72.5359269192, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.53488063812\n",
      "[NOR] Episode: 15530, Length: 315, Avg Reward: -37.6713887809, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.05177879333\n",
      "[NOR] Episode: 15540, Length: 176, Avg Reward: -79.7405023883, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.818157196\n",
      "[NOR] Episode: 15550, Length: 121, Avg Reward: -33.3118137176, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.92361783981\n",
      "[NOR] Episode: 15560, Length: 136, Avg Reward: -15.7173844673, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.34968316555\n",
      "[NOR] Episode: 15570, Length: 210, Avg Reward: -15.494947691, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.2788734436\n",
      "[NOR] Episode: 15580, Length: 198, Avg Reward: -97.5126938868, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 25.6224651337\n",
      "[NOR] Episode: 15590, Length: 76, Avg Reward: -39.2999340134, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.5314445496\n",
      "[NOR] Episode: 15600, Length: 232, Avg Reward: 27.3157671912, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 249.009613037\n",
      "[NOR] Episode: 15610, Length: 662, Avg Reward: -2.50469342984, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.74388885498\n",
      "[NOR] Episode: 15620, Length: 1000, Avg Reward: -23.3010434398, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.98905014992\n",
      "[NOR] Episode: 15630, Length: 536, Avg Reward: 10.7419502724, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.70098304749\n",
      "[NOR] Episode: 15640, Length: 1000, Avg Reward: -80.2911640286, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.22499990463\n",
      "[NOR] Episode: 15650, Length: 561, Avg Reward: -70.324451229, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.23772621155\n",
      "[NOR] Episode: 15660, Length: 483, Avg Reward: -15.2072975646, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.97175788879\n",
      "[NOR] Episode: 15670, Length: 1000, Avg Reward: 77.7523732746, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 28.1984825134\n",
      "[NOR] Episode: 15680, Length: 660, Avg Reward: 46.229842462, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 33.1347198486\n",
      "[NOR] Episode: 15690, Length: 470, Avg Reward: 48.4940143445, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 26.8321342468\n",
      "[NOR] Episode: 15700, Length: 262, Avg Reward: -15.4107765421, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 237.778717041\n",
      "[NOR] Episode: 15710, Length: 449, Avg Reward: -76.1079129682, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.90563702583\n",
      "[NOR] Episode: 15720, Length: 85, Avg Reward: -41.4901189707, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.23447012901\n",
      "[NOR] Episode: 15730, Length: 163, Avg Reward: -145.559517544, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.37926912308\n",
      "[NOR] Episode: 15740, Length: 89, Avg Reward: -124.949087649, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.299495697\n",
      "[NOR] Episode: 15750, Length: 329, Avg Reward: -146.677737844, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.62505245209\n",
      "[NOR] Episode: 15760, Length: 375, Avg Reward: -173.297955096, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.2771987915\n",
      "[NOR] Episode: 15770, Length: 1000, Avg Reward: -5.64783773807, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 264.697937012\n",
      "[NOR] Episode: 15780, Length: 105, Avg Reward: -50.726373123, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.61480903625\n",
      "[NOR] Episode: 15790, Length: 618, Avg Reward: 68.1005210908, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.05974292755\n",
      "[NOR] Episode: 15800, Length: 876, Avg Reward: 71.8529214718, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 295.385925293\n",
      "[NOR] Episode: 15810, Length: 434, Avg Reward: 93.720233962, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.5457038879\n",
      "[NOR] Episode: 15820, Length: 1000, Avg Reward: -43.9113723273, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.05405688286\n",
      "[NOR] Episode: 15830, Length: 254, Avg Reward: -22.8837327553, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.7453937531\n",
      "[NOR] Episode: 15840, Length: 367, Avg Reward: -35.5115354664, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.2162666321\n",
      "[NOR] Episode: 15850, Length: 535, Avg Reward: -6.58696135484, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 227.345275879\n",
      "[NOR] Episode: 15860, Length: 519, Avg Reward: 46.7604876001, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.43795013428\n",
      "[NOR] Episode: 15870, Length: 1000, Avg Reward: 78.0501837456, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.10253143311\n",
      "[NOR] Episode: 15880, Length: 646, Avg Reward: 20.344372236, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 34.5112876892\n",
      "[NOR] Episode: 15890, Length: 276, Avg Reward: 29.5961964173, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.6652345657\n",
      "[NOR] Episode: 15900, Length: 112, Avg Reward: 128.828343883, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.11171245575\n",
      "[NOR] Episode: 15910, Length: 627, Avg Reward: 118.933150207, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0826951265335\n",
      "[NOR] Episode: 15920, Length: 1000, Avg Reward: -17.5630132682, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.2796268463\n",
      "[NOR] Episode: 15930, Length: 1000, Avg Reward: 24.7241845956, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.62059426308\n",
      "[NOR] Episode: 15940, Length: 578, Avg Reward: 22.7259798878, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 257.002807617\n",
      "[NOR] Episode: 15950, Length: 1000, Avg Reward: 46.0776098584, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.80967092514\n",
      "[NOR] Episode: 15960, Length: 191, Avg Reward: 32.9734873776, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.64559006691\n",
      "[NOR] Episode: 15970, Length: 414, Avg Reward: -71.2528808859, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.96789264679\n",
      "[NOR] Episode: 15980, Length: 587, Avg Reward: 54.0732843908, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 252.479980469\n",
      "[NOR] Episode: 15990, Length: 377, Avg Reward: 116.542046704, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.39877897501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 06:28:50,461] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video016000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 16000, Length: 236, Avg Reward: 138.279622371, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.27499294281\n",
      "[NOR] Episode: 16010, Length: 509, Avg Reward: 107.974856102, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.2123794556\n",
      "[NOR] Episode: 16020, Length: 475, Avg Reward: -20.8171567115, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.9823894501\n",
      "[NOR] Episode: 16030, Length: 807, Avg Reward: 107.684101645, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.0249233246\n",
      "[NOR] Episode: 16040, Length: 412, Avg Reward: 74.3010346959, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.294859409332\n",
      "[NOR] Episode: 16050, Length: 1000, Avg Reward: 57.955131158, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 261.179077148\n",
      "[NOR] Episode: 16060, Length: 867, Avg Reward: -19.6408703695, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.28025579453\n",
      "[NOR] Episode: 16070, Length: 530, Avg Reward: 18.3704740041, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.04826831818\n",
      "[NOR] Episode: 16080, Length: 1000, Avg Reward: 1.00057332302, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.07017087936\n",
      "[NOR] Episode: 16090, Length: 1000, Avg Reward: 38.981836267, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.590203762054\n",
      "[NOR] Episode: 16100, Length: 1000, Avg Reward: 45.1505810195, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.91196870804\n",
      "[NOR] Episode: 16110, Length: 305, Avg Reward: -72.616865695, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.46807432175\n",
      "[NOR] Episode: 16120, Length: 1000, Avg Reward: -39.884866847, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.7021522522\n",
      "[NOR] Episode: 16130, Length: 1000, Avg Reward: -89.4008128037, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.385278701782\n",
      "[NOR] Episode: 16140, Length: 1000, Avg Reward: -58.2813985127, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.40666007996\n",
      "[NOR] Episode: 16150, Length: 985, Avg Reward: -1.34120811849, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.17389118671\n",
      "[NOR] Episode: 16160, Length: 908, Avg Reward: -5.56000296652, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.4940338135\n",
      "[NOR] Episode: 16170, Length: 1000, Avg Reward: 8.38359931601, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.37720966339\n",
      "[NOR] Episode: 16180, Length: 1000, Avg Reward: -50.2674829854, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.4154720306\n",
      "[NOR] Episode: 16190, Length: 1000, Avg Reward: -48.8796973459, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 267.225280762\n",
      "[NOR] Episode: 16200, Length: 1000, Avg Reward: -91.9062312649, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.71195530891\n",
      "[NOR] Episode: 16210, Length: 1000, Avg Reward: -63.4644510401, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.273146152496\n",
      "[NOR] Episode: 16220, Length: 1000, Avg Reward: -47.9850097564, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.19678735733\n",
      "[NOR] Episode: 16230, Length: 1000, Avg Reward: -13.6386577426, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.85907137394\n",
      "[NOR] Episode: 16240, Length: 1000, Avg Reward: -68.1041181308, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.90263342857\n",
      "[NOR] Episode: 16250, Length: 849, Avg Reward: 12.3467304347, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.90007472038\n",
      "[NOR] Episode: 16260, Length: 728, Avg Reward: 128.55526299, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 272.925109863\n",
      "[NOR] Episode: 16270, Length: 1000, Avg Reward: 141.99545337, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.3296456337\n",
      "[NOR] Episode: 16280, Length: 462, Avg Reward: 86.4973754014, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.04209518433\n",
      "[NOR] Episode: 16290, Length: 1000, Avg Reward: 35.829745689, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.41161417961\n",
      "[NOR] Episode: 16300, Length: 1000, Avg Reward: 81.9943804039, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.967180430889\n",
      "[NOR] Episode: 16310, Length: 364, Avg Reward: 90.534720951, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.91546630859\n",
      "[NOR] Episode: 16320, Length: 769, Avg Reward: 148.217899023, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.40279138088\n",
      "[NOR] Episode: 16330, Length: 889, Avg Reward: 121.947054172, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.448585867882\n",
      "[NOR] Episode: 16340, Length: 390, Avg Reward: 47.0229703203, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.15510606766\n",
      "[NOR] Episode: 16350, Length: 1000, Avg Reward: 134.655090232, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.89133739471\n",
      "[NOR] Episode: 16360, Length: 577, Avg Reward: 64.1449748607, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.278221607208\n",
      "[NOR] Episode: 16370, Length: 452, Avg Reward: 80.978839142, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.881176590919\n",
      "[NOR] Episode: 16380, Length: 579, Avg Reward: 43.106188962, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.01006603241\n",
      "[NOR] Episode: 16390, Length: 792, Avg Reward: 81.4561764881, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.49816572666\n",
      "[NOR] Episode: 16400, Length: 433, Avg Reward: -1.59363206909, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.137472510338\n",
      "[NOR] Episode: 16410, Length: 427, Avg Reward: 62.3470921177, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.9121704102\n",
      "[NOR] Episode: 16420, Length: 509, Avg Reward: 102.931976493, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.1816711426\n",
      "[NOR] Episode: 16430, Length: 516, Avg Reward: 74.2254631558, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.15931129456\n",
      "[NOR] Episode: 16440, Length: 635, Avg Reward: 107.722918159, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 262.616699219\n",
      "[NOR] Episode: 16450, Length: 580, Avg Reward: 133.332559634, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.75525009632\n",
      "[NOR] Episode: 16460, Length: 474, Avg Reward: 61.4611124695, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 243.563293457\n",
      "[NOR] Episode: 16470, Length: 165, Avg Reward: 107.677737007, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.86959457397\n",
      "[NOR] Episode: 16480, Length: 1000, Avg Reward: 4.05857794376, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.09224438667\n",
      "[NOR] Episode: 16490, Length: 229, Avg Reward: 37.6580450164, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.63418388367\n",
      "[NOR] Episode: 16500, Length: 244, Avg Reward: 33.1643819499, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.24754190445\n",
      "[NOR] Episode: 16510, Length: 143, Avg Reward: 4.01295535814, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.699858248234\n",
      "[NOR] Episode: 16520, Length: 143, Avg Reward: -46.5960813539, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.48107266426\n",
      "[NOR] Episode: 16530, Length: 1000, Avg Reward: -49.1848753227, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 257.153167725\n",
      "[NOR] Episode: 16540, Length: 159, Avg Reward: -5.24610962697, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.38461399078\n",
      "[NOR] Episode: 16550, Length: 82, Avg Reward: -38.5127118128, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.6409239769\n",
      "[NOR] Episode: 16560, Length: 1000, Avg Reward: 34.2352775992, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.19775104523\n",
      "[NOR] Episode: 16570, Length: 1000, Avg Reward: -28.3237478509, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.23350477219\n",
      "[NOR] Episode: 16580, Length: 1000, Avg Reward: -50.3645356662, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.92973542213\n",
      "[NOR] Episode: 16590, Length: 1000, Avg Reward: 20.235080222, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.59247493744\n",
      "[NOR] Episode: 16600, Length: 793, Avg Reward: 31.6586404425, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.4590425491\n",
      "[NOR] Episode: 16610, Length: 1000, Avg Reward: 29.470780429, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 264.437591553\n",
      "[NOR] Episode: 16620, Length: 725, Avg Reward: 75.3425158813, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.86697363853\n",
      "[NOR] Episode: 16630, Length: 77, Avg Reward: 68.526824475, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.63206577301\n",
      "[NOR] Episode: 16640, Length: 1000, Avg Reward: 92.5946719554, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.67610967159\n",
      "[NOR] Episode: 16650, Length: 378, Avg Reward: 116.486739215, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.7350196838\n",
      "[NOR] Episode: 16660, Length: 265, Avg Reward: 155.518106757, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.26217472553\n",
      "[NOR] Episode: 16670, Length: 376, Avg Reward: 11.3799741482, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.228281080723\n",
      "[NOR] Episode: 16680, Length: 888, Avg Reward: 54.0365044625, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.13401222229\n",
      "[NOR] Episode: 16690, Length: 669, Avg Reward: 47.3703496823, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.42212986946\n",
      "[NOR] Episode: 16700, Length: 212, Avg Reward: 4.55421738875, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0507819652557\n",
      "[NOR] Episode: 16710, Length: 489, Avg Reward: 22.8048298061, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.102857112885\n",
      "[NOR] Episode: 16720, Length: 1000, Avg Reward: 86.9336373217, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.15285801888\n",
      "[NOR] Episode: 16730, Length: 343, Avg Reward: 37.2320395458, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.802942276\n",
      "[NOR] Episode: 16740, Length: 521, Avg Reward: -25.287985319, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 248.025848389\n",
      "[NOR] Episode: 16750, Length: 345, Avg Reward: 43.16389843, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.2372546196\n",
      "[NOR] Episode: 16760, Length: 196, Avg Reward: -7.68227453786, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.38519573212\n",
      "[NOR] Episode: 16770, Length: 900, Avg Reward: -29.6976985492, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.1949501038\n",
      "[NOR] Episode: 16780, Length: 837, Avg Reward: -116.456479316, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 252.024185181\n",
      "[NOR] Episode: 16790, Length: 1000, Avg Reward: -19.726614623, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.84980487823\n",
      "[NOR] Episode: 16800, Length: 82, Avg Reward: -94.6167225723, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.32138633728\n",
      "[NOR] Episode: 16810, Length: 81, Avg Reward: -200.496777847, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.37142169476\n",
      "[NOR] Episode: 16820, Length: 481, Avg Reward: -123.587516225, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.94348669052\n",
      "[NOR] Episode: 16830, Length: 766, Avg Reward: 10.9121506835, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.32472896576\n",
      "[NOR] Episode: 16840, Length: 594, Avg Reward: -58.6019019214, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.536859750748\n",
      "[NOR] Episode: 16850, Length: 660, Avg Reward: -31.0408457774, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.29582214355\n",
      "[NOR] Episode: 16860, Length: 109, Avg Reward: 1.11513040559, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.31310915947\n",
      "[NOR] Episode: 16870, Length: 354, Avg Reward: -59.8968043456, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.59419870377\n",
      "[NOR] Episode: 16880, Length: 148, Avg Reward: -14.3412507987, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.3324239254\n",
      "[NOR] Episode: 16890, Length: 115, Avg Reward: 107.27946735, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.87345194817\n",
      "[NOR] Episode: 16900, Length: 414, Avg Reward: 86.7164988174, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.10586929321\n",
      "[NOR] Episode: 16910, Length: 886, Avg Reward: -38.1536836422, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.02160453796\n",
      "[NOR] Episode: 16920, Length: 762, Avg Reward: -19.6948384261, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 255.403549194\n",
      "[NOR] Episode: 16930, Length: 1000, Avg Reward: 12.6157562441, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.39420413971\n",
      "[NOR] Episode: 16940, Length: 503, Avg Reward: -50.695605401, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.947613120079\n",
      "[NOR] Episode: 16950, Length: 747, Avg Reward: -25.0052734532, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.03392124176\n",
      "[NOR] Episode: 16960, Length: 1000, Avg Reward: 10.3920128789, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.49160528183\n",
      "[NOR] Episode: 16970, Length: 978, Avg Reward: -114.635709119, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.45355176926\n",
      "[NOR] Episode: 16980, Length: 600, Avg Reward: -53.9457529378, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.62960982323\n",
      "[NOR] Episode: 16990, Length: 180, Avg Reward: 19.7531101376, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.73759222031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 07:14:45,307] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video017000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 17000, Length: 202, Avg Reward: -6.93980991803, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.04566860199\n",
      "[NOR] Episode: 17010, Length: 452, Avg Reward: -52.2958655985, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.49793624878\n",
      "[NOR] Episode: 17020, Length: 375, Avg Reward: -52.1877883086, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.150255143642\n",
      "[NOR] Episode: 17030, Length: 257, Avg Reward: -83.7499921031, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.42313671112\n",
      "[NOR] Episode: 17040, Length: 761, Avg Reward: -60.6798753579, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.8206119537\n",
      "[NOR] Episode: 17050, Length: 1000, Avg Reward: -7.87656131469, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.531370937824\n",
      "[NOR] Episode: 17060, Length: 1000, Avg Reward: -87.073302014, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.91306066513\n",
      "[NOR] Episode: 17070, Length: 1000, Avg Reward: -63.396964729, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.3516998291\n",
      "[NOR] Episode: 17080, Length: 163, Avg Reward: -48.3730193557, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.33870124817\n",
      "[NOR] Episode: 17090, Length: 1000, Avg Reward: -10.9464938202, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.68069982529\n",
      "[NOR] Episode: 17100, Length: 1000, Avg Reward: -2.10354319412, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.5064637661\n",
      "[NOR] Episode: 17110, Length: 225, Avg Reward: 99.3506502962, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.70730614662\n",
      "[NOR] Episode: 17120, Length: 516, Avg Reward: 39.9048848832, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.54181098938\n",
      "[NOR] Episode: 17130, Length: 170, Avg Reward: 123.570431962, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.98081970215\n",
      "[NOR] Episode: 17140, Length: 279, Avg Reward: 119.589017649, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.01774835587\n",
      "[NOR] Episode: 17150, Length: 329, Avg Reward: -29.684838633, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.9270915985\n",
      "[NOR] Episode: 17160, Length: 421, Avg Reward: -16.1735261658, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.7134666443\n",
      "[NOR] Episode: 17170, Length: 666, Avg Reward: -25.8051035856, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.266083955765\n",
      "[NOR] Episode: 17180, Length: 371, Avg Reward: -19.4452569573, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.51876401901\n",
      "[NOR] Episode: 17190, Length: 639, Avg Reward: -4.8394757931, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.96886062622\n",
      "[NOR] Episode: 17200, Length: 505, Avg Reward: -30.147986883, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.4340209961\n",
      "[NOR] Episode: 17210, Length: 1000, Avg Reward: -46.0347924845, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.1349620819\n",
      "[NOR] Episode: 17220, Length: 1000, Avg Reward: 23.7546738053, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.206100940704\n",
      "[NOR] Episode: 17230, Length: 724, Avg Reward: 29.2320873482, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.617959856987\n",
      "[NOR] Episode: 17240, Length: 1000, Avg Reward: -15.2740166737, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0815328359604\n",
      "[NOR] Episode: 17250, Length: 159, Avg Reward: -20.66005884, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 29.1747627258\n",
      "[NOR] Episode: 17260, Length: 201, Avg Reward: -42.5260703153, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 33.3712768555\n",
      "[NOR] Episode: 17270, Length: 178, Avg Reward: -63.9641872355, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.50720930099\n",
      "[NOR] Episode: 17280, Length: 115, Avg Reward: -129.599547539, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.70977830887\n",
      "[NOR] Episode: 17290, Length: 300, Avg Reward: -114.116163001, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.415195584297\n",
      "[NOR] Episode: 17300, Length: 135, Avg Reward: -132.930460849, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 247.885284424\n",
      "[NOR] Episode: 17310, Length: 1000, Avg Reward: -138.550110296, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.55176353455\n",
      "[NOR] Episode: 17320, Length: 161, Avg Reward: -69.2365366037, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.2335205078\n",
      "[NOR] Episode: 17330, Length: 146, Avg Reward: -163.105568417, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.2944393158\n",
      "[NOR] Episode: 17340, Length: 1000, Avg Reward: -103.28153985, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.7189950943\n",
      "[NOR] Episode: 17350, Length: 1000, Avg Reward: -74.4302646755, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.84277796745\n",
      "[NOR] Episode: 17360, Length: 523, Avg Reward: -147.430241968, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.5790643692\n",
      "[NOR] Episode: 17370, Length: 1000, Avg Reward: -120.434816414, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.4892606735\n",
      "[NOR] Episode: 17380, Length: 211, Avg Reward: -61.8469467093, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.89551663399\n",
      "[NOR] Episode: 17390, Length: 1000, Avg Reward: -83.9902969546, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.1550045013\n",
      "[NOR] Episode: 17400, Length: 1000, Avg Reward: -87.1736546686, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.346473067999\n",
      "[NOR] Episode: 17410, Length: 1000, Avg Reward: -94.8311644764, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.96283197403\n",
      "[NOR] Episode: 17420, Length: 1000, Avg Reward: -81.6691934368, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.37680435181\n",
      "[NOR] Episode: 17430, Length: 945, Avg Reward: -47.1160392447, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.721165657\n",
      "[NOR] Episode: 17440, Length: 1000, Avg Reward: -38.6019630511, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.0883979797\n",
      "[NOR] Episode: 17450, Length: 1000, Avg Reward: -51.7407882292, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.62678003311\n",
      "[NOR] Episode: 17460, Length: 1000, Avg Reward: -35.3397945061, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.70443630219\n",
      "[NOR] Episode: 17470, Length: 1000, Avg Reward: -24.326803618, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.1096572876\n",
      "[NOR] Episode: 17480, Length: 1000, Avg Reward: -29.9926228228, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.30867004395\n",
      "[NOR] Episode: 17490, Length: 1000, Avg Reward: 0.505095352756, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.5375785828\n",
      "[NOR] Episode: 17500, Length: 924, Avg Reward: 86.3965067557, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.93231868744\n",
      "[NOR] Episode: 17510, Length: 109, Avg Reward: 40.8527861151, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.4742846489\n",
      "[NOR] Episode: 17520, Length: 1000, Avg Reward: -32.1313843017, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 24.7289180756\n",
      "[NOR] Episode: 17530, Length: 1000, Avg Reward: -6.60835899975, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.17280685902\n",
      "[NOR] Episode: 17540, Length: 142, Avg Reward: 73.7663152972, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.79020786285\n",
      "[NOR] Episode: 17550, Length: 644, Avg Reward: 43.9086821273, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 266.638702393\n",
      "[NOR] Episode: 17560, Length: 457, Avg Reward: 48.2446576599, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.2538547516\n",
      "[NOR] Episode: 17570, Length: 163, Avg Reward: 73.9966909478, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.0501844883\n",
      "[NOR] Episode: 17580, Length: 1000, Avg Reward: 52.0285048977, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.59482479095\n",
      "[NOR] Episode: 17590, Length: 1000, Avg Reward: 8.68437860011, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.27217674255\n",
      "[NOR] Episode: 17600, Length: 1000, Avg Reward: -12.1824887656, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.3387413025\n",
      "[NOR] Episode: 17610, Length: 1000, Avg Reward: -20.1043604361, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.53184604645\n",
      "[NOR] Episode: 17620, Length: 1000, Avg Reward: -59.5739179532, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.0495538712\n",
      "[NOR] Episode: 17630, Length: 471, Avg Reward: -16.8678438171, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.12997913361\n",
      "[NOR] Episode: 17640, Length: 1000, Avg Reward: -47.9349220087, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.4203586578\n",
      "[NOR] Episode: 17650, Length: 1000, Avg Reward: -26.2376374, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 25.7897071838\n",
      "[NOR] Episode: 17660, Length: 1000, Avg Reward: 2.3633092204, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.5781993866\n",
      "[NOR] Episode: 17670, Length: 161, Avg Reward: -33.3099233593, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.8737869263\n",
      "[NOR] Episode: 17680, Length: 159, Avg Reward: -36.2999442703, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.92516589165\n",
      "[NOR] Episode: 17690, Length: 817, Avg Reward: 52.7628460272, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.88200759888\n",
      "[NOR] Episode: 17700, Length: 329, Avg Reward: 132.658802464, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.4957942963\n",
      "[NOR] Episode: 17710, Length: 265, Avg Reward: 101.621437945, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.70416307449\n",
      "[NOR] Episode: 17720, Length: 176, Avg Reward: -14.6509481572, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.3265209198\n",
      "[NOR] Episode: 17730, Length: 1000, Avg Reward: 26.451081017, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.0751533508\n",
      "[NOR] Episode: 17740, Length: 397, Avg Reward: -16.9199916055, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.33292770386\n",
      "[NOR] Episode: 17750, Length: 188, Avg Reward: -21.8617846525, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.20161342621\n",
      "[NOR] Episode: 17760, Length: 121, Avg Reward: 78.8055147207, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.941537737846\n",
      "[NOR] Episode: 17770, Length: 611, Avg Reward: 171.355273624, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.98859643936\n",
      "[NOR] Episode: 17780, Length: 119, Avg Reward: 94.6943469744, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.230890274\n",
      "[NOR] Episode: 17790, Length: 207, Avg Reward: 147.804621143, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.53906440735\n",
      "[NOR] Episode: 17800, Length: 145, Avg Reward: 49.3490562858, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.56491136551\n",
      "[NOR] Episode: 17810, Length: 139, Avg Reward: -47.8935547825, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.95504903793\n",
      "[NOR] Episode: 17820, Length: 100, Avg Reward: -72.545109532, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.19701266289\n",
      "[NOR] Episode: 17830, Length: 464, Avg Reward: -54.33518008, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.96950244904\n",
      "[NOR] Episode: 17840, Length: 191, Avg Reward: -22.0587117851, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.66663789749\n",
      "[NOR] Episode: 17850, Length: 91, Avg Reward: -121.319064028, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.39281845093\n",
      "[NOR] Episode: 17860, Length: 185, Avg Reward: -63.6110921112, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0457233041525\n",
      "[NOR] Episode: 17870, Length: 152, Avg Reward: -50.7705566845, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0907919406891\n",
      "[NOR] Episode: 17880, Length: 141, Avg Reward: -100.261671491, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.61742663383\n",
      "[NOR] Episode: 17890, Length: 122, Avg Reward: -81.5481829721, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.644458532333\n",
      "[NOR] Episode: 17900, Length: 81, Avg Reward: -145.264967593, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.04985141754\n",
      "[NOR] Episode: 17910, Length: 160, Avg Reward: -99.3568619239, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.90054225922\n",
      "[NOR] Episode: 17920, Length: 595, Avg Reward: -49.0666553427, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.50078392029\n",
      "[NOR] Episode: 17930, Length: 730, Avg Reward: 109.616288675, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.0543451309\n",
      "[NOR] Episode: 17940, Length: 368, Avg Reward: 87.0640006474, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.7324542999\n",
      "[NOR] Episode: 17950, Length: 126, Avg Reward: 39.9124597968, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.24394130707\n",
      "[NOR] Episode: 17960, Length: 183, Avg Reward: -62.3113664412, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.2924079895\n",
      "[NOR] Episode: 17970, Length: 134, Avg Reward: -56.8495620145, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.74281930923\n",
      "[NOR] Episode: 17980, Length: 340, Avg Reward: 107.562891073, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.6634349823\n",
      "[NOR] Episode: 17990, Length: 658, Avg Reward: 132.603742067, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.732837677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 07:56:20,240] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video018000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 18000, Length: 88, Avg Reward: -34.5678378565, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.21974086761\n",
      "[NOR] Episode: 18010, Length: 395, Avg Reward: -77.6564137714, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.92551612854\n",
      "[NOR] Episode: 18020, Length: 188, Avg Reward: -41.0689769164, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.87977600098\n",
      "[NOR] Episode: 18030, Length: 164, Avg Reward: 10.3884148772, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.04654693604\n",
      "[NOR] Episode: 18040, Length: 385, Avg Reward: 116.759271383, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 177.099243164\n",
      "[NOR] Episode: 18050, Length: 461, Avg Reward: 76.9115276604, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.65790605545\n",
      "[NOR] Episode: 18060, Length: 340, Avg Reward: 131.06370318, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.0454082489\n",
      "[NOR] Episode: 18070, Length: 517, Avg Reward: 165.497307194, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.492732942104\n",
      "[NOR] Episode: 18080, Length: 538, Avg Reward: 39.1556614198, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.89391803741\n",
      "[NOR] Episode: 18090, Length: 467, Avg Reward: 75.9602970547, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.46046733856\n",
      "[NOR] Episode: 18100, Length: 217, Avg Reward: 145.044683874, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.5089045763\n",
      "[NOR] Episode: 18110, Length: 641, Avg Reward: 111.451372447, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.70826482773\n",
      "[NOR] Episode: 18120, Length: 154, Avg Reward: -0.182089693602, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.87124919891\n",
      "[NOR] Episode: 18130, Length: 418, Avg Reward: 32.9537621771, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 255.6118927\n",
      "[NOR] Episode: 18140, Length: 320, Avg Reward: 52.8764171303, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.21157288551\n",
      "[NOR] Episode: 18150, Length: 451, Avg Reward: -29.9677715582, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.6145324707\n",
      "[NOR] Episode: 18160, Length: 139, Avg Reward: 73.7645107124, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.78698015213\n",
      "[NOR] Episode: 18170, Length: 343, Avg Reward: 89.7363837037, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.22556352615\n",
      "[NOR] Episode: 18180, Length: 462, Avg Reward: 98.3519039018, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.6965045929\n",
      "[NOR] Episode: 18190, Length: 295, Avg Reward: 44.6515297144, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.659656524658\n",
      "[NOR] Episode: 18200, Length: 991, Avg Reward: 109.878390735, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.36425733566\n",
      "[NOR] Episode: 18210, Length: 550, Avg Reward: 108.927093309, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.4508419037\n",
      "[NOR] Episode: 18220, Length: 505, Avg Reward: -20.3687841878, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.440907359123\n",
      "[NOR] Episode: 18230, Length: 1000, Avg Reward: 16.5845932705, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.7932767868\n",
      "[NOR] Episode: 18240, Length: 645, Avg Reward: -34.945901409, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.94499754906\n",
      "[NOR] Episode: 18250, Length: 346, Avg Reward: -16.9550398487, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.118656039238\n",
      "[NOR] Episode: 18260, Length: 819, Avg Reward: -47.7503103117, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.69063687325\n",
      "[NOR] Episode: 18270, Length: 611, Avg Reward: -9.34844389919, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.03392553329\n",
      "[NOR] Episode: 18280, Length: 706, Avg Reward: 167.961198509, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.701770663261\n",
      "[NOR] Episode: 18290, Length: 464, Avg Reward: 87.678353753, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.87327718735\n",
      "[NOR] Episode: 18300, Length: 752, Avg Reward: 62.4095967965, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.388353049755\n",
      "[NOR] Episode: 18310, Length: 534, Avg Reward: 77.9656879452, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.6704993248\n",
      "[NOR] Episode: 18320, Length: 856, Avg Reward: 93.1865437411, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.09531974792\n",
      "[NOR] Episode: 18330, Length: 1000, Avg Reward: 129.512732429, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.63146018982\n",
      "[NOR] Episode: 18340, Length: 364, Avg Reward: 62.5028903125, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.597365736961\n",
      "[NOR] Episode: 18350, Length: 1000, Avg Reward: 132.94282252, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.79345035553\n",
      "[NOR] Episode: 18360, Length: 1000, Avg Reward: 23.2039590715, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.92116641998\n",
      "[NOR] Episode: 18370, Length: 387, Avg Reward: 10.688785275, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.69624137878\n",
      "[NOR] Episode: 18380, Length: 1000, Avg Reward: -89.7734641398, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.81469106674\n",
      "[NOR] Episode: 18390, Length: 142, Avg Reward: -28.5391801228, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.81520462036\n",
      "[NOR] Episode: 18400, Length: 1000, Avg Reward: 36.1796336731, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.5926818848\n",
      "[NOR] Episode: 18410, Length: 150, Avg Reward: 37.0903996932, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.696987986565\n",
      "[NOR] Episode: 18420, Length: 135, Avg Reward: -43.3617006665, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.14765572548\n",
      "[NOR] Episode: 18430, Length: 110, Avg Reward: 53.3489010233, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.00029563904\n",
      "[NOR] Episode: 18440, Length: 458, Avg Reward: -72.9196047441, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.72378826141\n",
      "[NOR] Episode: 18450, Length: 410, Avg Reward: -116.068852864, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.67383956909\n",
      "[NOR] Episode: 18460, Length: 122, Avg Reward: -83.0268262192, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.849136352539\n",
      "[NOR] Episode: 18470, Length: 129, Avg Reward: -66.1500324531, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.28621339798\n",
      "[NOR] Episode: 18480, Length: 336, Avg Reward: -52.8979551739, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.74261832237\n",
      "[NOR] Episode: 18490, Length: 132, Avg Reward: -157.878421275, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0771341323853\n",
      "[NOR] Episode: 18500, Length: 125, Avg Reward: -196.468203162, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.42673563957\n",
      "[NOR] Episode: 18510, Length: 136, Avg Reward: -205.458230307, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.425503015518\n",
      "[NOR] Episode: 18520, Length: 128, Avg Reward: -217.001814956, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.37226104736\n",
      "[NOR] Episode: 18530, Length: 275, Avg Reward: -230.135382026, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.411994099617\n",
      "[NOR] Episode: 18540, Length: 246, Avg Reward: -123.077763309, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.36848855019\n",
      "[NOR] Episode: 18550, Length: 399, Avg Reward: -81.2424500912, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.510307312\n",
      "[NOR] Episode: 18560, Length: 227, Avg Reward: -117.608721728, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 252.133239746\n",
      "[NOR] Episode: 18570, Length: 218, Avg Reward: -103.406882531, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.55557441711\n",
      "[NOR] Episode: 18580, Length: 725, Avg Reward: -125.475947274, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.68718004227\n",
      "[NOR] Episode: 18590, Length: 98, Avg Reward: -42.0879100887, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.03201007843\n",
      "[NOR] Episode: 18600, Length: 1000, Avg Reward: -74.8214061883, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.73735570908\n",
      "[NOR] Episode: 18610, Length: 1000, Avg Reward: -92.2211173015, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.34843444824\n",
      "[NOR] Episode: 18620, Length: 129, Avg Reward: 5.25750499768, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.68643188477\n",
      "[NOR] Episode: 18630, Length: 1000, Avg Reward: -0.299206286749, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.2656011581\n",
      "[NOR] Episode: 18640, Length: 1000, Avg Reward: -18.3489844031, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -57.1984519958\n",
      "[NOR] Episode: 18650, Length: 1000, Avg Reward: -54.9983021594, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.46595668793\n",
      "[NOR] Episode: 18660, Length: 1000, Avg Reward: -71.2841651788, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.47496247292\n",
      "[NOR] Episode: 18670, Length: 1000, Avg Reward: -69.4088710721, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.52531385422\n",
      "[NOR] Episode: 18680, Length: 1000, Avg Reward: -76.2621857352, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.30959224701\n",
      "[NOR] Episode: 18690, Length: 485, Avg Reward: -123.285940771, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.9633054733\n",
      "[NOR] Episode: 18700, Length: 1000, Avg Reward: -119.654943165, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.5144071579\n",
      "[NOR] Episode: 18710, Length: 972, Avg Reward: -89.3852828098, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -87.6647491455\n",
      "[NOR] Episode: 18720, Length: 252, Avg Reward: -77.2036622366, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.81036281586\n",
      "[NOR] Episode: 18730, Length: 128, Avg Reward: -80.3012433215, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.40141558647\n",
      "[NOR] Episode: 18740, Length: 162, Avg Reward: -191.434677912, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -20.1710624695\n",
      "[NOR] Episode: 18750, Length: 519, Avg Reward: -222.206570687, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.94463682175\n",
      "[NOR] Episode: 18760, Length: 118, Avg Reward: -244.280914241, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.30072593689\n",
      "[NOR] Episode: 18770, Length: 239, Avg Reward: -214.769541046, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.56230473518\n",
      "[NOR] Episode: 18780, Length: 174, Avg Reward: -211.702376417, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.05230855942\n",
      "[NOR] Episode: 18790, Length: 87, Avg Reward: -228.982664884, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.20345020294\n",
      "[NOR] Episode: 18800, Length: 125, Avg Reward: -313.017557971, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.35513782501\n",
      "[NOR] Episode: 18810, Length: 84, Avg Reward: -341.029492657, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.99621629715\n",
      "[NOR] Episode: 18820, Length: 62, Avg Reward: -197.134920523, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.07133030891\n",
      "[NOR] Episode: 18830, Length: 98, Avg Reward: -291.456632284, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.4796295166\n",
      "[NOR] Episode: 18840, Length: 400, Avg Reward: -223.457977704, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0414042472839\n",
      "[NOR] Episode: 18850, Length: 186, Avg Reward: -189.547447631, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -41.7630157471\n",
      "[NOR] Episode: 18860, Length: 164, Avg Reward: -211.58562208, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.4674777985\n",
      "[NOR] Episode: 18870, Length: 177, Avg Reward: -220.99271299, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -39.4907608032\n",
      "[NOR] Episode: 18880, Length: 238, Avg Reward: -206.157310591, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -16.7761116028\n",
      "[NOR] Episode: 18890, Length: 330, Avg Reward: -114.147789794, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.01686382294\n",
      "[NOR] Episode: 18900, Length: 220, Avg Reward: -178.223464104, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.34019565582\n",
      "[NOR] Episode: 18910, Length: 140, Avg Reward: -145.919167418, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.7453184128\n",
      "[NOR] Episode: 18920, Length: 167, Avg Reward: -209.816670792, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -26.6845283508\n",
      "[NOR] Episode: 18930, Length: 136, Avg Reward: -232.198468806, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.06376075745\n",
      "[NOR] Episode: 18940, Length: 90, Avg Reward: -221.713892383, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.26253414154\n",
      "[NOR] Episode: 18950, Length: 91, Avg Reward: -204.448896201, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.65724277496\n",
      "[NOR] Episode: 18960, Length: 158, Avg Reward: -243.420513533, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.67024087906\n",
      "[NOR] Episode: 18970, Length: 141, Avg Reward: -230.661244154, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 30.4265022278\n",
      "[NOR] Episode: 18980, Length: 214, Avg Reward: -160.476544135, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 170.042938232\n",
      "[NOR] Episode: 18990, Length: 149, Avg Reward: -205.641989448, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.8742904663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 08:25:13,894] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video019000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 19000, Length: 124, Avg Reward: -195.869054824, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -18.1347846985\n",
      "[NOR] Episode: 19010, Length: 148, Avg Reward: -231.741754073, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.7635269165\n",
      "[NOR] Episode: 19020, Length: 168, Avg Reward: -254.279331777, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -13.1024637222\n",
      "[NOR] Episode: 19030, Length: 87, Avg Reward: -237.943811065, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -71.9389190674\n",
      "[NOR] Episode: 19040, Length: 151, Avg Reward: -253.455171794, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.72317218781\n",
      "[NOR] Episode: 19050, Length: 239, Avg Reward: -225.247301633, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.378988742828\n",
      "[NOR] Episode: 19060, Length: 572, Avg Reward: -129.081639319, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.95998334885\n",
      "[NOR] Episode: 19070, Length: 552, Avg Reward: -65.365335031, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 29.6887626648\n",
      "[NOR] Episode: 19080, Length: 477, Avg Reward: -86.0611939932, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -72.4256362915\n",
      "[NOR] Episode: 19090, Length: 232, Avg Reward: -43.8244493729, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.9556846619\n",
      "[NOR] Episode: 19100, Length: 197, Avg Reward: -146.305294719, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -12.012843132\n",
      "[NOR] Episode: 19110, Length: 209, Avg Reward: -124.755437643, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.38348388672\n",
      "[NOR] Episode: 19120, Length: 1000, Avg Reward: -130.191242769, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -38.9936981201\n",
      "[NOR] Episode: 19130, Length: 1000, Avg Reward: -146.073973497, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 238.388244629\n",
      "[NOR] Episode: 19140, Length: 122, Avg Reward: -158.175131525, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -48.2343940735\n",
      "[NOR] Episode: 19150, Length: 176, Avg Reward: -312.730167716, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -32.268913269\n",
      "[NOR] Episode: 19160, Length: 126, Avg Reward: -315.602599307, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 107.593322754\n",
      "[NOR] Episode: 19170, Length: 149, Avg Reward: -313.676915256, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.5400686264\n",
      "[NOR] Episode: 19180, Length: 146, Avg Reward: -374.079941831, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 25.6547660828\n",
      "[NOR] Episode: 19190, Length: 135, Avg Reward: -383.968208977, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -19.1952133179\n",
      "[NOR] Episode: 19200, Length: 141, Avg Reward: -381.442807767, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.64105796814\n",
      "[NOR] Episode: 19210, Length: 134, Avg Reward: -404.910111096, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -136.326507568\n",
      "[NOR] Episode: 19220, Length: 178, Avg Reward: -393.866345422, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -52.8366203308\n",
      "[NOR] Episode: 19230, Length: 176, Avg Reward: -347.783952897, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -19.0939979553\n",
      "[NOR] Episode: 19240, Length: 114, Avg Reward: -340.892280381, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.52264022827\n",
      "[NOR] Episode: 19250, Length: 137, Avg Reward: -388.198380493, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 172.527679443\n",
      "[NOR] Episode: 19260, Length: 133, Avg Reward: -370.231220741, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -31.0249576569\n",
      "[NOR] Episode: 19270, Length: 125, Avg Reward: -311.472963838, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 26.2541599274\n",
      "[NOR] Episode: 19280, Length: 110, Avg Reward: -379.602815867, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.71960258484\n",
      "[NOR] Episode: 19290, Length: 202, Avg Reward: -333.912403994, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.1339764595\n",
      "[NOR] Episode: 19300, Length: 203, Avg Reward: -361.977490886, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -74.1797027588\n",
      "[NOR] Episode: 19310, Length: 180, Avg Reward: -384.067353916, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -39.7733459473\n",
      "[NOR] Episode: 19320, Length: 165, Avg Reward: -390.091717337, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.0547218323\n",
      "[NOR] Episode: 19330, Length: 89, Avg Reward: -374.072094359, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -30.5583610535\n",
      "[NOR] Episode: 19340, Length: 89, Avg Reward: -470.516789332, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 81.638710022\n",
      "[NOR] Episode: 19350, Length: 126, Avg Reward: -467.632794242, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.60562896729\n",
      "[NOR] Episode: 19360, Length: 149, Avg Reward: -463.609158794, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 104.743408203\n",
      "[NOR] Episode: 19370, Length: 105, Avg Reward: -434.557578852, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -29.8733596802\n",
      "[NOR] Episode: 19380, Length: 170, Avg Reward: -475.845938907, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -42.4414024353\n",
      "[NOR] Episode: 19390, Length: 121, Avg Reward: -407.821763133, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.9053401947\n",
      "[NOR] Episode: 19400, Length: 133, Avg Reward: -563.479114547, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.6840362549\n",
      "[NOR] Episode: 19410, Length: 102, Avg Reward: -472.074142224, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.19179582596\n",
      "[NOR] Episode: 19420, Length: 107, Avg Reward: -492.653590528, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -38.9600105286\n",
      "[NOR] Episode: 19430, Length: 155, Avg Reward: -532.092881138, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.205076217651\n",
      "[NOR] Episode: 19440, Length: 137, Avg Reward: -531.493942592, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -87.1945571899\n",
      "[NOR] Episode: 19450, Length: 151, Avg Reward: -521.675872484, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -37.8129081726\n",
      "[NOR] Episode: 19460, Length: 180, Avg Reward: -482.77515098, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -35.4455070496\n",
      "[NOR] Episode: 19470, Length: 164, Avg Reward: -488.011340738, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 78.5159759521\n",
      "[NOR] Episode: 19480, Length: 205, Avg Reward: -551.13315076, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -21.2233734131\n",
      "[NOR] Episode: 19490, Length: 85, Avg Reward: -489.9459425, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -35.4636917114\n",
      "[NOR] Episode: 19500, Length: 96, Avg Reward: -490.379806777, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -25.6868553162\n",
      "[NOR] Episode: 19510, Length: 96, Avg Reward: -526.54665233, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -17.9157161713\n",
      "[NOR] Episode: 19520, Length: 194, Avg Reward: -447.759722842, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -18.1859359741\n",
      "[NOR] Episode: 19530, Length: 133, Avg Reward: -379.297428106, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.9394721985\n",
      "[NOR] Episode: 19540, Length: 245, Avg Reward: -431.11802725, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -12.7442455292\n",
      "[NOR] Episode: 19550, Length: 131, Avg Reward: -410.516506385, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.67287826538\n",
      "[NOR] Episode: 19560, Length: 109, Avg Reward: -385.306235976, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.73764801025\n",
      "[NOR] Episode: 19570, Length: 153, Avg Reward: -350.585373051, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.5654773712\n",
      "[NOR] Episode: 19580, Length: 136, Avg Reward: -352.723494417, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -21.6504898071\n",
      "[NOR] Episode: 19590, Length: 173, Avg Reward: -461.435075341, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -23.1964092255\n",
      "[NOR] Episode: 19600, Length: 180, Avg Reward: -486.920185381, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -33.5051345825\n",
      "[NOR] Episode: 19610, Length: 138, Avg Reward: -477.910359718, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -17.4110832214\n",
      "[NOR] Episode: 19620, Length: 131, Avg Reward: -461.495728834, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 75.1344604492\n",
      "[NOR] Episode: 19630, Length: 117, Avg Reward: -461.459720385, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0269658565521\n",
      "[NOR] Episode: 19640, Length: 114, Avg Reward: -380.403107062, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -27.1415596008\n",
      "[NOR] Episode: 19650, Length: 91, Avg Reward: -429.607697617, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -22.0898475647\n",
      "[NOR] Episode: 19660, Length: 126, Avg Reward: -473.995071613, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -28.4233474731\n",
      "[NOR] Episode: 19670, Length: 144, Avg Reward: -504.950602558, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -21.902135849\n",
      "[NOR] Episode: 19680, Length: 160, Avg Reward: -500.46977019, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.9724845886\n",
      "[NOR] Episode: 19690, Length: 139, Avg Reward: -555.990716755, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -67.9593811035\n",
      "[NOR] Episode: 19700, Length: 118, Avg Reward: -465.848721919, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -15.8823471069\n",
      "[NOR] Episode: 19710, Length: 109, Avg Reward: -453.191269238, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -42.8716583252\n",
      "[NOR] Episode: 19720, Length: 125, Avg Reward: -475.953972865, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.12843465805\n",
      "[NOR] Episode: 19730, Length: 138, Avg Reward: -461.995446298, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -16.9360809326\n",
      "[NOR] Episode: 19740, Length: 116, Avg Reward: -463.86510613, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -19.5924224854\n",
      "[NOR] Episode: 19750, Length: 151, Avg Reward: -450.270536772, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -38.8276596069\n",
      "[NOR] Episode: 19760, Length: 101, Avg Reward: -389.722935209, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -27.1241874695\n",
      "[NOR] Episode: 19770, Length: 99, Avg Reward: -501.853157116, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.81617069244\n",
      "[NOR] Episode: 19780, Length: 160, Avg Reward: -476.4911453, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.349103927612\n",
      "[NOR] Episode: 19790, Length: 115, Avg Reward: -489.400279733, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -16.4607486725\n",
      "[NOR] Episode: 19800, Length: 132, Avg Reward: -439.160161109, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.3902626038\n",
      "[NOR] Episode: 19810, Length: 171, Avg Reward: -456.350244363, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.38520050049\n",
      "[NOR] Episode: 19820, Length: 139, Avg Reward: -480.251094036, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -20.2101421356\n",
      "[NOR] Episode: 19830, Length: 103, Avg Reward: -392.799510585, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 28.142162323\n",
      "[NOR] Episode: 19840, Length: 129, Avg Reward: -418.290602256, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.68368148804\n",
      "[NOR] Episode: 19850, Length: 106, Avg Reward: -415.541553369, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.71181297302\n",
      "[NOR] Episode: 19860, Length: 107, Avg Reward: -426.75161486, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -31.1861991882\n",
      "[NOR] Episode: 19870, Length: 127, Avg Reward: -444.740448403, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.49823284149\n",
      "[NOR] Episode: 19880, Length: 185, Avg Reward: -397.209592576, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.67946004868\n",
      "[NOR] Episode: 19890, Length: 99, Avg Reward: -391.163507065, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 43.4717788696\n",
      "[NOR] Episode: 19900, Length: 121, Avg Reward: -439.538920013, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.05729579926\n",
      "[NOR] Episode: 19910, Length: 128, Avg Reward: -470.686509512, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -26.0434207916\n",
      "[NOR] Episode: 19920, Length: 91, Avg Reward: -462.465523459, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.27498245239\n",
      "[NOR] Episode: 19930, Length: 123, Avg Reward: -484.337722742, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -27.7673168182\n",
      "[NOR] Episode: 19940, Length: 187, Avg Reward: -445.783968828, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.82724285126\n",
      "[NOR] Episode: 19950, Length: 79, Avg Reward: -449.582731351, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -19.1910552979\n",
      "[NOR] Episode: 19960, Length: 93, Avg Reward: -441.061807159, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.03172302246\n",
      "[NOR] Episode: 19970, Length: 111, Avg Reward: -433.170375886, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.1259632111\n",
      "[NOR] Episode: 19980, Length: 206, Avg Reward: -369.062448674, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.1766681671\n",
      "[NOR] Episode: 19990, Length: 160, Avg Reward: -370.431965468, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.27562332153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 08:35:09,639] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video020000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 20000, Length: 250, Avg Reward: -342.316351553, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.2268648148\n",
      "[NOR] Episode: 20010, Length: 173, Avg Reward: -362.701255621, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 434.938110352\n",
      "[NOR] Episode: 20020, Length: 158, Avg Reward: -340.884399798, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 30.1310310364\n",
      "[NOR] Episode: 20030, Length: 129, Avg Reward: -389.272831052, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 38.6866226196\n",
      "[NOR] Episode: 20040, Length: 158, Avg Reward: -408.531975409, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.4561829567\n",
      "[NOR] Episode: 20050, Length: 132, Avg Reward: -379.74794836, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -13.6854457855\n",
      "[NOR] Episode: 20060, Length: 97, Avg Reward: -415.202742878, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.3174962997\n",
      "[NOR] Episode: 20070, Length: 204, Avg Reward: -357.674582593, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.9281797409\n",
      "[NOR] Episode: 20080, Length: 117, Avg Reward: -377.582859195, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -46.4504699707\n",
      "[NOR] Episode: 20090, Length: 115, Avg Reward: -395.40371393, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -41.4930229187\n",
      "[NOR] Episode: 20100, Length: 126, Avg Reward: -366.199852489, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.89281368256\n",
      "[NOR] Episode: 20110, Length: 89, Avg Reward: -365.876295317, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 212.825668335\n",
      "[NOR] Episode: 20120, Length: 87, Avg Reward: -388.971729113, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 64.4175567627\n",
      "[NOR] Episode: 20130, Length: 94, Avg Reward: -406.993694478, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.559658050537\n",
      "[NOR] Episode: 20140, Length: 98, Avg Reward: -393.728147356, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -23.1703338623\n",
      "[NOR] Episode: 20150, Length: 145, Avg Reward: -424.754770518, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.1668167114\n",
      "[NOR] Episode: 20160, Length: 122, Avg Reward: -437.278246231, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.61080694199\n",
      "[NOR] Episode: 20170, Length: 192, Avg Reward: -421.926847606, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.68574810028\n",
      "[NOR] Episode: 20180, Length: 100, Avg Reward: -441.448802627, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.81734704971\n",
      "[NOR] Episode: 20190, Length: 95, Avg Reward: -463.135399801, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.11102819443\n",
      "[NOR] Episode: 20200, Length: 161, Avg Reward: -471.294994488, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 33.4940338135\n",
      "[NOR] Episode: 20210, Length: 124, Avg Reward: -466.71487526, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.8133687973\n",
      "[NOR] Episode: 20220, Length: 83, Avg Reward: -441.056059866, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.4939594269\n",
      "[NOR] Episode: 20230, Length: 75, Avg Reward: -417.474234666, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.306810379\n",
      "[NOR] Episode: 20240, Length: 130, Avg Reward: -441.132174415, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.7981095314\n",
      "[NOR] Episode: 20250, Length: 181, Avg Reward: -421.113653424, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.6301002502\n",
      "[NOR] Episode: 20260, Length: 111, Avg Reward: -427.79180623, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.0247039795\n",
      "[NOR] Episode: 20270, Length: 124, Avg Reward: -450.08037248, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.444717884064\n",
      "[NOR] Episode: 20280, Length: 85, Avg Reward: -359.616280655, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.894826889\n",
      "[NOR] Episode: 20290, Length: 82, Avg Reward: -430.964720863, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.05124330521\n",
      "[NOR] Episode: 20300, Length: 102, Avg Reward: -414.856480703, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -42.8053131104\n",
      "[NOR] Episode: 20310, Length: 82, Avg Reward: -356.758342749, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -20.1431751251\n",
      "[NOR] Episode: 20320, Length: 77, Avg Reward: -371.498181721, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.14787483215\n",
      "[NOR] Episode: 20330, Length: 119, Avg Reward: -376.002779508, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.7735424042\n",
      "[NOR] Episode: 20340, Length: 67, Avg Reward: -324.584054409, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.95016384125\n",
      "[NOR] Episode: 20350, Length: 101, Avg Reward: -373.401964903, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.9738483429\n",
      "[NOR] Episode: 20360, Length: 172, Avg Reward: -368.181323836, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 104.016998291\n",
      "[NOR] Episode: 20370, Length: 86, Avg Reward: -354.092678056, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -12.8946628571\n",
      "[NOR] Episode: 20380, Length: 119, Avg Reward: -380.041706433, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 30.3464050293\n",
      "[NOR] Episode: 20390, Length: 85, Avg Reward: -380.017930318, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.288922071457\n",
      "[NOR] Episode: 20400, Length: 111, Avg Reward: -379.489701644, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.819697380066\n",
      "[NOR] Episode: 20410, Length: 198, Avg Reward: -438.775223667, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 29.1891098022\n",
      "[NOR] Episode: 20420, Length: 79, Avg Reward: -366.406803119, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 108.940330505\n",
      "[NOR] Episode: 20430, Length: 138, Avg Reward: -373.353687063, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 203.8046875\n",
      "[NOR] Episode: 20440, Length: 84, Avg Reward: -363.65805754, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.13547325134\n",
      "[NOR] Episode: 20450, Length: 92, Avg Reward: -440.516935816, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 239.7371521\n",
      "[NOR] Episode: 20460, Length: 100, Avg Reward: -417.06870063, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 131.584671021\n",
      "[NOR] Episode: 20470, Length: 96, Avg Reward: -421.664563903, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.64145469666\n",
      "[NOR] Episode: 20480, Length: 89, Avg Reward: -364.405403179, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.08009719849\n",
      "[NOR] Episode: 20490, Length: 90, Avg Reward: -372.242274432, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.450382232666\n",
      "[NOR] Episode: 20500, Length: 67, Avg Reward: -405.76291278, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.3699178696\n",
      "[NOR] Episode: 20510, Length: 80, Avg Reward: -387.185229847, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.33999252319\n",
      "[NOR] Episode: 20520, Length: 299, Avg Reward: -339.604552602, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.977514267\n",
      "[NOR] Episode: 20530, Length: 123, Avg Reward: -354.17594753, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 38.6140975952\n",
      "[NOR] Episode: 20540, Length: 83, Avg Reward: -364.554148066, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.43940973282\n",
      "[NOR] Episode: 20550, Length: 124, Avg Reward: -329.9247259, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.2693061829\n",
      "[NOR] Episode: 20560, Length: 128, Avg Reward: -363.856393909, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 27.4067211151\n",
      "[NOR] Episode: 20570, Length: 115, Avg Reward: -319.122247206, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.1593484879\n",
      "[NOR] Episode: 20580, Length: 97, Avg Reward: -322.005122913, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 115.351486206\n",
      "[NOR] Episode: 20590, Length: 84, Avg Reward: -283.433280899, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 27.8423576355\n",
      "[NOR] Episode: 20600, Length: 78, Avg Reward: -283.215062585, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 251.342956543\n",
      "[NOR] Episode: 20610, Length: 72, Avg Reward: -294.61274739, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 496.2265625\n",
      "[NOR] Episode: 20620, Length: 119, Avg Reward: -307.127685555, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.24545884132\n",
      "[NOR] Episode: 20630, Length: 109, Avg Reward: -281.059624064, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.57381486893\n",
      "[NOR] Episode: 20640, Length: 127, Avg Reward: -273.344459707, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.38051080704\n",
      "[NOR] Episode: 20650, Length: 113, Avg Reward: -294.440869319, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.60074210167\n",
      "[NOR] Episode: 20660, Length: 101, Avg Reward: -295.503491081, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.0036277771\n",
      "[NOR] Episode: 20670, Length: 59, Avg Reward: -308.131935564, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.09249210358\n",
      "[NOR] Episode: 20680, Length: 102, Avg Reward: -260.827424118, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.33950614929\n",
      "[NOR] Episode: 20690, Length: 68, Avg Reward: -291.665216334, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.6771688461\n",
      "[NOR] Episode: 20700, Length: 186, Avg Reward: -291.168551932, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 30.855550766\n",
      "[NOR] Episode: 20710, Length: 136, Avg Reward: -312.159278667, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.54411315918\n",
      "[NOR] Episode: 20720, Length: 82, Avg Reward: -345.588706237, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 173.14024353\n",
      "[NOR] Episode: 20730, Length: 121, Avg Reward: -336.49003047, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.2201089859\n",
      "[NOR] Episode: 20740, Length: 111, Avg Reward: -302.176980885, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.4965486526\n",
      "[NOR] Episode: 20750, Length: 83, Avg Reward: -301.3471969, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.4789018631\n",
      "[NOR] Episode: 20760, Length: 107, Avg Reward: -310.738047115, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.9885444641\n",
      "[NOR] Episode: 20770, Length: 104, Avg Reward: -320.821402228, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -39.3299942017\n",
      "[NOR] Episode: 20780, Length: 83, Avg Reward: -298.699896579, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.8973369598\n",
      "[NOR] Episode: 20790, Length: 81, Avg Reward: -315.438121756, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.49073982239\n",
      "[NOR] Episode: 20800, Length: 80, Avg Reward: -296.264048462, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.78578090668\n",
      "[NOR] Episode: 20810, Length: 73, Avg Reward: -300.318806726, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -13.1394357681\n",
      "[NOR] Episode: 20820, Length: 170, Avg Reward: -272.549168623, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.9613513947\n",
      "[NOR] Episode: 20830, Length: 94, Avg Reward: -304.899372891, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.057674408\n",
      "[NOR] Episode: 20840, Length: 96, Avg Reward: -328.490689659, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.2956008911\n",
      "[NOR] Episode: 20850, Length: 115, Avg Reward: -309.659853677, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 44.7895851135\n",
      "[NOR] Episode: 20860, Length: 101, Avg Reward: -281.222002819, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.79276657104\n",
      "[NOR] Episode: 20870, Length: 112, Avg Reward: -316.084382016, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 122.88104248\n",
      "[NOR] Episode: 20880, Length: 149, Avg Reward: -262.275799027, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 257.685974121\n",
      "[NOR] Episode: 20890, Length: 150, Avg Reward: -344.681657908, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.0021705627\n",
      "[NOR] Episode: 20900, Length: 167, Avg Reward: -343.252565674, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 134.634414673\n",
      "[NOR] Episode: 20910, Length: 103, Avg Reward: -343.864946819, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.9148578644\n",
      "[NOR] Episode: 20920, Length: 93, Avg Reward: -300.268680382, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.48838090897\n",
      "[NOR] Episode: 20930, Length: 106, Avg Reward: -390.654658587, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 98.2015380859\n",
      "[NOR] Episode: 20940, Length: 168, Avg Reward: -408.601988521, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.41910409927\n",
      "[NOR] Episode: 20950, Length: 92, Avg Reward: -349.852196752, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.1872634888\n",
      "[NOR] Episode: 20960, Length: 245, Avg Reward: -400.352598047, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.40796279907\n",
      "[NOR] Episode: 20970, Length: 172, Avg Reward: -362.817916355, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 240.746856689\n",
      "[NOR] Episode: 20980, Length: 100, Avg Reward: -373.574557966, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.7004203796\n",
      "[NOR] Episode: 20990, Length: 108, Avg Reward: -385.098734402, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.133272171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 08:42:36,456] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video021000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 21000, Length: 213, Avg Reward: -358.061485808, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -15.6661624908\n",
      "[NOR] Episode: 21010, Length: 283, Avg Reward: -337.774905588, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 177.033477783\n",
      "[NOR] Episode: 21020, Length: 137, Avg Reward: -374.745019859, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.632768631\n",
      "[NOR] Episode: 21030, Length: 138, Avg Reward: -281.523529416, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 354.779785156\n",
      "[NOR] Episode: 21040, Length: 221, Avg Reward: -327.830600287, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.66844081879\n",
      "[NOR] Episode: 21050, Length: 173, Avg Reward: -357.020274682, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -36.9437599182\n",
      "[NOR] Episode: 21060, Length: 226, Avg Reward: -324.827763337, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -15.4343738556\n",
      "[NOR] Episode: 21070, Length: 183, Avg Reward: -273.940141896, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -31.2561149597\n",
      "[NOR] Episode: 21080, Length: 110, Avg Reward: -342.247886052, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.2726650238\n",
      "[NOR] Episode: 21090, Length: 98, Avg Reward: -311.916874709, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 472.677612305\n",
      "[NOR] Episode: 21100, Length: 102, Avg Reward: -338.285447445, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.84285235405\n",
      "[NOR] Episode: 21110, Length: 90, Avg Reward: -345.129496841, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.18405461311\n",
      "[NOR] Episode: 21120, Length: 99, Avg Reward: -306.695149165, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.6373767853\n",
      "[NOR] Episode: 21130, Length: 106, Avg Reward: -336.698682917, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 281.909606934\n",
      "[NOR] Episode: 21140, Length: 99, Avg Reward: -326.616814122, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.43549442291\n",
      "[NOR] Episode: 21150, Length: 93, Avg Reward: -347.762410576, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.96608066559\n",
      "[NOR] Episode: 21160, Length: 108, Avg Reward: -333.307454855, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 35.3125953674\n",
      "[NOR] Episode: 21170, Length: 98, Avg Reward: -308.163403904, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -17.4837112427\n",
      "[NOR] Episode: 21180, Length: 154, Avg Reward: -290.175138367, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -13.9326705933\n",
      "[NOR] Episode: 21190, Length: 127, Avg Reward: -285.878119679, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.1878967285\n",
      "[NOR] Episode: 21200, Length: 130, Avg Reward: -322.18026553, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.79565525055\n",
      "[NOR] Episode: 21210, Length: 132, Avg Reward: -351.62760303, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.35793876648\n",
      "[NOR] Episode: 21220, Length: 121, Avg Reward: -342.95526337, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -75.765335083\n",
      "[NOR] Episode: 21230, Length: 146, Avg Reward: -353.353239826, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.47819423676\n",
      "[NOR] Episode: 21240, Length: 83, Avg Reward: -318.465481202, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 28.9887008667\n",
      "[NOR] Episode: 21250, Length: 126, Avg Reward: -317.612223002, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.86976337433\n",
      "[NOR] Episode: 21260, Length: 173, Avg Reward: -353.788140476, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.127537846565\n",
      "[NOR] Episode: 21270, Length: 182, Avg Reward: -326.795421695, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.1665220261\n",
      "[NOR] Episode: 21280, Length: 119, Avg Reward: -299.506215919, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.03135728836\n",
      "[NOR] Episode: 21290, Length: 111, Avg Reward: -296.611773466, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.3864660263\n",
      "[NOR] Episode: 21300, Length: 115, Avg Reward: -267.876470074, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.9638023376\n",
      "[NOR] Episode: 21310, Length: 126, Avg Reward: -300.719739981, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.42911195755\n",
      "[NOR] Episode: 21320, Length: 110, Avg Reward: -366.215358643, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -13.2182598114\n",
      "[NOR] Episode: 21330, Length: 145, Avg Reward: -352.312695346, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 33.5075187683\n",
      "[NOR] Episode: 21340, Length: 79, Avg Reward: -289.79766289, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.0731420517\n",
      "[NOR] Episode: 21350, Length: 120, Avg Reward: -271.500343071, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.65637683868\n",
      "[NOR] Episode: 21360, Length: 100, Avg Reward: -177.44679258, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.17607736588\n",
      "[NOR] Episode: 21370, Length: 67, Avg Reward: -186.18742831, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -47.4522132874\n",
      "[NOR] Episode: 21380, Length: 105, Avg Reward: -168.847788354, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.13116455078\n",
      "[NOR] Episode: 21390, Length: 106, Avg Reward: -209.743983505, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.1203985214\n",
      "[NOR] Episode: 21400, Length: 97, Avg Reward: -164.818104926, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 28.9513664246\n",
      "[NOR] Episode: 21410, Length: 104, Avg Reward: -161.222945274, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 28.5651340485\n",
      "[NOR] Episode: 21420, Length: 91, Avg Reward: -192.46123243, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 121.884384155\n",
      "[NOR] Episode: 21430, Length: 118, Avg Reward: -181.516257135, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.29110360146\n",
      "[NOR] Episode: 21440, Length: 70, Avg Reward: -202.210360427, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 149.490493774\n",
      "[NOR] Episode: 21450, Length: 102, Avg Reward: -161.784731384, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.9919662476\n",
      "[NOR] Episode: 21460, Length: 104, Avg Reward: -201.641564617, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.11115455627\n",
      "[NOR] Episode: 21470, Length: 91, Avg Reward: -232.303309946, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.05230569839\n",
      "[NOR] Episode: 21480, Length: 150, Avg Reward: -224.277478993, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.07636022568\n",
      "[NOR] Episode: 21490, Length: 93, Avg Reward: -196.528191687, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.53953647614\n",
      "[NOR] Episode: 21500, Length: 72, Avg Reward: -232.979454147, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 411.499572754\n",
      "[NOR] Episode: 21510, Length: 119, Avg Reward: -258.319859589, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -17.3506374359\n",
      "[NOR] Episode: 21520, Length: 87, Avg Reward: -262.219870393, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.115012645721\n",
      "[NOR] Episode: 21530, Length: 88, Avg Reward: -223.573364624, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.24401187897\n",
      "[NOR] Episode: 21540, Length: 142, Avg Reward: -288.84546911, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.99551963806\n",
      "[NOR] Episode: 21550, Length: 71, Avg Reward: -328.46352801, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 48.2084197998\n",
      "[NOR] Episode: 21560, Length: 85, Avg Reward: -193.919531083, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 244.153991699\n",
      "[NOR] Episode: 21570, Length: 104, Avg Reward: -239.004610648, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.85030698776\n",
      "[NOR] Episode: 21580, Length: 92, Avg Reward: -227.117853907, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.6142997742\n",
      "[NOR] Episode: 21590, Length: 80, Avg Reward: -214.962009836, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.86577606201\n",
      "[NOR] Episode: 21600, Length: 70, Avg Reward: -289.716520252, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.30399942398\n",
      "[NOR] Episode: 21610, Length: 196, Avg Reward: -325.013127036, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.17474937439\n",
      "[NOR] Episode: 21620, Length: 252, Avg Reward: -283.579865333, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.610543608665\n",
      "[NOR] Episode: 21630, Length: 282, Avg Reward: -299.025848668, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.2347335815\n",
      "[NOR] Episode: 21640, Length: 207, Avg Reward: -368.424616886, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -101.758834839\n",
      "[NOR] Episode: 21650, Length: 141, Avg Reward: -323.204136826, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.53385400772\n",
      "[NOR] Episode: 21660, Length: 180, Avg Reward: -245.242137998, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.45109224319\n",
      "[NOR] Episode: 21670, Length: 144, Avg Reward: -231.454031022, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.7019004822\n",
      "[NOR] Episode: 21680, Length: 116, Avg Reward: -183.365847723, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.46624946594\n",
      "[NOR] Episode: 21690, Length: 159, Avg Reward: -241.022205029, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 211.743881226\n",
      "[NOR] Episode: 21700, Length: 107, Avg Reward: -258.421243568, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.67481851578\n",
      "[NOR] Episode: 21710, Length: 149, Avg Reward: -178.289421058, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 266.543426514\n",
      "[NOR] Episode: 21720, Length: 128, Avg Reward: -143.700659257, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.58166503906\n",
      "[NOR] Episode: 21730, Length: 97, Avg Reward: -212.238937458, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.6735038757\n",
      "[NOR] Episode: 21740, Length: 108, Avg Reward: -221.52015158, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.6900100708\n",
      "[NOR] Episode: 21750, Length: 124, Avg Reward: -268.69263281, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.383796811104\n",
      "[NOR] Episode: 21760, Length: 127, Avg Reward: -250.015953355, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.80393695831\n",
      "[NOR] Episode: 21770, Length: 155, Avg Reward: -263.230248127, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.8662338257\n",
      "[NOR] Episode: 21780, Length: 87, Avg Reward: -235.635607396, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.3345632553\n",
      "[NOR] Episode: 21790, Length: 139, Avg Reward: -295.938501492, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.17465877533\n",
      "[NOR] Episode: 21800, Length: 102, Avg Reward: -282.690327588, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.7216691971\n",
      "[NOR] Episode: 21810, Length: 92, Avg Reward: -260.983822178, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.63797521591\n",
      "[NOR] Episode: 21820, Length: 109, Avg Reward: -339.389365905, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.58672332764\n",
      "[NOR] Episode: 21830, Length: 96, Avg Reward: -303.261374212, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.031498909\n",
      "[NOR] Episode: 21840, Length: 125, Avg Reward: -304.227424915, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 199.236175537\n",
      "[NOR] Episode: 21850, Length: 73, Avg Reward: -356.315955876, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.393355846405\n",
      "[NOR] Episode: 21860, Length: 69, Avg Reward: -312.209206796, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.5136833191\n",
      "[NOR] Episode: 21870, Length: 81, Avg Reward: -329.137625574, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.46634244919\n",
      "[NOR] Episode: 21880, Length: 94, Avg Reward: -356.497081042, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.2095413208\n",
      "[NOR] Episode: 21890, Length: 78, Avg Reward: -272.208259569, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 29.9194831848\n",
      "[NOR] Episode: 21900, Length: 78, Avg Reward: -310.682748212, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 203.259048462\n",
      "[NOR] Episode: 21910, Length: 96, Avg Reward: -253.278784543, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.5292892456\n",
      "[NOR] Episode: 21920, Length: 97, Avg Reward: -228.761126086, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 248.72555542\n",
      "[NOR] Episode: 21930, Length: 98, Avg Reward: -352.519897695, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.2388019562\n",
      "[NOR] Episode: 21940, Length: 107, Avg Reward: -372.984613793, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.48589468002\n",
      "[NOR] Episode: 21950, Length: 80, Avg Reward: -156.828637078, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -16.4989566803\n",
      "[NOR] Episode: 21960, Length: 90, Avg Reward: -252.289339604, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.08279204369\n",
      "[NOR] Episode: 21970, Length: 93, Avg Reward: -199.826312059, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.02054691315\n",
      "[NOR] Episode: 21980, Length: 87, Avg Reward: -262.793811416, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.1538438797\n",
      "[NOR] Episode: 21990, Length: 111, Avg Reward: -191.04862503, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.91723918915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 08:49:55,026] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video022000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 22000, Length: 81, Avg Reward: -178.792387346, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 25.3762607574\n",
      "[NOR] Episode: 22010, Length: 100, Avg Reward: -197.878988606, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -34.1102561951\n",
      "[NOR] Episode: 22020, Length: 78, Avg Reward: -221.681707943, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -26.0618057251\n",
      "[NOR] Episode: 22030, Length: 116, Avg Reward: -281.55545436, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.104706287384\n",
      "[NOR] Episode: 22040, Length: 96, Avg Reward: -265.06301879, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.11439800262\n",
      "[NOR] Episode: 22050, Length: 113, Avg Reward: -255.653339285, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.18973731995\n",
      "[NOR] Episode: 22060, Length: 71, Avg Reward: -283.010512224, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -16.6288948059\n",
      "[NOR] Episode: 22070, Length: 77, Avg Reward: -193.29971561, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -22.5575752258\n",
      "[NOR] Episode: 22080, Length: 87, Avg Reward: -314.242222114, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 294.794067383\n",
      "[NOR] Episode: 22090, Length: 77, Avg Reward: -218.561786759, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.4916286469\n",
      "[NOR] Episode: 22100, Length: 76, Avg Reward: -198.886383837, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.35769367218\n",
      "[NOR] Episode: 22110, Length: 90, Avg Reward: -189.199839286, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.1476812363\n",
      "[NOR] Episode: 22120, Length: 106, Avg Reward: -189.820800646, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 47.1685523987\n",
      "[NOR] Episode: 22130, Length: 81, Avg Reward: -234.216595117, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.11611294746\n",
      "[NOR] Episode: 22140, Length: 94, Avg Reward: -182.872391107, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.5833272934\n",
      "[NOR] Episode: 22150, Length: 85, Avg Reward: -178.496100464, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.65337562561\n",
      "[NOR] Episode: 22160, Length: 75, Avg Reward: -150.908393159, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.3400478363\n",
      "[NOR] Episode: 22170, Length: 138, Avg Reward: -171.692087291, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.65573883057\n",
      "[NOR] Episode: 22180, Length: 78, Avg Reward: -127.679886764, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.916179657\n",
      "[NOR] Episode: 22190, Length: 63, Avg Reward: -195.656426908, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.33087921143\n",
      "[NOR] Episode: 22200, Length: 65, Avg Reward: -226.666392526, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.23692321777\n",
      "[NOR] Episode: 22210, Length: 98, Avg Reward: -202.490632242, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -13.9459762573\n",
      "[NOR] Episode: 22220, Length: 67, Avg Reward: -179.809533167, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.53149580956\n",
      "[NOR] Episode: 22230, Length: 90, Avg Reward: -191.857939288, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.68628978729\n",
      "[NOR] Episode: 22240, Length: 99, Avg Reward: -170.343613633, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.59330558777\n",
      "[NOR] Episode: 22250, Length: 92, Avg Reward: -180.385288245, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 35.6015357971\n",
      "[NOR] Episode: 22260, Length: 98, Avg Reward: -180.705948857, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.45100045204\n",
      "[NOR] Episode: 22270, Length: 53, Avg Reward: -180.945641935, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.63134384155\n",
      "[NOR] Episode: 22280, Length: 79, Avg Reward: -209.538612922, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 296.377807617\n",
      "[NOR] Episode: 22290, Length: 67, Avg Reward: -260.088141348, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.7859382629\n",
      "[NOR] Episode: 22300, Length: 72, Avg Reward: -228.909715314, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 33.3255195618\n",
      "[NOR] Episode: 22310, Length: 53, Avg Reward: -235.1117362, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.2575368881\n",
      "[NOR] Episode: 22320, Length: 64, Avg Reward: -209.138245142, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.1854572296\n",
      "[NOR] Episode: 22330, Length: 95, Avg Reward: -210.010998277, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.0846414566\n",
      "[NOR] Episode: 22340, Length: 60, Avg Reward: -235.217486458, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 112.671646118\n",
      "[NOR] Episode: 22350, Length: 56, Avg Reward: -177.514687425, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -13.4192180634\n",
      "[NOR] Episode: 22360, Length: 56, Avg Reward: -168.782582566, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.579406261444\n",
      "[NOR] Episode: 22370, Length: 77, Avg Reward: -200.295263198, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.9376716614\n",
      "[NOR] Episode: 22380, Length: 80, Avg Reward: -173.984461456, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 26.9253864288\n",
      "[NOR] Episode: 22390, Length: 69, Avg Reward: -176.239898354, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.68843698502\n",
      "[NOR] Episode: 22400, Length: 65, Avg Reward: -152.968888866, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 170.156784058\n",
      "[NOR] Episode: 22410, Length: 104, Avg Reward: -162.84738985, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 188.280563354\n",
      "[NOR] Episode: 22420, Length: 71, Avg Reward: -163.739865451, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -18.1281337738\n",
      "[NOR] Episode: 22430, Length: 90, Avg Reward: -132.467223929, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.11949586868\n",
      "[NOR] Episode: 22440, Length: 68, Avg Reward: -183.9043351, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -17.5645599365\n",
      "[NOR] Episode: 22450, Length: 78, Avg Reward: -165.463227924, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.42485761642\n",
      "[NOR] Episode: 22460, Length: 59, Avg Reward: -228.548332832, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.071064949\n",
      "[NOR] Episode: 22470, Length: 82, Avg Reward: -184.526406837, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 215.375961304\n",
      "[NOR] Episode: 22480, Length: 88, Avg Reward: -246.814044449, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 107.792610168\n",
      "[NOR] Episode: 22490, Length: 76, Avg Reward: -184.327666565, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.24295139313\n",
      "[NOR] Episode: 22500, Length: 85, Avg Reward: -162.345665025, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.459743738174\n",
      "[NOR] Episode: 22510, Length: 87, Avg Reward: -196.106193781, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 27.6418094635\n",
      "[NOR] Episode: 22520, Length: 111, Avg Reward: -230.787515002, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0719614028931\n",
      "[NOR] Episode: 22530, Length: 87, Avg Reward: -160.205006398, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.17123317719\n",
      "[NOR] Episode: 22540, Length: 61, Avg Reward: -217.751493578, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 107.744941711\n",
      "[NOR] Episode: 22550, Length: 77, Avg Reward: -210.610114039, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.4115047455\n",
      "[NOR] Episode: 22560, Length: 86, Avg Reward: -196.562537037, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -13.7589244843\n",
      "[NOR] Episode: 22570, Length: 92, Avg Reward: -241.965064613, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.28001785278\n",
      "[NOR] Episode: 22580, Length: 82, Avg Reward: -244.581903752, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.3199739456\n",
      "[NOR] Episode: 22590, Length: 76, Avg Reward: -181.699992355, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.604711532593\n",
      "[NOR] Episode: 22600, Length: 87, Avg Reward: -217.552032822, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.36266899109\n",
      "[NOR] Episode: 22610, Length: 58, Avg Reward: -167.961825419, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.26358175278\n",
      "[NOR] Episode: 22620, Length: 89, Avg Reward: -204.088792628, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -15.3026742935\n",
      "[NOR] Episode: 22630, Length: 78, Avg Reward: -144.898693103, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 103.728988647\n",
      "[NOR] Episode: 22640, Length: 116, Avg Reward: -135.968809461, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.58001899719\n",
      "[NOR] Episode: 22650, Length: 89, Avg Reward: -189.460600627, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.4989585876\n",
      "[NOR] Episode: 22660, Length: 88, Avg Reward: -168.822065485, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.19723892212\n",
      "[NOR] Episode: 22670, Length: 66, Avg Reward: -143.820244321, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.08196520805\n",
      "[NOR] Episode: 22680, Length: 60, Avg Reward: -150.459054365, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.0260076523\n",
      "[NOR] Episode: 22690, Length: 56, Avg Reward: -163.59404608, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.53019285202\n",
      "[NOR] Episode: 22700, Length: 101, Avg Reward: -217.191766258, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.50625562668\n",
      "[NOR] Episode: 22710, Length: 74, Avg Reward: -195.554958756, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.7119483948\n",
      "[NOR] Episode: 22720, Length: 54, Avg Reward: -246.267230207, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.27871131897\n",
      "[NOR] Episode: 22730, Length: 73, Avg Reward: -235.089045188, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 48.5952262878\n",
      "[NOR] Episode: 22740, Length: 96, Avg Reward: -259.879676873, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.27917671204\n",
      "[NOR] Episode: 22750, Length: 106, Avg Reward: -299.706214193, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.0228266716\n",
      "[NOR] Episode: 22760, Length: 59, Avg Reward: -254.346703019, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.7008781433\n",
      "[NOR] Episode: 22770, Length: 56, Avg Reward: -199.949567238, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 236.541671753\n",
      "[NOR] Episode: 22780, Length: 89, Avg Reward: -209.092524504, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 210.574798584\n",
      "[NOR] Episode: 22790, Length: 95, Avg Reward: -188.592210643, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -13.7311077118\n",
      "[NOR] Episode: 22800, Length: 94, Avg Reward: -262.787341808, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.19586753845\n",
      "[NOR] Episode: 22810, Length: 52, Avg Reward: -272.412130165, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.4325866699\n",
      "[NOR] Episode: 22820, Length: 83, Avg Reward: -267.986276795, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 39.1614379883\n",
      "[NOR] Episode: 22830, Length: 77, Avg Reward: -260.529833548, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 243.509765625\n",
      "[NOR] Episode: 22840, Length: 73, Avg Reward: -271.590904962, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.32143592834\n",
      "[NOR] Episode: 22850, Length: 68, Avg Reward: -235.255746389, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.409110307693\n",
      "[NOR] Episode: 22860, Length: 59, Avg Reward: -232.456975104, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0504784584045\n",
      "[NOR] Episode: 22870, Length: 102, Avg Reward: -256.715371306, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -16.7220516205\n",
      "[NOR] Episode: 22880, Length: 85, Avg Reward: -244.385464168, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 62.7321090698\n",
      "[NOR] Episode: 22890, Length: 78, Avg Reward: -273.06850457, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.45685386658\n",
      "[NOR] Episode: 22900, Length: 85, Avg Reward: -263.658816118, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.19057893753\n",
      "[NOR] Episode: 22910, Length: 83, Avg Reward: -256.385871061, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -17.7043113708\n",
      "[NOR] Episode: 22920, Length: 76, Avg Reward: -232.622240466, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.23113656044\n",
      "[NOR] Episode: 22930, Length: 55, Avg Reward: -244.568371474, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 176.919250488\n",
      "[NOR] Episode: 22940, Length: 59, Avg Reward: -271.558758377, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 107.989852905\n",
      "[NOR] Episode: 22950, Length: 73, Avg Reward: -278.026923065, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 269.901794434\n",
      "[NOR] Episode: 22960, Length: 56, Avg Reward: -307.091434131, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.83576917648\n",
      "[NOR] Episode: 22970, Length: 58, Avg Reward: -275.774001974, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -20.617609024\n",
      "[NOR] Episode: 22980, Length: 55, Avg Reward: -292.866175076, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 235.167495728\n",
      "[NOR] Episode: 22990, Length: 76, Avg Reward: -287.802697265, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.22414982319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 08:54:38,392] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video023000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 23000, Length: 66, Avg Reward: -267.969567046, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.4741535187\n",
      "[NOR] Episode: 23010, Length: 76, Avg Reward: -281.127775172, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.05968761444\n",
      "[NOR] Episode: 23020, Length: 78, Avg Reward: -264.454824083, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -17.2330703735\n",
      "[NOR] Episode: 23030, Length: 94, Avg Reward: -247.280326021, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.09115314484\n",
      "[NOR] Episode: 23040, Length: 56, Avg Reward: -295.735256838, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -40.1912574768\n",
      "[NOR] Episode: 23050, Length: 62, Avg Reward: -256.408563456, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.4739074707\n",
      "[NOR] Episode: 23060, Length: 74, Avg Reward: -232.743391965, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.87731480598\n",
      "[NOR] Episode: 23070, Length: 75, Avg Reward: -272.451416817, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.65976142883\n",
      "[NOR] Episode: 23080, Length: 84, Avg Reward: -293.345212042, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.89160633087\n",
      "[NOR] Episode: 23090, Length: 63, Avg Reward: -279.127002264, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 203.919815063\n",
      "[NOR] Episode: 23100, Length: 87, Avg Reward: -285.109051465, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.612969875336\n",
      "[NOR] Episode: 23110, Length: 95, Avg Reward: -240.179367093, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.17344808578\n",
      "[NOR] Episode: 23120, Length: 75, Avg Reward: -282.301376963, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -12.1128063202\n",
      "[NOR] Episode: 23130, Length: 53, Avg Reward: -266.006636668, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.28850746155\n",
      "[NOR] Episode: 23140, Length: 64, Avg Reward: -287.76778144, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.29727220535\n",
      "[NOR] Episode: 23150, Length: 77, Avg Reward: -275.93049663, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 110.557975769\n",
      "[NOR] Episode: 23160, Length: 68, Avg Reward: -315.738556335, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 290.222381592\n",
      "[NOR] Episode: 23170, Length: 73, Avg Reward: -281.85700186, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 58.6242218018\n",
      "[NOR] Episode: 23180, Length: 84, Avg Reward: -292.251220653, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.30293321609\n",
      "[NOR] Episode: 23190, Length: 61, Avg Reward: -277.223055343, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0885133743286\n",
      "[NOR] Episode: 23200, Length: 82, Avg Reward: -295.10278969, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.3550605774\n",
      "[NOR] Episode: 23210, Length: 77, Avg Reward: -293.228559474, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -17.3648643494\n",
      "[NOR] Episode: 23220, Length: 75, Avg Reward: -290.644240772, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.84701156616\n",
      "[NOR] Episode: 23230, Length: 56, Avg Reward: -279.22297983, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.99354219437\n",
      "[NOR] Episode: 23240, Length: 81, Avg Reward: -241.431535438, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.61786937714\n",
      "[NOR] Episode: 23250, Length: 62, Avg Reward: -227.55085808, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.105877399445\n",
      "[NOR] Episode: 23260, Length: 108, Avg Reward: -212.067916937, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 82.5882720947\n",
      "[NOR] Episode: 23270, Length: 89, Avg Reward: -151.120736775, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.8657712936\n",
      "[NOR] Episode: 23280, Length: 71, Avg Reward: -216.49483681, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.51018738747\n",
      "[NOR] Episode: 23290, Length: 53, Avg Reward: -236.136250112, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0132926702499\n",
      "[NOR] Episode: 23300, Length: 50, Avg Reward: -257.810324084, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.87186813354\n",
      "[NOR] Episode: 23310, Length: 83, Avg Reward: -324.454317291, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 225.132400513\n",
      "[NOR] Episode: 23320, Length: 56, Avg Reward: -296.108800084, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -23.4746074677\n",
      "[NOR] Episode: 23330, Length: 76, Avg Reward: -289.340137136, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 34.6803817749\n",
      "[NOR] Episode: 23340, Length: 58, Avg Reward: -295.301376735, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.63768744469\n",
      "[NOR] Episode: 23350, Length: 131, Avg Reward: -263.072650999, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.66862344742\n",
      "[NOR] Episode: 23360, Length: 56, Avg Reward: -276.121309137, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.32749938965\n",
      "[NOR] Episode: 23370, Length: 72, Avg Reward: -212.339447596, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -12.7830018997\n",
      "[NOR] Episode: 23380, Length: 58, Avg Reward: -273.681084807, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 243.310379028\n",
      "[NOR] Episode: 23390, Length: 90, Avg Reward: -245.025993194, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 245.647277832\n",
      "[NOR] Episode: 23400, Length: 76, Avg Reward: -281.338774647, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 229.833312988\n",
      "[NOR] Episode: 23410, Length: 111, Avg Reward: -233.547702588, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.490292131901\n",
      "[NOR] Episode: 23420, Length: 73, Avg Reward: -172.949098271, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.77701926231\n",
      "[NOR] Episode: 23430, Length: 77, Avg Reward: -201.666729335, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -15.0555438995\n",
      "[NOR] Episode: 23440, Length: 64, Avg Reward: -263.447920192, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 279.261657715\n",
      "[NOR] Episode: 23450, Length: 65, Avg Reward: -253.10004239, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.15856361389\n",
      "[NOR] Episode: 23460, Length: 87, Avg Reward: -233.437203679, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.1174507141\n",
      "[NOR] Episode: 23470, Length: 85, Avg Reward: -266.456423076, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.65277910233\n",
      "[NOR] Episode: 23480, Length: 60, Avg Reward: -240.38078733, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.7389450073\n",
      "[NOR] Episode: 23490, Length: 59, Avg Reward: -188.264409492, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.87006473541\n",
      "[NOR] Episode: 23500, Length: 74, Avg Reward: -141.914271292, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.95018768311\n",
      "[NOR] Episode: 23510, Length: 98, Avg Reward: -190.460386, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.51810741425\n",
      "[NOR] Episode: 23520, Length: 69, Avg Reward: -251.675502211, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.7284297943\n",
      "[NOR] Episode: 23530, Length: 70, Avg Reward: -274.448318596, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 50.2743835449\n",
      "[NOR] Episode: 23540, Length: 85, Avg Reward: -273.262534633, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.15889787674\n",
      "[NOR] Episode: 23550, Length: 94, Avg Reward: -243.252829464, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.01629638672\n",
      "[NOR] Episode: 23560, Length: 72, Avg Reward: -241.200918672, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 279.806488037\n",
      "[NOR] Episode: 23570, Length: 52, Avg Reward: -234.464359064, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.90669488907\n",
      "[NOR] Episode: 23580, Length: 57, Avg Reward: -300.412506026, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.77567863464\n",
      "[NOR] Episode: 23590, Length: 98, Avg Reward: -301.632404431, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.394033432\n",
      "[NOR] Episode: 23600, Length: 62, Avg Reward: -278.152126239, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 47.5637741089\n",
      "[NOR] Episode: 23610, Length: 69, Avg Reward: -247.359237574, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 273.095947266\n",
      "[NOR] Episode: 23620, Length: 72, Avg Reward: -249.709243446, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 43.1602096558\n",
      "[NOR] Episode: 23630, Length: 60, Avg Reward: -262.953345706, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 225.778594971\n",
      "[NOR] Episode: 23640, Length: 74, Avg Reward: -340.162627486, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 80.7618789673\n",
      "[NOR] Episode: 23650, Length: 55, Avg Reward: -289.168374857, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.032954216\n",
      "[NOR] Episode: 23660, Length: 72, Avg Reward: -303.72176931, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 121.080955505\n",
      "[NOR] Episode: 23670, Length: 67, Avg Reward: -302.650253242, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -33.2314987183\n",
      "[NOR] Episode: 23680, Length: 67, Avg Reward: -299.870738529, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.1119728088\n",
      "[NOR] Episode: 23690, Length: 62, Avg Reward: -246.904404564, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 209.551513672\n",
      "[NOR] Episode: 23700, Length: 79, Avg Reward: -280.925375349, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 46.7457618713\n",
      "[NOR] Episode: 23710, Length: 75, Avg Reward: -328.71403693, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 283.220458984\n",
      "[NOR] Episode: 23720, Length: 75, Avg Reward: -286.20873516, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 24.1201210022\n",
      "[NOR] Episode: 23730, Length: 83, Avg Reward: -271.93525183, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 24.339302063\n",
      "[NOR] Episode: 23740, Length: 71, Avg Reward: -300.342442309, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.6682939529\n",
      "[NOR] Episode: 23750, Length: 58, Avg Reward: -276.010757861, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.32258987427\n",
      "[NOR] Episode: 23760, Length: 62, Avg Reward: -324.402265258, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.6061725616\n",
      "[NOR] Episode: 23770, Length: 65, Avg Reward: -284.90346148, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.5870475769\n",
      "[NOR] Episode: 23780, Length: 70, Avg Reward: -217.015186296, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.37774801254\n",
      "[NOR] Episode: 23790, Length: 71, Avg Reward: -300.395133687, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.06184959412\n",
      "[NOR] Episode: 23800, Length: 55, Avg Reward: -259.08672758, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -26.5143661499\n",
      "[NOR] Episode: 23810, Length: 92, Avg Reward: -234.366303258, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0723781585693\n",
      "[NOR] Episode: 23820, Length: 61, Avg Reward: -286.439055552, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.0668592453\n",
      "[NOR] Episode: 23830, Length: 56, Avg Reward: -301.916097576, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.51929855347\n",
      "[NOR] Episode: 23840, Length: 81, Avg Reward: -291.497270511, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -17.6266441345\n",
      "[NOR] Episode: 23850, Length: 92, Avg Reward: -320.488596932, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.96210289\n",
      "[NOR] Episode: 23860, Length: 74, Avg Reward: -270.853995946, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -15.1368560791\n",
      "[NOR] Episode: 23870, Length: 79, Avg Reward: -269.261805433, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.96586894989\n",
      "[NOR] Episode: 23880, Length: 74, Avg Reward: -256.299208427, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.5661888123\n",
      "[NOR] Episode: 23890, Length: 72, Avg Reward: -262.26637458, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 94.0373764038\n",
      "[NOR] Episode: 23900, Length: 72, Avg Reward: -228.444526547, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.50467586517\n",
      "[NOR] Episode: 23910, Length: 57, Avg Reward: -206.186090719, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.972962021828\n",
      "[NOR] Episode: 23920, Length: 66, Avg Reward: -213.32176768, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.039264679\n",
      "[NOR] Episode: 23930, Length: 70, Avg Reward: -194.191554029, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.85520672798\n",
      "[NOR] Episode: 23940, Length: 91, Avg Reward: -210.329083014, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.296702057123\n",
      "[NOR] Episode: 23950, Length: 52, Avg Reward: -245.607911987, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.37173938751\n",
      "[NOR] Episode: 23960, Length: 72, Avg Reward: -212.822552972, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.0882968903\n",
      "[NOR] Episode: 23970, Length: 83, Avg Reward: -179.043016516, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.10505771637\n",
      "[NOR] Episode: 23980, Length: 87, Avg Reward: -218.566605735, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.60650444031\n",
      "[NOR] Episode: 23990, Length: 69, Avg Reward: -228.085200971, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.331228971481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 08:58:57,365] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video024000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 24000, Length: 77, Avg Reward: -251.454301299, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 39.9619903564\n",
      "[NOR] Episode: 24010, Length: 91, Avg Reward: -201.2048424, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.95321023464\n",
      "[NOR] Episode: 24020, Length: 58, Avg Reward: -197.217921385, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 48.3752822876\n",
      "[NOR] Episode: 24030, Length: 64, Avg Reward: -279.616947772, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.63410186768\n",
      "[NOR] Episode: 24040, Length: 57, Avg Reward: -278.670330393, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.00252723694\n",
      "[NOR] Episode: 24050, Length: 86, Avg Reward: -220.611105672, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 53.146686554\n",
      "[NOR] Episode: 24060, Length: 85, Avg Reward: -264.758807317, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.44446611404\n",
      "[NOR] Episode: 24070, Length: 63, Avg Reward: -276.638100399, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 213.343795776\n",
      "[NOR] Episode: 24080, Length: 91, Avg Reward: -221.19875561, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.17035591602\n",
      "[NOR] Episode: 24090, Length: 75, Avg Reward: -248.991084845, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -17.7803535461\n",
      "[NOR] Episode: 24100, Length: 78, Avg Reward: -181.823021968, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 107.003250122\n",
      "[NOR] Episode: 24110, Length: 60, Avg Reward: -172.498851844, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 31.1111125946\n",
      "[NOR] Episode: 24120, Length: 79, Avg Reward: -173.53139779, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 364.847503662\n",
      "[NOR] Episode: 24130, Length: 56, Avg Reward: -190.089641331, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.98090171814\n",
      "[NOR] Episode: 24140, Length: 78, Avg Reward: -219.379489327, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 241.423919678\n",
      "[NOR] Episode: 24150, Length: 57, Avg Reward: -215.307267343, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.4501657486\n",
      "[NOR] Episode: 24160, Length: 53, Avg Reward: -193.474302918, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.6666994095\n",
      "[NOR] Episode: 24170, Length: 94, Avg Reward: -207.088249258, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.30931091309\n",
      "[NOR] Episode: 24180, Length: 66, Avg Reward: -179.463229933, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 103.334503174\n",
      "[NOR] Episode: 24190, Length: 62, Avg Reward: -218.780780052, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 278.948486328\n",
      "[NOR] Episode: 24200, Length: 68, Avg Reward: -158.974990539, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.3943920135\n",
      "[NOR] Episode: 24210, Length: 55, Avg Reward: -213.765553907, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.68468284607\n",
      "[NOR] Episode: 24220, Length: 80, Avg Reward: -202.195766312, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 41.919128418\n",
      "[NOR] Episode: 24230, Length: 85, Avg Reward: -204.655392885, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.11741781235\n",
      "[NOR] Episode: 24240, Length: 65, Avg Reward: -202.532044337, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.48991215229\n",
      "[NOR] Episode: 24250, Length: 57, Avg Reward: -187.319141123, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.0875959396\n",
      "[NOR] Episode: 24260, Length: 68, Avg Reward: -157.296459686, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.55679750443\n",
      "[NOR] Episode: 24270, Length: 55, Avg Reward: -171.003323844, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.93935632706\n",
      "[NOR] Episode: 24280, Length: 87, Avg Reward: -150.821755398, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.37812423706\n",
      "[NOR] Episode: 24290, Length: 64, Avg Reward: -150.419731858, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -15.2328329086\n",
      "[NOR] Episode: 24300, Length: 64, Avg Reward: -181.012521868, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.90220522881\n",
      "[NOR] Episode: 24310, Length: 83, Avg Reward: -258.697998237, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.2743530273\n",
      "[NOR] Episode: 24320, Length: 63, Avg Reward: -172.255684995, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -29.4703445435\n",
      "[NOR] Episode: 24330, Length: 90, Avg Reward: -184.373091352, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.83909261227\n",
      "[NOR] Episode: 24340, Length: 69, Avg Reward: -183.236923878, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 140.292678833\n",
      "[NOR] Episode: 24350, Length: 60, Avg Reward: -177.320043211, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.83565711975\n",
      "[NOR] Episode: 24360, Length: 79, Avg Reward: -188.592513579, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 132.305786133\n",
      "[NOR] Episode: 24370, Length: 66, Avg Reward: -201.458253434, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.87779521942\n",
      "[NOR] Episode: 24380, Length: 93, Avg Reward: -187.720993777, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.68097496033\n",
      "[NOR] Episode: 24390, Length: 68, Avg Reward: -169.644273926, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 41.1251373291\n",
      "[NOR] Episode: 24400, Length: 59, Avg Reward: -190.157014918, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.54203891754\n",
      "[NOR] Episode: 24410, Length: 54, Avg Reward: -189.424001934, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.5871543884\n",
      "[NOR] Episode: 24420, Length: 56, Avg Reward: -164.505224311, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.96347379684\n",
      "[NOR] Episode: 24430, Length: 64, Avg Reward: -172.818671136, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 125.994873047\n",
      "[NOR] Episode: 24440, Length: 71, Avg Reward: -154.399897808, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.17451167107\n",
      "[NOR] Episode: 24450, Length: 74, Avg Reward: -156.438008435, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 205.067245483\n",
      "[NOR] Episode: 24460, Length: 69, Avg Reward: -154.833943585, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.397584676743\n",
      "[NOR] Episode: 24470, Length: 64, Avg Reward: -142.89912578, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 36.7119293213\n",
      "[NOR] Episode: 24480, Length: 87, Avg Reward: -152.956587888, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 105.434005737\n",
      "[NOR] Episode: 24490, Length: 68, Avg Reward: -161.961580835, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.6483440399\n",
      "[NOR] Episode: 24500, Length: 89, Avg Reward: -136.291263297, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.56408119202\n",
      "[NOR] Episode: 24510, Length: 81, Avg Reward: -166.220623711, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.77358651161\n",
      "[NOR] Episode: 24520, Length: 61, Avg Reward: -155.815621863, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 99.9367980957\n",
      "[NOR] Episode: 24530, Length: 88, Avg Reward: -177.369211061, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 145.090164185\n",
      "[NOR] Episode: 24540, Length: 76, Avg Reward: -132.292505147, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 253.524459839\n",
      "[NOR] Episode: 24550, Length: 92, Avg Reward: -142.331461835, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 136.501464844\n",
      "[NOR] Episode: 24560, Length: 78, Avg Reward: -137.347305315, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 51.8070068359\n",
      "[NOR] Episode: 24570, Length: 105, Avg Reward: -142.434184964, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 301.871429443\n",
      "[NOR] Episode: 24580, Length: 64, Avg Reward: -147.417982101, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.21266937256\n",
      "[NOR] Episode: 24590, Length: 64, Avg Reward: -158.741821725, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.58461332321\n",
      "[NOR] Episode: 24600, Length: 79, Avg Reward: -152.570692796, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.677531719208\n",
      "[NOR] Episode: 24610, Length: 76, Avg Reward: -149.218559293, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 302.082244873\n",
      "[NOR] Episode: 24620, Length: 78, Avg Reward: -150.967607683, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 29.9635295868\n",
      "[NOR] Episode: 24630, Length: 78, Avg Reward: -162.264680839, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.65914189816\n",
      "[NOR] Episode: 24640, Length: 90, Avg Reward: -151.044673441, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 184.773605347\n",
      "[NOR] Episode: 24650, Length: 77, Avg Reward: -142.492965078, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.40899467468\n",
      "[NOR] Episode: 24660, Length: 64, Avg Reward: -184.304473395, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 225.299377441\n",
      "[NOR] Episode: 24670, Length: 87, Avg Reward: -153.257237314, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -20.0387878418\n",
      "[NOR] Episode: 24680, Length: 69, Avg Reward: -150.265803962, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 252.921203613\n",
      "[NOR] Episode: 24690, Length: 88, Avg Reward: -155.847305086, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 87.5143051147\n",
      "[NOR] Episode: 24700, Length: 63, Avg Reward: -140.543726125, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 77.3691101074\n",
      "[NOR] Episode: 24710, Length: 74, Avg Reward: -148.900029066, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.88991689682\n",
      "[NOR] Episode: 24720, Length: 66, Avg Reward: -151.59004041, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.84581136703\n",
      "[NOR] Episode: 24730, Length: 82, Avg Reward: -172.04366101, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.907910347\n",
      "[NOR] Episode: 24740, Length: 71, Avg Reward: -167.988117918, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -12.589799881\n",
      "[NOR] Episode: 24750, Length: 88, Avg Reward: -148.816655127, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.28116583824\n",
      "[NOR] Episode: 24760, Length: 69, Avg Reward: -197.391828611, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.73662137985\n",
      "[NOR] Episode: 24770, Length: 85, Avg Reward: -218.968278146, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.5042476654\n",
      "[NOR] Episode: 24780, Length: 98, Avg Reward: -170.912513428, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.00023603439\n",
      "[NOR] Episode: 24790, Length: 85, Avg Reward: -157.786636138, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.948956012726\n",
      "[NOR] Episode: 24800, Length: 65, Avg Reward: -186.372650721, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.15853881836\n",
      "[NOR] Episode: 24810, Length: 63, Avg Reward: -160.869710316, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 32.3496856689\n",
      "[NOR] Episode: 24820, Length: 69, Avg Reward: -161.731686526, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.36575388908\n",
      "[NOR] Episode: 24830, Length: 101, Avg Reward: -182.726042904, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -20.7585029602\n",
      "[NOR] Episode: 24840, Length: 98, Avg Reward: -166.601812544, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.7637519836\n",
      "[NOR] Episode: 24850, Length: 82, Avg Reward: -158.995136964, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.1513671875\n",
      "[NOR] Episode: 24860, Length: 62, Avg Reward: -161.069979535, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.48941993713\n",
      "[NOR] Episode: 24870, Length: 74, Avg Reward: -156.714148514, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 171.401138306\n",
      "[NOR] Episode: 24880, Length: 73, Avg Reward: -156.897459049, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.05147838593\n",
      "[NOR] Episode: 24890, Length: 82, Avg Reward: -157.4699201, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.6870574951\n",
      "[NOR] Episode: 24900, Length: 76, Avg Reward: -148.846453662, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.21102142334\n",
      "[NOR] Episode: 24910, Length: 79, Avg Reward: -131.204026522, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 74.8680953979\n",
      "[NOR] Episode: 24920, Length: 70, Avg Reward: -126.193488464, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 228.512634277\n",
      "[NOR] Episode: 24930, Length: 82, Avg Reward: -169.689315252, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 109.099212646\n",
      "[NOR] Episode: 24940, Length: 74, Avg Reward: -220.68133003, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.51586389542\n",
      "[NOR] Episode: 24950, Length: 61, Avg Reward: -166.350260368, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.9628381729\n",
      "[NOR] Episode: 24960, Length: 87, Avg Reward: -175.9365657, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.49840164185\n",
      "[NOR] Episode: 24970, Length: 62, Avg Reward: -167.879257775, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.77443313599\n",
      "[NOR] Episode: 24980, Length: 60, Avg Reward: -174.103361535, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 29.888841629\n",
      "[NOR] Episode: 24990, Length: 92, Avg Reward: -167.103953072, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.51979732513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 09:03:29,022] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video025000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 25000, Length: 55, Avg Reward: -164.922832028, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.01035165787\n",
      "[NOR] Episode: 25010, Length: 68, Avg Reward: -172.925706388, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.0309638977\n",
      "[NOR] Episode: 25020, Length: 72, Avg Reward: -174.383060997, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.3339576721\n",
      "[NOR] Episode: 25030, Length: 53, Avg Reward: -163.251365443, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 25.9325084686\n",
      "[NOR] Episode: 25040, Length: 59, Avg Reward: -177.388164504, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.66248595715\n",
      "[NOR] Episode: 25050, Length: 68, Avg Reward: -170.995113156, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 381.576416016\n",
      "[NOR] Episode: 25060, Length: 58, Avg Reward: -168.807815929, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.707572937\n",
      "[NOR] Episode: 25070, Length: 63, Avg Reward: -150.475527179, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.95307540894\n",
      "[NOR] Episode: 25080, Length: 71, Avg Reward: -133.051093319, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.3526668549\n",
      "[NOR] Episode: 25090, Length: 84, Avg Reward: -146.922870091, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 28.5576133728\n",
      "[NOR] Episode: 25100, Length: 71, Avg Reward: -154.02088361, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 151.600219727\n",
      "[NOR] Episode: 25110, Length: 66, Avg Reward: -153.710163098, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.94977760315\n",
      "[NOR] Episode: 25120, Length: 70, Avg Reward: -159.716352225, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.34191536903\n",
      "[NOR] Episode: 25130, Length: 77, Avg Reward: -150.400849169, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.52482771873\n",
      "[NOR] Episode: 25140, Length: 52, Avg Reward: -153.852415416, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 169.334320068\n",
      "[NOR] Episode: 25150, Length: 75, Avg Reward: -147.678819569, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.73042011261\n",
      "[NOR] Episode: 25160, Length: 81, Avg Reward: -168.705711124, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 312.486633301\n",
      "[NOR] Episode: 25170, Length: 82, Avg Reward: -155.590224364, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.01298046112\n",
      "[NOR] Episode: 25180, Length: 68, Avg Reward: -158.056468649, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.83015823364\n",
      "[NOR] Episode: 25190, Length: 65, Avg Reward: -162.591980586, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.256567955\n",
      "[NOR] Episode: 25200, Length: 90, Avg Reward: -152.212172852, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 270.674377441\n",
      "[NOR] Episode: 25210, Length: 50, Avg Reward: -159.116038272, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 29.1092338562\n",
      "[NOR] Episode: 25220, Length: 99, Avg Reward: -170.688116347, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 418.079223633\n",
      "[NOR] Episode: 25230, Length: 70, Avg Reward: -147.332840466, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 112.911994934\n",
      "[NOR] Episode: 25240, Length: 60, Avg Reward: -169.716617853, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.92875862122\n",
      "[NOR] Episode: 25250, Length: 93, Avg Reward: -153.464681526, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.214250832796\n",
      "[NOR] Episode: 25260, Length: 98, Avg Reward: -161.024112677, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.64572429657\n",
      "[NOR] Episode: 25270, Length: 80, Avg Reward: -160.473930188, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 199.319610596\n",
      "[NOR] Episode: 25280, Length: 74, Avg Reward: -145.882196074, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.90419602394\n",
      "[NOR] Episode: 25290, Length: 66, Avg Reward: -154.809751902, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.7849779129\n",
      "[NOR] Episode: 25300, Length: 77, Avg Reward: -136.060159757, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.6018047333\n",
      "[NOR] Episode: 25310, Length: 54, Avg Reward: -165.599787398, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 195.656600952\n",
      "[NOR] Episode: 25320, Length: 107, Avg Reward: -141.44327228, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 24.1482963562\n",
      "[NOR] Episode: 25330, Length: 80, Avg Reward: -154.895619786, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.8273973465\n",
      "[NOR] Episode: 25340, Length: 84, Avg Reward: -159.412921183, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.29092979431\n",
      "[NOR] Episode: 25350, Length: 89, Avg Reward: -156.928147127, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.26139688492\n",
      "[NOR] Episode: 25360, Length: 69, Avg Reward: -152.430420597, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 224.625946045\n",
      "[NOR] Episode: 25370, Length: 83, Avg Reward: -154.395889369, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -10.8117790222\n",
      "[NOR] Episode: 25380, Length: 98, Avg Reward: -151.47877096, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.61424922943\n",
      "[NOR] Episode: 25390, Length: 71, Avg Reward: -135.806808168, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 72.3103637695\n",
      "[NOR] Episode: 25400, Length: 85, Avg Reward: -165.813160743, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 50.0255699158\n",
      "[NOR] Episode: 25410, Length: 90, Avg Reward: -162.121746926, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 205.36932373\n",
      "[NOR] Episode: 25420, Length: 74, Avg Reward: -157.354018729, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.5153884888\n",
      "[NOR] Episode: 25430, Length: 91, Avg Reward: -164.239642589, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.363966912031\n",
      "[NOR] Episode: 25440, Length: 85, Avg Reward: -145.19937417, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.4406952858\n",
      "[NOR] Episode: 25450, Length: 66, Avg Reward: -155.483182768, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.146918356419\n",
      "[NOR] Episode: 25460, Length: 66, Avg Reward: -164.918544142, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.29989147186\n",
      "[NOR] Episode: 25470, Length: 84, Avg Reward: -171.441886374, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 114.270202637\n",
      "[NOR] Episode: 25480, Length: 75, Avg Reward: -154.418598186, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.4488401413\n",
      "[NOR] Episode: 25490, Length: 75, Avg Reward: -149.120274338, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.75253605843\n",
      "[NOR] Episode: 25500, Length: 87, Avg Reward: -155.932449021, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.43842029572\n",
      "[NOR] Episode: 25510, Length: 55, Avg Reward: -166.238687048, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 29.5188026428\n",
      "[NOR] Episode: 25520, Length: 77, Avg Reward: -145.917807926, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 178.465484619\n",
      "[NOR] Episode: 25530, Length: 77, Avg Reward: -145.537791661, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.975801229477\n",
      "[NOR] Episode: 25540, Length: 72, Avg Reward: -148.785324091, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.21075534821\n",
      "[NOR] Episode: 25550, Length: 76, Avg Reward: -162.726146656, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.65353286266\n",
      "[NOR] Episode: 25560, Length: 67, Avg Reward: -162.974211245, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.95790982246\n",
      "[NOR] Episode: 25570, Length: 51, Avg Reward: -158.234014611, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.13445663452\n",
      "[NOR] Episode: 25580, Length: 56, Avg Reward: -171.297364889, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.03973436356\n",
      "[NOR] Episode: 25590, Length: 106, Avg Reward: -178.649215752, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.1533203125\n",
      "[NOR] Episode: 25600, Length: 84, Avg Reward: -185.917059964, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.73562812805\n",
      "[NOR] Episode: 25610, Length: 68, Avg Reward: -181.463979176, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.09965324402\n",
      "[NOR] Episode: 25620, Length: 85, Avg Reward: -151.785248824, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 122.295333862\n",
      "[NOR] Episode: 25630, Length: 76, Avg Reward: -144.060788091, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.66671848297\n",
      "[NOR] Episode: 25640, Length: 87, Avg Reward: -135.078427416, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.9588098526\n",
      "[NOR] Episode: 25650, Length: 73, Avg Reward: -153.587896177, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 35.1427650452\n",
      "[NOR] Episode: 25660, Length: 83, Avg Reward: -153.977056189, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 237.986907959\n",
      "[NOR] Episode: 25670, Length: 60, Avg Reward: -147.833083049, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.828166008\n",
      "[NOR] Episode: 25680, Length: 83, Avg Reward: -159.325760504, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 32.0011177063\n",
      "[NOR] Episode: 25690, Length: 66, Avg Reward: -164.9124177, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 362.125976562\n",
      "[NOR] Episode: 25700, Length: 81, Avg Reward: -164.527664188, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.15099143982\n",
      "[NOR] Episode: 25710, Length: 85, Avg Reward: -162.753234182, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 39.2381286621\n",
      "[NOR] Episode: 25720, Length: 61, Avg Reward: -162.543791334, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.87591743469\n",
      "[NOR] Episode: 25730, Length: 65, Avg Reward: -171.668236626, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.03834915161\n",
      "[NOR] Episode: 25740, Length: 62, Avg Reward: -176.256296164, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.5285217762\n",
      "[NOR] Episode: 25750, Length: 71, Avg Reward: -178.811605239, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.77190828323\n",
      "[NOR] Episode: 25760, Length: 54, Avg Reward: -167.773424425, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.0459690094\n",
      "[NOR] Episode: 25770, Length: 66, Avg Reward: -160.689890416, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.6470785141\n",
      "[NOR] Episode: 25780, Length: 58, Avg Reward: -168.869567451, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.84723973274\n",
      "[NOR] Episode: 25790, Length: 70, Avg Reward: -165.745217835, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.01955366135\n",
      "[NOR] Episode: 25800, Length: 94, Avg Reward: -145.153478396, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.14927339554\n",
      "[NOR] Episode: 25810, Length: 68, Avg Reward: -164.604191512, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 24.1571826935\n",
      "[NOR] Episode: 25820, Length: 88, Avg Reward: -158.771665175, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 161.663726807\n",
      "[NOR] Episode: 25830, Length: 62, Avg Reward: -154.138168171, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.2065172195\n",
      "[NOR] Episode: 25840, Length: 67, Avg Reward: -149.702178172, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.66635084152\n",
      "[NOR] Episode: 25850, Length: 78, Avg Reward: -149.998679356, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.58019065857\n",
      "[NOR] Episode: 25860, Length: 63, Avg Reward: -148.122895738, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.88392233849\n",
      "[NOR] Episode: 25870, Length: 56, Avg Reward: -143.700989612, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 284.054077148\n",
      "[NOR] Episode: 25880, Length: 95, Avg Reward: -168.592326291, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.31709194183\n",
      "[NOR] Episode: 25890, Length: 81, Avg Reward: -159.489913762, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 549.23449707\n",
      "[NOR] Episode: 25900, Length: 60, Avg Reward: -156.240683844, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.80799770355\n",
      "[NOR] Episode: 25910, Length: 82, Avg Reward: -168.018257248, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.58681738377\n",
      "[NOR] Episode: 25920, Length: 102, Avg Reward: -157.969685346, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.37989890575\n",
      "[NOR] Episode: 25930, Length: 69, Avg Reward: -154.712311907, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 95.7433624268\n",
      "[NOR] Episode: 25940, Length: 61, Avg Reward: -159.136758274, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.543119430542\n",
      "[NOR] Episode: 25950, Length: 76, Avg Reward: -161.175102691, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.83498859406\n",
      "[NOR] Episode: 25960, Length: 77, Avg Reward: -155.361659108, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 207.292800903\n",
      "[NOR] Episode: 25970, Length: 64, Avg Reward: -159.093098438, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.464952945709\n",
      "[NOR] Episode: 25980, Length: 103, Avg Reward: -139.365128579, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.41691970825\n",
      "[NOR] Episode: 25990, Length: 64, Avg Reward: -154.788741876, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.68447625637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 09:07:58,845] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video026000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 26000, Length: 73, Avg Reward: -166.878456384, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.92252159119\n",
      "[NOR] Episode: 26010, Length: 74, Avg Reward: -171.605379983, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 314.056304932\n",
      "[NOR] Episode: 26020, Length: 95, Avg Reward: -163.247315247, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.833814621\n",
      "[NOR] Episode: 26030, Length: 59, Avg Reward: -156.946941742, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 286.426727295\n",
      "[NOR] Episode: 26040, Length: 57, Avg Reward: -157.905298813, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.720085382462\n",
      "[NOR] Episode: 26050, Length: 99, Avg Reward: -159.724534802, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.6959104538\n",
      "[NOR] Episode: 26060, Length: 128, Avg Reward: -157.048842766, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.39549815655\n",
      "[NOR] Episode: 26070, Length: 94, Avg Reward: -167.755156716, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.59687566757\n",
      "[NOR] Episode: 26080, Length: 96, Avg Reward: -159.393210716, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.56944739819\n",
      "[NOR] Episode: 26090, Length: 57, Avg Reward: -169.582025746, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.571122169495\n",
      "[NOR] Episode: 26100, Length: 90, Avg Reward: -135.321336783, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 130.960296631\n",
      "[NOR] Episode: 26110, Length: 80, Avg Reward: -167.202017082, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 160.782546997\n",
      "[NOR] Episode: 26120, Length: 51, Avg Reward: -155.039446055, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.75337791443\n",
      "[NOR] Episode: 26130, Length: 80, Avg Reward: -161.733858758, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.13774776459\n",
      "[NOR] Episode: 26140, Length: 64, Avg Reward: -139.085433276, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 33.3262710571\n",
      "[NOR] Episode: 26150, Length: 75, Avg Reward: -166.513103319, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.35977828503\n",
      "[NOR] Episode: 26160, Length: 70, Avg Reward: -152.409234767, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.17575740814\n",
      "[NOR] Episode: 26170, Length: 63, Avg Reward: -164.098510677, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.55356502533\n",
      "[NOR] Episode: 26180, Length: 98, Avg Reward: -157.26069282, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.37545967102\n",
      "[NOR] Episode: 26190, Length: 84, Avg Reward: -149.848732488, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.42402923107\n",
      "[NOR] Episode: 26200, Length: 70, Avg Reward: -162.398221556, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.0591106415\n",
      "[NOR] Episode: 26210, Length: 100, Avg Reward: -157.954684482, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.25376772881\n",
      "[NOR] Episode: 26220, Length: 53, Avg Reward: -155.366104966, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.31376266479\n",
      "[NOR] Episode: 26230, Length: 83, Avg Reward: -209.050337511, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.49210548401\n",
      "[NOR] Episode: 26240, Length: 55, Avg Reward: -180.931891164, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 376.617401123\n",
      "[NOR] Episode: 26250, Length: 61, Avg Reward: -203.285526211, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.77242517471\n",
      "[NOR] Episode: 26260, Length: 68, Avg Reward: -158.733347947, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 224.905059814\n",
      "[NOR] Episode: 26270, Length: 64, Avg Reward: -144.354765832, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 120.845779419\n",
      "[NOR] Episode: 26280, Length: 78, Avg Reward: -161.08942109, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 189.573730469\n",
      "[NOR] Episode: 26290, Length: 75, Avg Reward: -143.312498048, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.63626861572\n",
      "[NOR] Episode: 26300, Length: 68, Avg Reward: -171.901312747, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.85920763016\n",
      "[NOR] Episode: 26310, Length: 63, Avg Reward: -160.001216596, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.84340763092\n",
      "[NOR] Episode: 26320, Length: 95, Avg Reward: -173.919891503, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.71290397644\n",
      "[NOR] Episode: 26330, Length: 89, Avg Reward: -163.176955435, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.613114833832\n",
      "[NOR] Episode: 26340, Length: 85, Avg Reward: -172.994184254, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 154.673980713\n",
      "[NOR] Episode: 26350, Length: 78, Avg Reward: -159.958091697, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.1593265533\n",
      "[NOR] Episode: 26360, Length: 91, Avg Reward: -189.445372708, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.05737781525\n",
      "[NOR] Episode: 26370, Length: 64, Avg Reward: -162.100534166, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.11870765686\n",
      "[NOR] Episode: 26380, Length: 75, Avg Reward: -178.549245559, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.24030208588\n",
      "[NOR] Episode: 26390, Length: 78, Avg Reward: -166.742443868, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.15724945068\n",
      "[NOR] Episode: 26400, Length: 80, Avg Reward: -146.715236605, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 249.736724854\n",
      "[NOR] Episode: 26410, Length: 71, Avg Reward: -158.297191936, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.6654343605\n",
      "[NOR] Episode: 26420, Length: 100, Avg Reward: -136.953059784, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 303.527160645\n",
      "[NOR] Episode: 26430, Length: 65, Avg Reward: -169.911258604, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 58.6711273193\n",
      "[NOR] Episode: 26440, Length: 90, Avg Reward: -150.549685419, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.13460493088\n",
      "[NOR] Episode: 26450, Length: 61, Avg Reward: -156.112098159, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.0497436523\n",
      "[NOR] Episode: 26460, Length: 78, Avg Reward: -153.534399853, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.14557266235\n",
      "[NOR] Episode: 26470, Length: 54, Avg Reward: -157.163301071, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 43.5260238647\n",
      "[NOR] Episode: 26480, Length: 88, Avg Reward: -160.475635708, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.17623138428\n",
      "[NOR] Episode: 26490, Length: 56, Avg Reward: -151.926548871, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 27.0366306305\n",
      "[NOR] Episode: 26500, Length: 90, Avg Reward: -160.890617277, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.0630865097\n",
      "[NOR] Episode: 26510, Length: 62, Avg Reward: -173.658938213, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.57062673569\n",
      "[NOR] Episode: 26520, Length: 72, Avg Reward: -148.130596717, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.13284921646\n",
      "[NOR] Episode: 26530, Length: 70, Avg Reward: -144.451280517, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.54825472832\n",
      "[NOR] Episode: 26540, Length: 93, Avg Reward: -171.643503149, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 171.483551025\n",
      "[NOR] Episode: 26550, Length: 67, Avg Reward: -152.839470109, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.679303646088\n",
      "[NOR] Episode: 26560, Length: 80, Avg Reward: -148.596534554, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.855709075928\n",
      "[NOR] Episode: 26570, Length: 91, Avg Reward: -151.27985246, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 72.8326034546\n",
      "[NOR] Episode: 26580, Length: 70, Avg Reward: -157.057842306, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 25.1992835999\n",
      "[NOR] Episode: 26590, Length: 110, Avg Reward: -165.128107882, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.16199541092\n",
      "[NOR] Episode: 26600, Length: 71, Avg Reward: -160.943041579, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.999546051\n",
      "[NOR] Episode: 26610, Length: 97, Avg Reward: -150.331145936, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 77.2343444824\n",
      "[NOR] Episode: 26620, Length: 84, Avg Reward: -150.897369658, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.872822523117\n",
      "[NOR] Episode: 26630, Length: 94, Avg Reward: -162.41881886, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.7185292244\n",
      "[NOR] Episode: 26640, Length: 77, Avg Reward: -150.13875119, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.69646310806\n",
      "[NOR] Episode: 26650, Length: 56, Avg Reward: -141.058771978, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 114.628395081\n",
      "[NOR] Episode: 26660, Length: 76, Avg Reward: -149.584783431, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 485.679748535\n",
      "[NOR] Episode: 26670, Length: 66, Avg Reward: -160.993941043, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.56743049622\n",
      "[NOR] Episode: 26680, Length: 52, Avg Reward: -145.902813042, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 235.206619263\n",
      "[NOR] Episode: 26690, Length: 69, Avg Reward: -164.539515069, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.59857845306\n",
      "[NOR] Episode: 26700, Length: 84, Avg Reward: -167.84210843, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 174.536331177\n",
      "[NOR] Episode: 26710, Length: 55, Avg Reward: -144.524513283, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.65900087357\n",
      "[NOR] Episode: 26720, Length: 78, Avg Reward: -148.163596941, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.2023830414\n",
      "[NOR] Episode: 26730, Length: 63, Avg Reward: -156.839746139, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 62.9426193237\n",
      "[NOR] Episode: 26740, Length: 75, Avg Reward: -155.200518656, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.6425533295\n",
      "[NOR] Episode: 26750, Length: 97, Avg Reward: -144.38257501, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.4409558773\n",
      "[NOR] Episode: 26760, Length: 69, Avg Reward: -168.750909915, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 86.8024215698\n",
      "[NOR] Episode: 26770, Length: 57, Avg Reward: -149.22825816, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.31554818153\n",
      "[NOR] Episode: 26780, Length: 69, Avg Reward: -150.853417191, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.07154178619\n",
      "[NOR] Episode: 26790, Length: 61, Avg Reward: -147.665647681, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.08118534088\n",
      "[NOR] Episode: 26800, Length: 88, Avg Reward: -152.83708321, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.759021759\n",
      "[NOR] Episode: 26810, Length: 76, Avg Reward: -148.919680492, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.3565311432\n",
      "[NOR] Episode: 26820, Length: 92, Avg Reward: -138.262380585, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.116804838181\n",
      "[NOR] Episode: 26830, Length: 79, Avg Reward: -142.231164851, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 30.2559089661\n",
      "[NOR] Episode: 26840, Length: 70, Avg Reward: -149.941726853, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 347.199737549\n",
      "[NOR] Episode: 26850, Length: 61, Avg Reward: -158.108065574, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 326.355621338\n",
      "[NOR] Episode: 26860, Length: 90, Avg Reward: -155.050156705, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.48608922958\n",
      "[NOR] Episode: 26870, Length: 75, Avg Reward: -156.570708708, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.20635890961\n",
      "[NOR] Episode: 26880, Length: 87, Avg Reward: -157.125104859, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 103.03188324\n",
      "[NOR] Episode: 26890, Length: 69, Avg Reward: -151.912459651, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 109.959747314\n",
      "[NOR] Episode: 26900, Length: 79, Avg Reward: -142.90352308, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.90325784683\n",
      "[NOR] Episode: 26910, Length: 92, Avg Reward: -148.215808033, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0534870624542\n",
      "[NOR] Episode: 26920, Length: 65, Avg Reward: -153.346508015, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 28.1385879517\n",
      "[NOR] Episode: 26930, Length: 71, Avg Reward: -154.312032981, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 340.028625488\n",
      "[NOR] Episode: 26940, Length: 72, Avg Reward: -162.074805387, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.7489833832\n",
      "[NOR] Episode: 26950, Length: 95, Avg Reward: -153.271212969, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.27129983902\n",
      "[NOR] Episode: 26960, Length: 54, Avg Reward: -153.380529273, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 93.0916442871\n",
      "[NOR] Episode: 26970, Length: 67, Avg Reward: -160.158924836, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.87585282326\n",
      "[NOR] Episode: 26980, Length: 63, Avg Reward: -148.600617342, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.986143887043\n",
      "[NOR] Episode: 26990, Length: 88, Avg Reward: -143.938044922, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.487763881683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 09:12:30,091] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video027000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 27000, Length: 91, Avg Reward: -144.176330923, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 57.833732605\n",
      "[NOR] Episode: 27010, Length: 89, Avg Reward: -131.176471397, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.530030012131\n",
      "[NOR] Episode: 27020, Length: 63, Avg Reward: -158.267646514, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.55554389954\n",
      "[NOR] Episode: 27030, Length: 95, Avg Reward: -141.094341866, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.1758861542\n",
      "[NOR] Episode: 27040, Length: 74, Avg Reward: -156.101382089, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 181.891448975\n",
      "[NOR] Episode: 27050, Length: 57, Avg Reward: -151.045179674, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 319.23449707\n",
      "[NOR] Episode: 27060, Length: 75, Avg Reward: -151.286927903, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 263.116699219\n",
      "[NOR] Episode: 27070, Length: 59, Avg Reward: -134.452537335, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 214.472747803\n",
      "[NOR] Episode: 27080, Length: 61, Avg Reward: -132.629351894, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.44035553932\n",
      "[NOR] Episode: 27090, Length: 65, Avg Reward: -152.683945199, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.5794124603\n",
      "[NOR] Episode: 27100, Length: 106, Avg Reward: -162.035022906, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.931011199951\n",
      "[NOR] Episode: 27110, Length: 91, Avg Reward: -152.730370126, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.774478912354\n",
      "[NOR] Episode: 27120, Length: 60, Avg Reward: -165.012227176, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.84844946861\n",
      "[NOR] Episode: 27130, Length: 89, Avg Reward: -153.043123264, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.87719345093\n",
      "[NOR] Episode: 27140, Length: 82, Avg Reward: -149.162935688, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 202.256439209\n",
      "[NOR] Episode: 27150, Length: 74, Avg Reward: -144.020918553, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.6046714783\n",
      "[NOR] Episode: 27160, Length: 66, Avg Reward: -140.988047836, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 286.101654053\n",
      "[NOR] Episode: 27170, Length: 67, Avg Reward: -136.006360148, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.209815979\n",
      "[NOR] Episode: 27180, Length: 64, Avg Reward: -142.52839559, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.91682338715\n",
      "[NOR] Episode: 27190, Length: 86, Avg Reward: -145.373202578, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 64.2490692139\n",
      "[NOR] Episode: 27200, Length: 67, Avg Reward: -140.208650131, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.90247631073\n",
      "[NOR] Episode: 27210, Length: 96, Avg Reward: -146.903358655, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 250.973693848\n",
      "[NOR] Episode: 27220, Length: 55, Avg Reward: -139.945566007, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.99540328979\n",
      "[NOR] Episode: 27230, Length: 63, Avg Reward: -135.344804424, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 251.261505127\n",
      "[NOR] Episode: 27240, Length: 82, Avg Reward: -140.867027748, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.52012062073\n",
      "[NOR] Episode: 27250, Length: 57, Avg Reward: -161.309511609, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 236.318527222\n",
      "[NOR] Episode: 27260, Length: 98, Avg Reward: -149.151242325, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.236076355\n",
      "[NOR] Episode: 27270, Length: 94, Avg Reward: -150.517197128, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.55299091339\n",
      "[NOR] Episode: 27280, Length: 75, Avg Reward: -155.758105416, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.18782281876\n",
      "[NOR] Episode: 27290, Length: 87, Avg Reward: -152.153164474, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.14088869095\n",
      "[NOR] Episode: 27300, Length: 77, Avg Reward: -159.776288706, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 24.9134464264\n",
      "[NOR] Episode: 27310, Length: 85, Avg Reward: -146.230480985, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.463493704796\n",
      "[NOR] Episode: 27320, Length: 80, Avg Reward: -141.804002585, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.3330383301\n",
      "[NOR] Episode: 27330, Length: 70, Avg Reward: -146.481160752, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.73393821716\n",
      "[NOR] Episode: 27340, Length: 73, Avg Reward: -138.584574543, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 36.7212982178\n",
      "[NOR] Episode: 27350, Length: 71, Avg Reward: -144.088817086, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 198.150558472\n",
      "[NOR] Episode: 27360, Length: 70, Avg Reward: -145.038363739, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.65187358856\n",
      "[NOR] Episode: 27370, Length: 74, Avg Reward: -144.889285343, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.5023574829\n",
      "[NOR] Episode: 27380, Length: 65, Avg Reward: -138.332193474, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.51172828674\n",
      "[NOR] Episode: 27390, Length: 64, Avg Reward: -142.227367837, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.7741737366\n",
      "[NOR] Episode: 27400, Length: 80, Avg Reward: -152.479557483, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 441.637145996\n",
      "[NOR] Episode: 27410, Length: 96, Avg Reward: -147.385519213, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.353241801262\n",
      "[NOR] Episode: 27420, Length: 95, Avg Reward: -144.87840425, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.678974032402\n",
      "[NOR] Episode: 27430, Length: 68, Avg Reward: -155.697889503, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.18141698837\n",
      "[NOR] Episode: 27440, Length: 82, Avg Reward: -150.674411051, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.529959201813\n",
      "[NOR] Episode: 27450, Length: 62, Avg Reward: -149.067915109, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 104.402511597\n",
      "[NOR] Episode: 27460, Length: 71, Avg Reward: -142.170178763, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.15817499161\n",
      "[NOR] Episode: 27470, Length: 62, Avg Reward: -148.619628342, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.005859375\n",
      "[NOR] Episode: 27480, Length: 67, Avg Reward: -151.35294269, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.18208026886\n",
      "[NOR] Episode: 27490, Length: 61, Avg Reward: -155.259288478, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 94.7340698242\n",
      "[NOR] Episode: 27500, Length: 85, Avg Reward: -154.257592395, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 207.153167725\n",
      "[NOR] Episode: 27510, Length: 73, Avg Reward: -144.968403014, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 156.549819946\n",
      "[NOR] Episode: 27520, Length: 56, Avg Reward: -138.227341227, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.80725002289\n",
      "[NOR] Episode: 27530, Length: 76, Avg Reward: -152.477261016, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 152.469085693\n",
      "[NOR] Episode: 27540, Length: 81, Avg Reward: -143.950298569, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 253.385299683\n",
      "[NOR] Episode: 27550, Length: 62, Avg Reward: -143.940287993, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 233.766479492\n",
      "[NOR] Episode: 27560, Length: 64, Avg Reward: -145.152740646, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 31.1826190948\n",
      "[NOR] Episode: 27570, Length: 70, Avg Reward: -140.306462402, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 79.2615814209\n",
      "[NOR] Episode: 27580, Length: 59, Avg Reward: -139.240461698, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 302.424957275\n",
      "[NOR] Episode: 27590, Length: 81, Avg Reward: -134.962199264, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -11.3566026688\n",
      "[NOR] Episode: 27600, Length: 72, Avg Reward: -141.084926514, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.50141763687\n",
      "[NOR] Episode: 27610, Length: 63, Avg Reward: -145.023935466, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 93.3000335693\n",
      "[NOR] Episode: 27620, Length: 87, Avg Reward: -145.868240352, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.92674601078\n",
      "[NOR] Episode: 27630, Length: 74, Avg Reward: -148.282543375, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.3508071899\n",
      "[NOR] Episode: 27640, Length: 81, Avg Reward: -144.07690239, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.86264419556\n",
      "[NOR] Episode: 27650, Length: 66, Avg Reward: -136.110712603, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 123.83782196\n",
      "[NOR] Episode: 27660, Length: 72, Avg Reward: -150.455594667, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 223.884124756\n",
      "[NOR] Episode: 27670, Length: 86, Avg Reward: -159.930263485, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 119.358108521\n",
      "[NOR] Episode: 27680, Length: 77, Avg Reward: -148.394032457, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.99434375763\n",
      "[NOR] Episode: 27690, Length: 60, Avg Reward: -138.107882933, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.50249528885\n",
      "[NOR] Episode: 27700, Length: 75, Avg Reward: -145.214949063, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 69.7886428833\n",
      "[NOR] Episode: 27710, Length: 55, Avg Reward: -152.272961003, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.26955699921\n",
      "[NOR] Episode: 27720, Length: 60, Avg Reward: -144.845838645, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 88.313659668\n",
      "[NOR] Episode: 27730, Length: 87, Avg Reward: -164.273311445, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 252.553283691\n",
      "[NOR] Episode: 27740, Length: 74, Avg Reward: -156.509672822, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.503950119\n",
      "[NOR] Episode: 27750, Length: 74, Avg Reward: -150.864566476, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 206.691192627\n",
      "[NOR] Episode: 27760, Length: 77, Avg Reward: -153.836930106, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.87836277485\n",
      "[NOR] Episode: 27770, Length: 67, Avg Reward: -155.939114382, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 98.9855270386\n",
      "[NOR] Episode: 27780, Length: 57, Avg Reward: -152.135195079, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 168.02142334\n",
      "[NOR] Episode: 27790, Length: 87, Avg Reward: -156.662425225, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -14.467798233\n",
      "[NOR] Episode: 27800, Length: 89, Avg Reward: -170.572500928, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.689239263535\n",
      "[NOR] Episode: 27810, Length: 68, Avg Reward: -150.12184431, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.28683137894\n",
      "[NOR] Episode: 27820, Length: 77, Avg Reward: -143.069240445, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.27548265457\n",
      "[NOR] Episode: 27830, Length: 107, Avg Reward: -157.38554054, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.9741802216\n",
      "[NOR] Episode: 27840, Length: 74, Avg Reward: -149.074838507, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 114.425994873\n",
      "[NOR] Episode: 27850, Length: 84, Avg Reward: -146.262471963, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.91543769836\n",
      "[NOR] Episode: 27860, Length: 81, Avg Reward: -146.032116763, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 33.3557701111\n",
      "[NOR] Episode: 27870, Length: 65, Avg Reward: -143.20505887, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 266.878295898\n",
      "[NOR] Episode: 27880, Length: 82, Avg Reward: -160.64066848, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.505993962288\n",
      "[NOR] Episode: 27890, Length: 65, Avg Reward: -153.638391402, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 87.8828353882\n",
      "[NOR] Episode: 27900, Length: 72, Avg Reward: -146.078758642, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.224772810936\n",
      "[NOR] Episode: 27910, Length: 99, Avg Reward: -150.921676479, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.90500736237\n",
      "[NOR] Episode: 27920, Length: 70, Avg Reward: -145.716249263, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 29.1430587769\n",
      "[NOR] Episode: 27930, Length: 63, Avg Reward: -156.977734656, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 51.8043136597\n",
      "[NOR] Episode: 27940, Length: 61, Avg Reward: -148.124915435, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.6608748436\n",
      "[NOR] Episode: 27950, Length: 84, Avg Reward: -153.298544933, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.69832229614\n",
      "[NOR] Episode: 27960, Length: 98, Avg Reward: -150.688947755, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 316.66796875\n",
      "[NOR] Episode: 27970, Length: 88, Avg Reward: -157.508811741, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.28517818451\n",
      "[NOR] Episode: 27980, Length: 95, Avg Reward: -142.265543317, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 208.200454712\n",
      "[NOR] Episode: 27990, Length: 72, Avg Reward: -148.486389501, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.95205259323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 09:17:10,537] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video028000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 28000, Length: 72, Avg Reward: -156.445692118, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.32931315899\n",
      "[NOR] Episode: 28010, Length: 71, Avg Reward: -158.925769564, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 81.9434280396\n",
      "[NOR] Episode: 28020, Length: 87, Avg Reward: -149.697796542, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 71.8801727295\n",
      "[NOR] Episode: 28030, Length: 85, Avg Reward: -147.50077033, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.103390932083\n",
      "[NOR] Episode: 28040, Length: 91, Avg Reward: -144.33356712, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.8439399004\n",
      "[NOR] Episode: 28050, Length: 98, Avg Reward: -149.807322758, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.36744356155\n",
      "[NOR] Episode: 28060, Length: 85, Avg Reward: -138.773067055, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.85751628876\n",
      "[NOR] Episode: 28070, Length: 68, Avg Reward: -140.238057063, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.94532251358\n",
      "[NOR] Episode: 28080, Length: 77, Avg Reward: -147.600679655, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.84461688995\n",
      "[NOR] Episode: 28090, Length: 64, Avg Reward: -145.491491398, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.304008543491\n",
      "[NOR] Episode: 28100, Length: 69, Avg Reward: -134.828677308, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 307.929260254\n",
      "[NOR] Episode: 28110, Length: 73, Avg Reward: -144.114753291, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.0822601318\n",
      "[NOR] Episode: 28120, Length: 95, Avg Reward: -154.951523362, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.1159992218\n",
      "[NOR] Episode: 28130, Length: 71, Avg Reward: -143.800752303, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 269.929992676\n",
      "[NOR] Episode: 28140, Length: 86, Avg Reward: -154.161110582, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.39693069458\n",
      "[NOR] Episode: 28150, Length: 66, Avg Reward: -163.679453457, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.02638339996\n",
      "[NOR] Episode: 28160, Length: 80, Avg Reward: -150.626854194, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.34559011459\n",
      "[NOR] Episode: 28170, Length: 84, Avg Reward: -147.291848145, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.719155311584\n",
      "[NOR] Episode: 28180, Length: 72, Avg Reward: -151.283628883, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.61780083179\n",
      "[NOR] Episode: 28190, Length: 98, Avg Reward: -136.86865237, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 98.2190551758\n",
      "[NOR] Episode: 28200, Length: 89, Avg Reward: -149.900889327, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.52358818054\n",
      "[NOR] Episode: 28210, Length: 88, Avg Reward: -145.574949905, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.92071259022\n",
      "[NOR] Episode: 28220, Length: 90, Avg Reward: -142.109134052, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.5424580574\n",
      "[NOR] Episode: 28230, Length: 97, Avg Reward: -146.342001538, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 26.8250293732\n",
      "[NOR] Episode: 28240, Length: 68, Avg Reward: -153.714599306, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 319.937438965\n",
      "[NOR] Episode: 28250, Length: 92, Avg Reward: -163.399931835, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 599.024047852\n",
      "[NOR] Episode: 28260, Length: 102, Avg Reward: -169.45171097, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.03778672218\n",
      "[NOR] Episode: 28270, Length: 72, Avg Reward: -153.466584801, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.4963550568\n",
      "[NOR] Episode: 28280, Length: 90, Avg Reward: -159.535698862, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.93126297\n",
      "[NOR] Episode: 28290, Length: 57, Avg Reward: -146.143887916, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 61.772277832\n",
      "[NOR] Episode: 28300, Length: 85, Avg Reward: -146.925595329, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.0685329437\n",
      "[NOR] Episode: 28310, Length: 59, Avg Reward: -146.967996226, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.4102296829\n",
      "[NOR] Episode: 28320, Length: 75, Avg Reward: -143.735233841, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 616.256958008\n",
      "[NOR] Episode: 28330, Length: 80, Avg Reward: -165.714744497, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.592983484268\n",
      "[NOR] Episode: 28340, Length: 84, Avg Reward: -163.827769972, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.62398123741\n",
      "[NOR] Episode: 28350, Length: 92, Avg Reward: -175.770303199, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 159.907684326\n",
      "[NOR] Episode: 28360, Length: 67, Avg Reward: -155.599412221, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 128.162902832\n",
      "[NOR] Episode: 28370, Length: 89, Avg Reward: -155.799808964, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.858827531338\n",
      "[NOR] Episode: 28380, Length: 90, Avg Reward: -150.932451723, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.62933635712\n",
      "[NOR] Episode: 28390, Length: 73, Avg Reward: -158.832498394, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 58.1401062012\n",
      "[NOR] Episode: 28400, Length: 87, Avg Reward: -156.27185, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 89.1000900269\n",
      "[NOR] Episode: 28410, Length: 100, Avg Reward: -145.725572257, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 65.2124557495\n",
      "[NOR] Episode: 28420, Length: 68, Avg Reward: -151.092293338, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 33.9166679382\n",
      "[NOR] Episode: 28430, Length: 68, Avg Reward: -150.659800278, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 33.5029830933\n",
      "[NOR] Episode: 28440, Length: 65, Avg Reward: -156.466364802, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.0980548859\n",
      "[NOR] Episode: 28450, Length: 88, Avg Reward: -159.040995602, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.64046573639\n",
      "[NOR] Episode: 28460, Length: 96, Avg Reward: -153.595626637, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 31.9195785522\n",
      "[NOR] Episode: 28470, Length: 57, Avg Reward: -152.734991283, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.15210258961\n",
      "[NOR] Episode: 28480, Length: 73, Avg Reward: -151.551154875, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 92.1092987061\n",
      "[NOR] Episode: 28490, Length: 64, Avg Reward: -154.274278234, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.86457920074\n",
      "[NOR] Episode: 28500, Length: 71, Avg Reward: -147.765944549, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.56739330292\n",
      "[NOR] Episode: 28510, Length: 76, Avg Reward: -165.734158651, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 136.708267212\n",
      "[NOR] Episode: 28520, Length: 90, Avg Reward: -182.433224862, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.08270931244\n",
      "[NOR] Episode: 28530, Length: 83, Avg Reward: -167.48041776, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.31074428558\n",
      "[NOR] Episode: 28540, Length: 83, Avg Reward: -175.570475853, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.302786588669\n",
      "[NOR] Episode: 28550, Length: 79, Avg Reward: -162.57395818, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 56.3873023987\n",
      "[NOR] Episode: 28560, Length: 102, Avg Reward: -152.839247272, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 38.963809967\n",
      "[NOR] Episode: 28570, Length: 87, Avg Reward: -185.688927234, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 165.734207153\n",
      "[NOR] Episode: 28580, Length: 61, Avg Reward: -142.465804144, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.91190004349\n",
      "[NOR] Episode: 28590, Length: 96, Avg Reward: -157.75301294, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.12259340286\n",
      "[NOR] Episode: 28600, Length: 90, Avg Reward: -162.174592662, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.84077739716\n",
      "[NOR] Episode: 28610, Length: 94, Avg Reward: -159.052422506, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 95.7380371094\n",
      "[NOR] Episode: 28620, Length: 65, Avg Reward: -159.790839856, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.20421743393\n",
      "[NOR] Episode: 28630, Length: 58, Avg Reward: -179.27258986, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 133.660690308\n",
      "[NOR] Episode: 28640, Length: 74, Avg Reward: -167.207718706, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.94783639908\n",
      "[NOR] Episode: 28650, Length: 77, Avg Reward: -156.28534923, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 94.0054779053\n",
      "[NOR] Episode: 28660, Length: 57, Avg Reward: -150.111950281, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.2383670807\n",
      "[NOR] Episode: 28670, Length: 69, Avg Reward: -174.868865354, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.06273698807\n",
      "[NOR] Episode: 28680, Length: 86, Avg Reward: -168.832003908, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.9153213501\n",
      "[NOR] Episode: 28690, Length: 101, Avg Reward: -170.615675287, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.90588140488\n",
      "[NOR] Episode: 28700, Length: 76, Avg Reward: -151.224235474, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.83634233475\n",
      "[NOR] Episode: 28710, Length: 66, Avg Reward: -187.901622679, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.25884866714\n",
      "[NOR] Episode: 28720, Length: 102, Avg Reward: -174.253327233, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 111.776702881\n",
      "[NOR] Episode: 28730, Length: 59, Avg Reward: -169.953357489, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.35200881958\n",
      "[NOR] Episode: 28740, Length: 86, Avg Reward: -162.60750003, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.41482162476\n",
      "[NOR] Episode: 28750, Length: 66, Avg Reward: -186.043629162, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 167.155303955\n",
      "[NOR] Episode: 28760, Length: 58, Avg Reward: -193.934818752, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.609685659409\n",
      "[NOR] Episode: 28770, Length: 58, Avg Reward: -190.161011006, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 227.346893311\n",
      "[NOR] Episode: 28780, Length: 57, Avg Reward: -171.724752291, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 71.7910232544\n",
      "[NOR] Episode: 28790, Length: 93, Avg Reward: -179.221882065, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 311.297607422\n",
      "[NOR] Episode: 28800, Length: 99, Avg Reward: -179.937287251, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.50993013382\n",
      "[NOR] Episode: 28810, Length: 77, Avg Reward: -180.273105666, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 54.4449005127\n",
      "[NOR] Episode: 28820, Length: 60, Avg Reward: -194.350737702, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 182.358612061\n",
      "[NOR] Episode: 28830, Length: 70, Avg Reward: -196.272547176, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.7146730423\n",
      "[NOR] Episode: 28840, Length: 66, Avg Reward: -188.480655133, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.33291006088\n",
      "[NOR] Episode: 28850, Length: 98, Avg Reward: -188.340510944, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.533750712872\n",
      "[NOR] Episode: 28860, Length: 68, Avg Reward: -195.600928036, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.81016111374\n",
      "[NOR] Episode: 28870, Length: 88, Avg Reward: -183.380332467, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.81318211555\n",
      "[NOR] Episode: 28880, Length: 61, Avg Reward: -187.770329885, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.296585083\n",
      "[NOR] Episode: 28890, Length: 87, Avg Reward: -195.596868192, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.63172531128\n",
      "[NOR] Episode: 28900, Length: 55, Avg Reward: -177.168336059, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 108.016677856\n",
      "[NOR] Episode: 28910, Length: 111, Avg Reward: -182.764091217, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.19828796387\n",
      "[NOR] Episode: 28920, Length: 62, Avg Reward: -194.103725738, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.198016762733\n",
      "[NOR] Episode: 28930, Length: 81, Avg Reward: -187.10275931, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 221.681762695\n",
      "[NOR] Episode: 28940, Length: 108, Avg Reward: -194.622790785, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.285587191582\n",
      "[NOR] Episode: 28950, Length: 60, Avg Reward: -146.385262937, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.74064826965\n",
      "[NOR] Episode: 28960, Length: 64, Avg Reward: -197.245525434, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.66649723053\n",
      "[NOR] Episode: 28970, Length: 58, Avg Reward: -185.919672904, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.40640926361\n",
      "[NOR] Episode: 28980, Length: 100, Avg Reward: -188.512280071, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.16757059097\n",
      "[NOR] Episode: 28990, Length: 97, Avg Reward: -147.359641757, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 204.673065186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 09:21:55,563] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video029000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 29000, Length: 81, Avg Reward: -170.017973854, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.06403923035\n",
      "[NOR] Episode: 29010, Length: 63, Avg Reward: -199.643544572, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.22237920761\n",
      "[NOR] Episode: 29020, Length: 83, Avg Reward: -196.796398769, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 61.6628952026\n",
      "[NOR] Episode: 29030, Length: 97, Avg Reward: -216.246254023, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.22938632965\n",
      "[NOR] Episode: 29040, Length: 91, Avg Reward: -197.414400903, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.6381263733\n",
      "[NOR] Episode: 29050, Length: 75, Avg Reward: -198.174876583, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.6428804398\n",
      "[NOR] Episode: 29060, Length: 86, Avg Reward: -174.686879328, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.8603477478\n",
      "[NOR] Episode: 29070, Length: 100, Avg Reward: -183.778235111, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.09546709061\n",
      "[NOR] Episode: 29080, Length: 80, Avg Reward: -186.893196934, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.650618910789\n",
      "[NOR] Episode: 29090, Length: 102, Avg Reward: -171.67014089, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 262.727783203\n",
      "[NOR] Episode: 29100, Length: 59, Avg Reward: -173.985969957, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.9927101135\n",
      "[NOR] Episode: 29110, Length: 88, Avg Reward: -193.342849174, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 24.2375144958\n",
      "[NOR] Episode: 29120, Length: 90, Avg Reward: -182.758057201, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.91910386086\n",
      "[NOR] Episode: 29130, Length: 80, Avg Reward: -184.496740055, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 297.00177002\n",
      "[NOR] Episode: 29140, Length: 85, Avg Reward: -196.195298085, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.39043736458\n",
      "[NOR] Episode: 29150, Length: 93, Avg Reward: -175.201827451, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.22863006592\n",
      "[NOR] Episode: 29160, Length: 85, Avg Reward: -186.110571676, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 36.9353866577\n",
      "[NOR] Episode: 29170, Length: 82, Avg Reward: -168.73852882, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 275.793579102\n",
      "[NOR] Episode: 29180, Length: 80, Avg Reward: -184.940355047, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 28.3372821808\n",
      "[NOR] Episode: 29190, Length: 55, Avg Reward: -161.285589062, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.14245986938\n",
      "[NOR] Episode: 29200, Length: 70, Avg Reward: -198.858909815, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 27.2311134338\n",
      "[NOR] Episode: 29210, Length: 56, Avg Reward: -190.122439807, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 239.352798462\n",
      "[NOR] Episode: 29220, Length: 104, Avg Reward: -182.03080051, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.28136277199\n",
      "[NOR] Episode: 29230, Length: 67, Avg Reward: -185.40160982, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 149.391906738\n",
      "[NOR] Episode: 29240, Length: 91, Avg Reward: -170.432499558, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.32864522934\n",
      "[NOR] Episode: 29250, Length: 82, Avg Reward: -180.743308814, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 92.8754425049\n",
      "[NOR] Episode: 29260, Length: 67, Avg Reward: -209.808222234, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.09399056435\n",
      "[NOR] Episode: 29270, Length: 71, Avg Reward: -169.720527296, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.01880455017\n",
      "[NOR] Episode: 29280, Length: 74, Avg Reward: -169.451813855, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -6.3583483696\n",
      "[NOR] Episode: 29290, Length: 88, Avg Reward: -185.172197022, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.86506843567\n",
      "[NOR] Episode: 29300, Length: 77, Avg Reward: -174.975528012, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.03659439087\n",
      "[NOR] Episode: 29310, Length: 53, Avg Reward: -176.053593508, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.10182762146\n",
      "[NOR] Episode: 29320, Length: 75, Avg Reward: -162.947520022, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.0163002014\n",
      "[NOR] Episode: 29330, Length: 92, Avg Reward: -163.580236759, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.80659723282\n",
      "[NOR] Episode: 29340, Length: 74, Avg Reward: -169.449940709, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.4731941223\n",
      "[NOR] Episode: 29350, Length: 87, Avg Reward: -187.082042584, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -7.45349979401\n",
      "[NOR] Episode: 29360, Length: 68, Avg Reward: -188.099088025, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.46793365479\n",
      "[NOR] Episode: 29370, Length: 56, Avg Reward: -158.08573628, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.33698797226\n",
      "[NOR] Episode: 29380, Length: 69, Avg Reward: -182.276078054, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 124.262008667\n",
      "[NOR] Episode: 29390, Length: 70, Avg Reward: -176.568433582, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.8857479095\n",
      "[NOR] Episode: 29400, Length: 89, Avg Reward: -157.879218254, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.0899848938\n",
      "[NOR] Episode: 29410, Length: 88, Avg Reward: -154.456742568, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.9123197794\n",
      "[NOR] Episode: 29420, Length: 81, Avg Reward: -178.015572688, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.59173965454\n",
      "[NOR] Episode: 29430, Length: 98, Avg Reward: -178.478057108, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.45774102211\n",
      "[NOR] Episode: 29440, Length: 99, Avg Reward: -166.39387113, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.38344764709\n",
      "[NOR] Episode: 29450, Length: 86, Avg Reward: -152.368214898, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.68815946579\n",
      "[NOR] Episode: 29460, Length: 94, Avg Reward: -181.357476358, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.787346363068\n",
      "[NOR] Episode: 29470, Length: 89, Avg Reward: -168.884760554, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.1646089554\n",
      "[NOR] Episode: 29480, Length: 98, Avg Reward: -134.613856389, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 297.2109375\n",
      "[NOR] Episode: 29490, Length: 78, Avg Reward: -158.720969369, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.88657045364\n",
      "[NOR] Episode: 29500, Length: 63, Avg Reward: -154.139789694, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.38121259212\n",
      "[NOR] Episode: 29510, Length: 66, Avg Reward: -187.337111898, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 291.490325928\n",
      "[NOR] Episode: 29520, Length: 82, Avg Reward: -165.970967071, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 152.037918091\n",
      "[NOR] Episode: 29530, Length: 94, Avg Reward: -191.857366743, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.22089385986\n",
      "[NOR] Episode: 29540, Length: 107, Avg Reward: -177.025091007, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.0889110565\n",
      "[NOR] Episode: 29550, Length: 96, Avg Reward: -174.658453113, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.96291899681\n",
      "[NOR] Episode: 29560, Length: 89, Avg Reward: -187.619444781, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.71537160873\n",
      "[NOR] Episode: 29570, Length: 114, Avg Reward: -151.282929796, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 34.2184066772\n",
      "[NOR] Episode: 29580, Length: 93, Avg Reward: -170.358695652, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.7533633709\n",
      "[NOR] Episode: 29590, Length: 87, Avg Reward: -166.77815784, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 286.828063965\n",
      "[NOR] Episode: 29600, Length: 64, Avg Reward: -144.002280642, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.84096288681\n",
      "[NOR] Episode: 29610, Length: 71, Avg Reward: -170.297017462, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 82.7610702515\n",
      "[NOR] Episode: 29620, Length: 78, Avg Reward: -165.139133652, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.89703202248\n",
      "[NOR] Episode: 29630, Length: 94, Avg Reward: -161.338685565, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.7801055908\n",
      "[NOR] Episode: 29640, Length: 108, Avg Reward: -153.709343278, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 223.438949585\n",
      "[NOR] Episode: 29650, Length: 74, Avg Reward: -154.161955073, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.10309362411\n",
      "[NOR] Episode: 29660, Length: 109, Avg Reward: -152.663604998, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.1696281433\n",
      "[NOR] Episode: 29670, Length: 87, Avg Reward: -143.818640369, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 26.17943573\n",
      "[NOR] Episode: 29680, Length: 91, Avg Reward: -155.994255647, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0699426531792\n",
      "[NOR] Episode: 29690, Length: 70, Avg Reward: -167.131733275, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.5327453613\n",
      "[NOR] Episode: 29700, Length: 95, Avg Reward: -172.043944766, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.34015130997\n",
      "[NOR] Episode: 29710, Length: 83, Avg Reward: -143.777551165, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.930670380592\n",
      "[NOR] Episode: 29720, Length: 85, Avg Reward: -135.240213102, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.3947753906\n",
      "[NOR] Episode: 29730, Length: 68, Avg Reward: -160.573982332, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.641710281372\n",
      "[NOR] Episode: 29740, Length: 85, Avg Reward: -148.423142864, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.64809608459\n",
      "[NOR] Episode: 29750, Length: 80, Avg Reward: -174.869566678, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 171.273834229\n",
      "[NOR] Episode: 29760, Length: 90, Avg Reward: -179.861937459, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 189.788879395\n",
      "[NOR] Episode: 29770, Length: 118, Avg Reward: -164.817965354, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.72280216217\n",
      "[NOR] Episode: 29780, Length: 76, Avg Reward: -146.60067966, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 28.828874588\n",
      "[NOR] Episode: 29790, Length: 72, Avg Reward: -143.76541555, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 87.7042160034\n",
      "[NOR] Episode: 29800, Length: 95, Avg Reward: -161.100934888, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 161.72177124\n",
      "[NOR] Episode: 29810, Length: 96, Avg Reward: -153.042754366, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 38.8345375061\n",
      "[NOR] Episode: 29820, Length: 90, Avg Reward: -176.107300715, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 40.8604316711\n",
      "[NOR] Episode: 29830, Length: 62, Avg Reward: -139.273754716, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.53782367706\n",
      "[NOR] Episode: 29840, Length: 74, Avg Reward: -159.256214183, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.21024274826\n",
      "[NOR] Episode: 29850, Length: 80, Avg Reward: -133.063834242, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.855678319931\n",
      "[NOR] Episode: 29860, Length: 114, Avg Reward: -159.610386796, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.36077594757\n",
      "[NOR] Episode: 29870, Length: 129, Avg Reward: -132.013724153, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.51012802124\n",
      "[NOR] Episode: 29880, Length: 126, Avg Reward: -148.094394782, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 344.161865234\n",
      "[NOR] Episode: 29890, Length: 78, Avg Reward: -162.815756765, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 162.483642578\n",
      "[NOR] Episode: 29900, Length: 60, Avg Reward: -159.971720043, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 40.5265350342\n",
      "[NOR] Episode: 29910, Length: 93, Avg Reward: -144.347812094, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.58284425735\n",
      "[NOR] Episode: 29920, Length: 83, Avg Reward: -157.132431111, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.00623202324\n",
      "[NOR] Episode: 29930, Length: 75, Avg Reward: -161.495804644, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.0952243805\n",
      "[NOR] Episode: 29940, Length: 94, Avg Reward: -152.680597273, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 232.513961792\n",
      "[NOR] Episode: 29950, Length: 74, Avg Reward: -170.677829446, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.4135131836\n",
      "[NOR] Episode: 29960, Length: 112, Avg Reward: -180.541442421, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.89516973495\n",
      "[NOR] Episode: 29970, Length: 100, Avg Reward: -155.279563394, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.6961688995\n",
      "[NOR] Episode: 29980, Length: 102, Avg Reward: -150.304121955, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.75689029694\n",
      "[NOR] Episode: 29990, Length: 115, Avg Reward: -153.849279188, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.966728329659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 09:27:10,083] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video030000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 30000, Length: 120, Avg Reward: -144.25223124, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.669044971466\n",
      "[NOR] Episode: 30010, Length: 97, Avg Reward: -141.227283623, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.1975669861\n",
      "[NOR] Episode: 30020, Length: 65, Avg Reward: -174.719849622, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 60.0115966797\n",
      "[NOR] Episode: 30030, Length: 71, Avg Reward: -172.448510895, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.91933822632\n",
      "[NOR] Episode: 30040, Length: 89, Avg Reward: -199.631773822, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.4771900177\n",
      "[NOR] Episode: 30050, Length: 81, Avg Reward: -138.425839543, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.076084137\n",
      "[NOR] Episode: 30060, Length: 100, Avg Reward: -118.750522821, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 59.2250404358\n",
      "[NOR] Episode: 30070, Length: 63, Avg Reward: -143.156156533, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 94.830871582\n",
      "[NOR] Episode: 30080, Length: 111, Avg Reward: -127.860039517, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.1560955048\n",
      "[NOR] Episode: 30090, Length: 100, Avg Reward: -150.410042116, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.06419754028\n",
      "[NOR] Episode: 30100, Length: 129, Avg Reward: -134.464618275, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 319.515777588\n",
      "[NOR] Episode: 30110, Length: 64, Avg Reward: -140.841770535, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.2894878387\n",
      "[NOR] Episode: 30120, Length: 144, Avg Reward: -184.31743282, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 91.2046203613\n",
      "[NOR] Episode: 30130, Length: 101, Avg Reward: -165.882438383, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.635974884\n",
      "[NOR] Episode: 30140, Length: 85, Avg Reward: -154.843012171, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 102.642204285\n",
      "[NOR] Episode: 30150, Length: 104, Avg Reward: -150.038372574, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 242.500579834\n",
      "[NOR] Episode: 30160, Length: 81, Avg Reward: -166.66519344, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.3532934189\n",
      "[NOR] Episode: 30170, Length: 117, Avg Reward: -145.198571842, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.654827833176\n",
      "[NOR] Episode: 30180, Length: 114, Avg Reward: -158.538539577, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.1405377388\n",
      "[NOR] Episode: 30190, Length: 145, Avg Reward: -175.31764139, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 47.7651596069\n",
      "[NOR] Episode: 30200, Length: 206, Avg Reward: -184.967743973, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.4847354889\n",
      "[NOR] Episode: 30210, Length: 274, Avg Reward: -233.34498824, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.65814447403\n",
      "[NOR] Episode: 30220, Length: 247, Avg Reward: -245.755866177, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.28172159195\n",
      "[NOR] Episode: 30230, Length: 113, Avg Reward: -316.06532289, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.97627544403\n",
      "[NOR] Episode: 30240, Length: 323, Avg Reward: -309.727778713, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 39.6818237305\n",
      "[NOR] Episode: 30250, Length: 116, Avg Reward: -183.725564283, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.610901713371\n",
      "[NOR] Episode: 30260, Length: 74, Avg Reward: -139.451797651, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 254.394958496\n",
      "[NOR] Episode: 30270, Length: 299, Avg Reward: -246.542070365, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 76.6676025391\n",
      "[NOR] Episode: 30280, Length: 90, Avg Reward: -275.352025909, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.66653347015\n",
      "[NOR] Episode: 30290, Length: 176, Avg Reward: -199.835574566, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.3307094574\n",
      "[NOR] Episode: 30300, Length: 128, Avg Reward: -285.469053046, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.0301990509\n",
      "[NOR] Episode: 30310, Length: 87, Avg Reward: -169.906463413, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 201.636352539\n",
      "[NOR] Episode: 30320, Length: 236, Avg Reward: -250.340805182, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 147.009643555\n",
      "[NOR] Episode: 30330, Length: 153, Avg Reward: -183.641372029, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.46087265015\n",
      "[NOR] Episode: 30340, Length: 153, Avg Reward: -280.963066629, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.35873889923\n",
      "[NOR] Episode: 30350, Length: 209, Avg Reward: -275.408252006, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 156.998748779\n",
      "[NOR] Episode: 30360, Length: 215, Avg Reward: -240.823317164, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.03607320786\n",
      "[NOR] Episode: 30370, Length: 98, Avg Reward: -279.374430399, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.7994709015\n",
      "[NOR] Episode: 30380, Length: 195, Avg Reward: -284.076561703, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.578710556\n",
      "[NOR] Episode: 30390, Length: 105, Avg Reward: -282.640079099, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.06687855721\n",
      "[NOR] Episode: 30400, Length: 174, Avg Reward: -249.337209981, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.19380092621\n",
      "[NOR] Episode: 30410, Length: 107, Avg Reward: -262.397259214, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 71.9571914673\n",
      "[NOR] Episode: 30420, Length: 74, Avg Reward: -172.733659756, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.0026855469\n",
      "[NOR] Episode: 30430, Length: 208, Avg Reward: -175.592626249, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.11884593964\n",
      "[NOR] Episode: 30440, Length: 133, Avg Reward: -275.360255522, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.02351045609\n",
      "[NOR] Episode: 30450, Length: 189, Avg Reward: -211.080468279, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 30.365814209\n",
      "[NOR] Episode: 30460, Length: 140, Avg Reward: -152.280718964, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.46026802063\n",
      "[NOR] Episode: 30470, Length: 141, Avg Reward: -140.784211482, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.06874847412\n",
      "[NOR] Episode: 30480, Length: 152, Avg Reward: -162.824744904, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.0731241703\n",
      "[NOR] Episode: 30490, Length: 144, Avg Reward: -166.979005898, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 104.853881836\n",
      "[NOR] Episode: 30500, Length: 96, Avg Reward: -207.4520071, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 57.6697235107\n",
      "[NOR] Episode: 30510, Length: 190, Avg Reward: -186.863145207, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.4419574738\n",
      "[NOR] Episode: 30520, Length: 173, Avg Reward: -183.191902161, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.70554351807\n",
      "[NOR] Episode: 30530, Length: 190, Avg Reward: -169.210383043, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.27382898331\n",
      "[NOR] Episode: 30540, Length: 140, Avg Reward: -217.532751614, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.154668808\n",
      "[NOR] Episode: 30550, Length: 107, Avg Reward: -242.699091226, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.68961048126\n",
      "[NOR] Episode: 30560, Length: 102, Avg Reward: -173.364740738, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 29.2711372375\n",
      "[NOR] Episode: 30570, Length: 81, Avg Reward: -162.384780298, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.61835587025\n",
      "[NOR] Episode: 30580, Length: 104, Avg Reward: -206.732005386, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 150.979873657\n",
      "[NOR] Episode: 30590, Length: 289, Avg Reward: -231.668896953, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.806758880615\n",
      "[NOR] Episode: 30600, Length: 219, Avg Reward: -180.474466246, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.5268039703\n",
      "[NOR] Episode: 30610, Length: 86, Avg Reward: -156.327524585, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.817987442\n",
      "[NOR] Episode: 30620, Length: 79, Avg Reward: -153.809095491, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.33492088318\n",
      "[NOR] Episode: 30630, Length: 132, Avg Reward: -96.1527376773, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.01492214203\n",
      "[NOR] Episode: 30640, Length: 146, Avg Reward: -206.230436031, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.2341384888\n",
      "[NOR] Episode: 30650, Length: 347, Avg Reward: -184.762674889, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 50.4109268188\n",
      "[NOR] Episode: 30660, Length: 88, Avg Reward: -113.822033986, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.172515869141\n",
      "[NOR] Episode: 30670, Length: 206, Avg Reward: -134.928792943, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.06079745293\n",
      "[NOR] Episode: 30680, Length: 87, Avg Reward: -127.709177517, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 25.4298934937\n",
      "[NOR] Episode: 30690, Length: 189, Avg Reward: -112.08198637, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.837138593197\n",
      "[NOR] Episode: 30700, Length: 66, Avg Reward: -117.95500479, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.85154533386\n",
      "[NOR] Episode: 30710, Length: 105, Avg Reward: -155.870188305, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.562012314796\n",
      "[NOR] Episode: 30720, Length: 110, Avg Reward: -142.941433358, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 93.9340591431\n",
      "[NOR] Episode: 30730, Length: 70, Avg Reward: -130.903669781, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 167.532546997\n",
      "[NOR] Episode: 30740, Length: 137, Avg Reward: -105.257523697, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 85.2966156006\n",
      "[NOR] Episode: 30750, Length: 83, Avg Reward: -100.178691266, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 34.4100189209\n",
      "[NOR] Episode: 30760, Length: 69, Avg Reward: -130.266201011, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.1251430511\n",
      "[NOR] Episode: 30770, Length: 117, Avg Reward: -120.509868619, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 35.6727600098\n",
      "[NOR] Episode: 30780, Length: 171, Avg Reward: -132.624472937, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 388.200836182\n",
      "[NOR] Episode: 30790, Length: 70, Avg Reward: -129.728400857, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 235.029678345\n",
      "[NOR] Episode: 30800, Length: 123, Avg Reward: -131.82483549, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.30570936203\n",
      "[NOR] Episode: 30810, Length: 125, Avg Reward: -148.521049081, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 140.40838623\n",
      "[NOR] Episode: 30820, Length: 520, Avg Reward: -107.939088921, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.3141014576\n",
      "[NOR] Episode: 30830, Length: 118, Avg Reward: -159.916791264, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.738910675\n",
      "[NOR] Episode: 30840, Length: 83, Avg Reward: -147.155468009, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 403.969451904\n",
      "[NOR] Episode: 30850, Length: 79, Avg Reward: -94.819063685, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.45263123512\n",
      "[NOR] Episode: 30860, Length: 71, Avg Reward: -142.542177812, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.24824464321\n",
      "[NOR] Episode: 30870, Length: 141, Avg Reward: -72.711028847, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.58900129795\n",
      "[NOR] Episode: 30880, Length: 121, Avg Reward: -95.0183373754, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.03472423553\n",
      "[NOR] Episode: 30890, Length: 92, Avg Reward: -145.795644349, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.119659900665\n",
      "[NOR] Episode: 30900, Length: 1000, Avg Reward: -106.416190036, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.64612340927\n",
      "[NOR] Episode: 30910, Length: 76, Avg Reward: -132.948822011, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -8.36506652832\n",
      "[NOR] Episode: 30920, Length: 68, Avg Reward: -121.882393069, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.84376454353\n",
      "[NOR] Episode: 30930, Length: 109, Avg Reward: -104.595592158, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 26.939283371\n",
      "[NOR] Episode: 30940, Length: 93, Avg Reward: -112.648395267, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 88.6784362793\n",
      "[NOR] Episode: 30950, Length: 66, Avg Reward: -115.164862731, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.43360567093\n",
      "[NOR] Episode: 30960, Length: 67, Avg Reward: -108.323777454, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 111.200836182\n",
      "[NOR] Episode: 30970, Length: 116, Avg Reward: -115.863114391, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 221.057830811\n",
      "[NOR] Episode: 30980, Length: 81, Avg Reward: -143.348683909, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.2059211731\n",
      "[NOR] Episode: 30990, Length: 74, Avg Reward: -137.364889549, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.0130271912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 09:35:33,783] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video031000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 31000, Length: 69, Avg Reward: -133.346754536, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -20.3285007477\n",
      "[NOR] Episode: 31010, Length: 75, Avg Reward: -117.719143423, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 293.667114258\n",
      "[NOR] Episode: 31020, Length: 75, Avg Reward: -101.237148012, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.4867258072\n",
      "[NOR] Episode: 31030, Length: 68, Avg Reward: -119.426517426, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 127.026145935\n",
      "[NOR] Episode: 31040, Length: 82, Avg Reward: -108.371811108, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 120.159545898\n",
      "[NOR] Episode: 31050, Length: 68, Avg Reward: -100.967480647, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -30.7338542938\n",
      "[NOR] Episode: 31060, Length: 80, Avg Reward: -126.368000926, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -29.669757843\n",
      "[NOR] Episode: 31070, Length: 73, Avg Reward: -125.563975178, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 24.5401763916\n",
      "[NOR] Episode: 31080, Length: 61, Avg Reward: -118.014343403, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.15770149231\n",
      "[NOR] Episode: 31090, Length: 96, Avg Reward: -146.269722922, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.1812171936\n",
      "[NOR] Episode: 31100, Length: 97, Avg Reward: -122.525108781, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 131.190856934\n",
      "[NOR] Episode: 31110, Length: 82, Avg Reward: -126.197340003, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 173.097625732\n",
      "[NOR] Episode: 31120, Length: 93, Avg Reward: -126.34378277, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.16903305054\n",
      "[NOR] Episode: 31130, Length: 58, Avg Reward: -109.654659329, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 446.041015625\n",
      "[NOR] Episode: 31140, Length: 88, Avg Reward: -120.111942469, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 52.6064910889\n",
      "[NOR] Episode: 31150, Length: 95, Avg Reward: -116.86099272, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 150.159683228\n",
      "[NOR] Episode: 31160, Length: 74, Avg Reward: -140.422753403, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 179.498260498\n",
      "[NOR] Episode: 31170, Length: 70, Avg Reward: -117.064209061, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 77.9113769531\n",
      "[NOR] Episode: 31180, Length: 90, Avg Reward: -128.442350742, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.8293132782\n",
      "[NOR] Episode: 31190, Length: 94, Avg Reward: -136.30874436, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 270.195007324\n",
      "[NOR] Episode: 31200, Length: 76, Avg Reward: -115.636950127, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 88.7119750977\n",
      "[NOR] Episode: 31210, Length: 72, Avg Reward: -135.105687655, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 103.673835754\n",
      "[NOR] Episode: 31220, Length: 85, Avg Reward: -99.3427421212, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.33950805664\n",
      "[NOR] Episode: 31230, Length: 71, Avg Reward: -108.23705403, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.8732500076\n",
      "[NOR] Episode: 31240, Length: 90, Avg Reward: -126.040359328, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.591632843\n",
      "[NOR] Episode: 31250, Length: 71, Avg Reward: -129.701046327, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 70.7475585938\n",
      "[NOR] Episode: 31260, Length: 100, Avg Reward: -129.309135747, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.7486019135\n",
      "[NOR] Episode: 31270, Length: 87, Avg Reward: -125.735286973, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.20253658295\n",
      "[NOR] Episode: 31280, Length: 87, Avg Reward: -124.911175055, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.18696975708\n",
      "[NOR] Episode: 31290, Length: 69, Avg Reward: -133.210574563, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 143.809143066\n",
      "[NOR] Episode: 31300, Length: 72, Avg Reward: -115.113495634, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.4283169508\n",
      "[NOR] Episode: 31310, Length: 79, Avg Reward: -122.745656849, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.826150655746\n",
      "[NOR] Episode: 31320, Length: 92, Avg Reward: -138.048187128, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.33564138412\n",
      "[NOR] Episode: 31330, Length: 94, Avg Reward: -142.896639044, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 58.9602470398\n",
      "[NOR] Episode: 31340, Length: 73, Avg Reward: -111.372949363, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.740525722504\n",
      "[NOR] Episode: 31350, Length: 74, Avg Reward: -96.39696126, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.73801803589\n",
      "[NOR] Episode: 31360, Length: 67, Avg Reward: -123.005623176, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 188.195281982\n",
      "[NOR] Episode: 31370, Length: 64, Avg Reward: -125.835678918, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.71368598938\n",
      "[NOR] Episode: 31380, Length: 93, Avg Reward: -114.36611333, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -19.4729137421\n",
      "[NOR] Episode: 31390, Length: 64, Avg Reward: -123.456220297, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 96.8435821533\n",
      "[NOR] Episode: 31400, Length: 60, Avg Reward: -97.6010913537, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 198.237350464\n",
      "[NOR] Episode: 31410, Length: 118, Avg Reward: -82.7365311913, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.52017021179\n",
      "[NOR] Episode: 31420, Length: 102, Avg Reward: -106.737759986, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.4683761597\n",
      "[NOR] Episode: 31430, Length: 81, Avg Reward: -112.915764776, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 237.773422241\n",
      "[NOR] Episode: 31440, Length: 113, Avg Reward: -123.080266503, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.2113399506\n",
      "[NOR] Episode: 31450, Length: 76, Avg Reward: -114.368594281, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.83871769905\n",
      "[NOR] Episode: 31460, Length: 99, Avg Reward: -135.236836315, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.52074432373\n",
      "[NOR] Episode: 31470, Length: 74, Avg Reward: -130.55633825, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.23524129391\n",
      "[NOR] Episode: 31480, Length: 100, Avg Reward: -108.149710527, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.30813574791\n",
      "[NOR] Episode: 31490, Length: 95, Avg Reward: -100.88273446, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 74.9209976196\n",
      "[NOR] Episode: 31500, Length: 107, Avg Reward: -132.606964918, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 177.985198975\n",
      "[NOR] Episode: 31510, Length: 96, Avg Reward: -119.357642903, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.94231438637\n",
      "[NOR] Episode: 31520, Length: 113, Avg Reward: -123.575273642, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 87.8641662598\n",
      "[NOR] Episode: 31530, Length: 88, Avg Reward: -99.6449817942, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.865654170513\n",
      "[NOR] Episode: 31540, Length: 80, Avg Reward: -123.604771841, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.8604488373\n",
      "[NOR] Episode: 31550, Length: 104, Avg Reward: -137.27744939, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 260.690826416\n",
      "[NOR] Episode: 31560, Length: 88, Avg Reward: -122.12425459, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.62643492222\n",
      "[NOR] Episode: 31570, Length: 65, Avg Reward: -131.344473818, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 110.371177673\n",
      "[NOR] Episode: 31580, Length: 85, Avg Reward: -109.583682433, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.06443214417\n",
      "[NOR] Episode: 31590, Length: 90, Avg Reward: -119.275547337, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.7359428406\n",
      "[NOR] Episode: 31600, Length: 63, Avg Reward: -128.321387132, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 86.7352600098\n",
      "[NOR] Episode: 31610, Length: 165, Avg Reward: -117.537934852, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 35.2326545715\n",
      "[NOR] Episode: 31620, Length: 116, Avg Reward: -106.243529071, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.17248630524\n",
      "[NOR] Episode: 31630, Length: 101, Avg Reward: -119.312828508, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.10582542419\n",
      "[NOR] Episode: 31640, Length: 78, Avg Reward: -106.250633283, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 25.0399246216\n",
      "[NOR] Episode: 31650, Length: 100, Avg Reward: -88.7803158843, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.0277423859\n",
      "[NOR] Episode: 31660, Length: 176, Avg Reward: -171.960437939, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.26607608795\n",
      "[NOR] Episode: 31670, Length: 78, Avg Reward: -109.185489663, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.06395959854\n",
      "[NOR] Episode: 31680, Length: 88, Avg Reward: -100.840592186, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.73402118683\n",
      "[NOR] Episode: 31690, Length: 118, Avg Reward: -105.205603273, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 78.2691955566\n",
      "[NOR] Episode: 31700, Length: 97, Avg Reward: -112.613565115, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.47628998756\n",
      "[NOR] Episode: 31710, Length: 94, Avg Reward: -131.638573128, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.3462028503\n",
      "[NOR] Episode: 31720, Length: 75, Avg Reward: -108.863805421, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 176.789550781\n",
      "[NOR] Episode: 31730, Length: 100, Avg Reward: -116.742425209, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.2764511108\n",
      "[NOR] Episode: 31740, Length: 107, Avg Reward: -123.083716051, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 26.0762863159\n",
      "[NOR] Episode: 31750, Length: 93, Avg Reward: -108.98702813, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.34274697304\n",
      "[NOR] Episode: 31760, Length: 102, Avg Reward: -126.906169489, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 139.529296875\n",
      "[NOR] Episode: 31770, Length: 87, Avg Reward: -92.2796942649, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.808349132538\n",
      "[NOR] Episode: 31780, Length: 78, Avg Reward: -96.3525826489, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.16546750069\n",
      "[NOR] Episode: 31790, Length: 86, Avg Reward: -114.668611324, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.52763843536\n",
      "[NOR] Episode: 31800, Length: 64, Avg Reward: -141.767539562, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.5469741821\n",
      "[NOR] Episode: 31810, Length: 78, Avg Reward: -135.719808456, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.79669570923\n",
      "[NOR] Episode: 31820, Length: 79, Avg Reward: -113.838620415, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.7098293304\n",
      "[NOR] Episode: 31830, Length: 102, Avg Reward: -75.306782784, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 230.584793091\n",
      "[NOR] Episode: 31840, Length: 73, Avg Reward: -85.1485073465, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.03248882294\n",
      "[NOR] Episode: 31850, Length: 88, Avg Reward: -96.6838865383, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.37982177734\n",
      "[NOR] Episode: 31860, Length: 122, Avg Reward: -93.6735395414, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.47901582718\n",
      "[NOR] Episode: 31870, Length: 186, Avg Reward: -76.7963766646, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 50.7256469727\n",
      "[NOR] Episode: 31880, Length: 99, Avg Reward: -93.2809629557, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 169.989379883\n",
      "[NOR] Episode: 31890, Length: 90, Avg Reward: -83.059681841, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 424.081848145\n",
      "[NOR] Episode: 31900, Length: 135, Avg Reward: -128.961991816, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.2334251404\n",
      "[NOR] Episode: 31910, Length: 91, Avg Reward: -84.3113021545, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.20483493805\n",
      "[NOR] Episode: 31920, Length: 112, Avg Reward: -91.6843979716, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.6591377258\n",
      "[NOR] Episode: 31930, Length: 325, Avg Reward: -139.431041744, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 273.107543945\n",
      "[NOR] Episode: 31940, Length: 88, Avg Reward: -164.469669124, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.05238890648\n",
      "[NOR] Episode: 31950, Length: 120, Avg Reward: -75.490334678, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.70209503174\n",
      "[NOR] Episode: 31960, Length: 104, Avg Reward: -104.080548039, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 105.137664795\n",
      "[NOR] Episode: 31970, Length: 90, Avg Reward: -114.42354091, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.384059906\n",
      "[NOR] Episode: 31980, Length: 179, Avg Reward: -96.0991035196, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.25707912445\n",
      "[NOR] Episode: 31990, Length: 67, Avg Reward: -111.796351869, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.09105873108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 09:41:21,566] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video032000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 32000, Length: 216, Avg Reward: -128.237475412, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.20303583145\n",
      "[NOR] Episode: 32010, Length: 72, Avg Reward: -144.959607426, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.245734781027\n",
      "[NOR] Episode: 32020, Length: 64, Avg Reward: -143.522464868, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.40028202534\n",
      "[NOR] Episode: 32030, Length: 89, Avg Reward: -111.671469281, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 51.50522995\n",
      "[NOR] Episode: 32040, Length: 136, Avg Reward: -89.8293620834, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.822877287865\n",
      "[NOR] Episode: 32050, Length: 76, Avg Reward: -97.6693660712, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.4312915802\n",
      "[NOR] Episode: 32060, Length: 99, Avg Reward: -157.610728374, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 48.9243545532\n",
      "[NOR] Episode: 32070, Length: 69, Avg Reward: -129.144579187, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 398.526702881\n",
      "[NOR] Episode: 32080, Length: 92, Avg Reward: -127.486493211, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.7996339798\n",
      "[NOR] Episode: 32090, Length: 70, Avg Reward: -141.515913793, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.73027229309\n",
      "[NOR] Episode: 32100, Length: 66, Avg Reward: -122.429310589, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 78.0811538696\n",
      "[NOR] Episode: 32110, Length: 69, Avg Reward: -145.135719467, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.9261760712\n",
      "[NOR] Episode: 32120, Length: 106, Avg Reward: -127.294526378, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.65839385986\n",
      "[NOR] Episode: 32130, Length: 74, Avg Reward: -107.756483782, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.606762766838\n",
      "[NOR] Episode: 32140, Length: 79, Avg Reward: -150.671763775, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.392647624016\n",
      "[NOR] Episode: 32150, Length: 78, Avg Reward: -121.069941789, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.15025985241\n",
      "[NOR] Episode: 32160, Length: 75, Avg Reward: -120.336786067, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.43220329285\n",
      "[NOR] Episode: 32170, Length: 133, Avg Reward: -60.2338219947, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.7915699482\n",
      "[NOR] Episode: 32180, Length: 58, Avg Reward: -59.2145738236, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.34327280521\n",
      "[NOR] Episode: 32190, Length: 90, Avg Reward: -106.28270871, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.162492275238\n",
      "[NOR] Episode: 32200, Length: 99, Avg Reward: -101.03465052, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 101.599533081\n",
      "[NOR] Episode: 32210, Length: 550, Avg Reward: -83.2184711365, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.74841380119\n",
      "[NOR] Episode: 32220, Length: 70, Avg Reward: -130.080777645, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.716019749641\n",
      "[NOR] Episode: 32230, Length: 71, Avg Reward: -148.477510249, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.1468753815\n",
      "[NOR] Episode: 32240, Length: 70, Avg Reward: -132.281531578, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.40507388115\n",
      "[NOR] Episode: 32250, Length: 82, Avg Reward: -124.365125658, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.20243525505\n",
      "[NOR] Episode: 32260, Length: 56, Avg Reward: -112.907182793, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 41.7868804932\n",
      "[NOR] Episode: 32270, Length: 75, Avg Reward: -124.962820592, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.8385829926\n",
      "[NOR] Episode: 32280, Length: 90, Avg Reward: -141.103395466, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.2578849792\n",
      "[NOR] Episode: 32290, Length: 70, Avg Reward: -118.956859771, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 81.3124847412\n",
      "[NOR] Episode: 32300, Length: 109, Avg Reward: -123.169003331, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.84719562531\n",
      "[NOR] Episode: 32310, Length: 94, Avg Reward: -124.367308664, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.73893547058\n",
      "[NOR] Episode: 32320, Length: 171, Avg Reward: -117.892833437, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.68719768524\n",
      "[NOR] Episode: 32330, Length: 122, Avg Reward: -59.3223992419, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 134.808166504\n",
      "[NOR] Episode: 32340, Length: 89, Avg Reward: -113.685512591, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.8640766144\n",
      "[NOR] Episode: 32350, Length: 68, Avg Reward: -116.441941678, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 82.7637176514\n",
      "[NOR] Episode: 32360, Length: 184, Avg Reward: -103.795887741, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.89879965782\n",
      "[NOR] Episode: 32370, Length: 98, Avg Reward: -98.9801031608, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.475297778845\n",
      "[NOR] Episode: 32380, Length: 114, Avg Reward: -117.989561629, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.33595561981\n",
      "[NOR] Episode: 32390, Length: 131, Avg Reward: -100.39240895, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.95469474792\n",
      "[NOR] Episode: 32400, Length: 106, Avg Reward: -115.970726379, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.969734013081\n",
      "[NOR] Episode: 32410, Length: 137, Avg Reward: -120.078934829, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.94550967216\n",
      "[NOR] Episode: 32420, Length: 69, Avg Reward: -121.260102719, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 39.6266326904\n",
      "[NOR] Episode: 32430, Length: 78, Avg Reward: -124.84197079, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.07546806335\n",
      "[NOR] Episode: 32440, Length: 73, Avg Reward: -119.441523627, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 227.088272095\n",
      "[NOR] Episode: 32450, Length: 93, Avg Reward: -148.964375647, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 57.5416488647\n",
      "[NOR] Episode: 32460, Length: 96, Avg Reward: -141.658081875, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 76.4705505371\n",
      "[NOR] Episode: 32470, Length: 98, Avg Reward: -130.99817143, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.73281216621\n",
      "[NOR] Episode: 32480, Length: 99, Avg Reward: -140.607285212, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.528711319\n",
      "[NOR] Episode: 32490, Length: 83, Avg Reward: -133.951482872, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.989995181561\n",
      "[NOR] Episode: 32500, Length: 94, Avg Reward: -118.250062214, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 31.8760299683\n",
      "[NOR] Episode: 32510, Length: 80, Avg Reward: -126.798599007, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.3672428131\n",
      "[NOR] Episode: 32520, Length: 105, Avg Reward: -124.841522674, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 194.956481934\n",
      "[NOR] Episode: 32530, Length: 62, Avg Reward: -64.2927227609, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 49.8048095703\n",
      "[NOR] Episode: 32540, Length: 65, Avg Reward: -104.801401826, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.9912376404\n",
      "[NOR] Episode: 32550, Length: 90, Avg Reward: -139.611064113, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 20.8088569641\n",
      "[NOR] Episode: 32560, Length: 88, Avg Reward: -137.152726868, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 34.9330596924\n",
      "[NOR] Episode: 32570, Length: 135, Avg Reward: -110.054842794, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.21818971634\n",
      "[NOR] Episode: 32580, Length: 69, Avg Reward: -151.283154474, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.51957559586\n",
      "[NOR] Episode: 32590, Length: 83, Avg Reward: -121.567796307, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.62084388733\n",
      "[NOR] Episode: 32600, Length: 95, Avg Reward: -129.271642686, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.55854034424\n",
      "[NOR] Episode: 32610, Length: 79, Avg Reward: -125.63466698, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.15177106857\n",
      "[NOR] Episode: 32620, Length: 125, Avg Reward: -126.910998484, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.31854343414\n",
      "[NOR] Episode: 32630, Length: 98, Avg Reward: -119.32635083, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.45785474777\n",
      "[NOR] Episode: 32640, Length: 73, Avg Reward: -143.295340675, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 134.023040771\n",
      "[NOR] Episode: 32650, Length: 128, Avg Reward: -119.673146494, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.896314799786\n",
      "[NOR] Episode: 32660, Length: 162, Avg Reward: -93.3970620476, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 29.0411109924\n",
      "[NOR] Episode: 32670, Length: 132, Avg Reward: -64.4527093755, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 69.5413131714\n",
      "[NOR] Episode: 32680, Length: 110, Avg Reward: -91.2015984883, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.87401103973\n",
      "[NOR] Episode: 32690, Length: 97, Avg Reward: -104.645904213, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.98155021667\n",
      "[NOR] Episode: 32700, Length: 120, Avg Reward: -100.480023672, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 30.9926681519\n",
      "[NOR] Episode: 32710, Length: 148, Avg Reward: -101.113535452, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 88.5697021484\n",
      "[NOR] Episode: 32720, Length: 107, Avg Reward: -107.391221425, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.3819179535\n",
      "[NOR] Episode: 32730, Length: 237, Avg Reward: -82.3948941514, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.9516334534\n",
      "[NOR] Episode: 32740, Length: 150, Avg Reward: -72.5784727268, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.7640247345\n",
      "[NOR] Episode: 32750, Length: 168, Avg Reward: -97.3272977575, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.55621147156\n",
      "[NOR] Episode: 32760, Length: 128, Avg Reward: -110.420916064, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.30773568153\n",
      "[NOR] Episode: 32770, Length: 175, Avg Reward: -103.344916979, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 131.442443848\n",
      "[NOR] Episode: 32780, Length: 279, Avg Reward: -55.5269962332, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 67.3723907471\n",
      "[NOR] Episode: 32790, Length: 162, Avg Reward: -80.3390334051, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.9323577881\n",
      "[NOR] Episode: 32800, Length: 125, Avg Reward: -82.7256998892, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.6677808762\n",
      "[NOR] Episode: 32810, Length: 144, Avg Reward: -125.368872518, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.56895971298\n",
      "[NOR] Episode: 32820, Length: 104, Avg Reward: -124.235172299, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.26248848438\n",
      "[NOR] Episode: 32830, Length: 159, Avg Reward: -89.6409160448, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.617149353\n",
      "[NOR] Episode: 32840, Length: 149, Avg Reward: -36.7506198704, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.33229613304\n",
      "[NOR] Episode: 32850, Length: 86, Avg Reward: -66.07087696, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.07707333565\n",
      "[NOR] Episode: 32860, Length: 155, Avg Reward: -102.052696094, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.05326080322\n",
      "[NOR] Episode: 32870, Length: 121, Avg Reward: -86.4588951977, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.42059659958\n",
      "[NOR] Episode: 32880, Length: 215, Avg Reward: -105.221456985, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.83205723763\n",
      "[NOR] Episode: 32890, Length: 140, Avg Reward: -100.724369692, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.17740345\n",
      "[NOR] Episode: 32900, Length: 134, Avg Reward: -108.669736294, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.39831781387\n",
      "[NOR] Episode: 32910, Length: 117, Avg Reward: -115.684420693, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.06506681442\n",
      "[NOR] Episode: 32920, Length: 257, Avg Reward: -127.911604892, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.4411869049\n",
      "[NOR] Episode: 32930, Length: 142, Avg Reward: -105.237532899, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.25243091583\n",
      "[NOR] Episode: 32940, Length: 128, Avg Reward: -54.3897075316, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.8894867897\n",
      "[NOR] Episode: 32950, Length: 173, Avg Reward: -108.436101002, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.87902855873\n",
      "[NOR] Episode: 32960, Length: 190, Avg Reward: -94.6504427433, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 89.8476028442\n",
      "[NOR] Episode: 32970, Length: 131, Avg Reward: -36.1188705704, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.3607978821\n",
      "[NOR] Episode: 32980, Length: 279, Avg Reward: -64.8736335254, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.557438731194\n",
      "[NOR] Episode: 32990, Length: 144, Avg Reward: -73.7120914132, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.210746854544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 09:48:43,820] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video033000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 33000, Length: 107, Avg Reward: -86.2781395876, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.48206686974\n",
      "[NOR] Episode: 33010, Length: 116, Avg Reward: -79.2499906262, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.573325157166\n",
      "[NOR] Episode: 33020, Length: 86, Avg Reward: -82.397937899, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.67536830902\n",
      "[NOR] Episode: 33030, Length: 356, Avg Reward: -87.8579888415, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 428.9402771\n",
      "[NOR] Episode: 33040, Length: 210, Avg Reward: -101.646204573, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.14794254303\n",
      "[NOR] Episode: 33050, Length: 129, Avg Reward: -110.689325951, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 89.4447631836\n",
      "[NOR] Episode: 33060, Length: 584, Avg Reward: -73.2571984054, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.35595464706\n",
      "[NOR] Episode: 33070, Length: 78, Avg Reward: -109.618202556, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 184.988220215\n",
      "[NOR] Episode: 33080, Length: 127, Avg Reward: -71.8849709613, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 123.275466919\n",
      "[NOR] Episode: 33090, Length: 142, Avg Reward: -103.222092474, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 27.5535583496\n",
      "[NOR] Episode: 33100, Length: 151, Avg Reward: -122.809371647, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 146.504425049\n",
      "[NOR] Episode: 33110, Length: 184, Avg Reward: -73.3933631385, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 24.5882301331\n",
      "[NOR] Episode: 33120, Length: 227, Avg Reward: -106.924239698, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.09772872925\n",
      "[NOR] Episode: 33130, Length: 231, Avg Reward: -143.800076824, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 36.3143730164\n",
      "[NOR] Episode: 33140, Length: 323, Avg Reward: -196.627612105, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 198.206451416\n",
      "[NOR] Episode: 33150, Length: 415, Avg Reward: -152.358885861, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.77147340775\n",
      "[NOR] Episode: 33160, Length: 119, Avg Reward: -112.206368997, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.48148202896\n",
      "[NOR] Episode: 33170, Length: 99, Avg Reward: -115.031794395, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 135.363555908\n",
      "[NOR] Episode: 33180, Length: 260, Avg Reward: -98.8124821104, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -12.8743476868\n",
      "[NOR] Episode: 33190, Length: 162, Avg Reward: -63.5515830109, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 151.039916992\n",
      "[NOR] Episode: 33200, Length: 114, Avg Reward: -160.360978258, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.41542434692\n",
      "[NOR] Episode: 33210, Length: 412, Avg Reward: -125.511215434, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.24587583542\n",
      "[NOR] Episode: 33220, Length: 163, Avg Reward: -175.128298519, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 137.655944824\n",
      "[NOR] Episode: 33230, Length: 235, Avg Reward: -143.359321375, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.93074274063\n",
      "[NOR] Episode: 33240, Length: 116, Avg Reward: -137.302438105, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.238678783178\n",
      "[NOR] Episode: 33250, Length: 266, Avg Reward: -131.611541172, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.06838178635\n",
      "[NOR] Episode: 33260, Length: 152, Avg Reward: -147.788082671, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.2489433289\n",
      "[NOR] Episode: 33270, Length: 117, Avg Reward: -148.330211433, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 272.679626465\n",
      "[NOR] Episode: 33280, Length: 376, Avg Reward: -108.713857316, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.388718247414\n",
      "[NOR] Episode: 33290, Length: 470, Avg Reward: -148.686020115, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.20202541351\n",
      "[NOR] Episode: 33300, Length: 379, Avg Reward: -180.60140942, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.418095588684\n",
      "[NOR] Episode: 33310, Length: 211, Avg Reward: -187.043701703, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.340798377991\n",
      "[NOR] Episode: 33320, Length: 262, Avg Reward: -186.064063239, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.71283149719\n",
      "[NOR] Episode: 33330, Length: 340, Avg Reward: -189.270670425, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 82.7562179565\n",
      "[NOR] Episode: 33340, Length: 289, Avg Reward: -161.995157252, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.75534343719\n",
      "[NOR] Episode: 33350, Length: 368, Avg Reward: -181.068788444, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.533619761467\n",
      "[NOR] Episode: 33360, Length: 156, Avg Reward: -165.814054523, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.94321727753\n",
      "[NOR] Episode: 33370, Length: 312, Avg Reward: -163.248380863, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.21010303497\n",
      "[NOR] Episode: 33380, Length: 187, Avg Reward: -192.491891911, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.25251221657\n",
      "[NOR] Episode: 33390, Length: 227, Avg Reward: -144.637985337, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.58851528168\n",
      "[NOR] Episode: 33400, Length: 225, Avg Reward: -125.541078284, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.88801860809\n",
      "[NOR] Episode: 33410, Length: 685, Avg Reward: -115.925611136, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.27095127106\n",
      "[NOR] Episode: 33420, Length: 312, Avg Reward: -217.294688938, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.93255186081\n",
      "[NOR] Episode: 33430, Length: 210, Avg Reward: -210.733894604, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.272735834122\n",
      "[NOR] Episode: 33440, Length: 296, Avg Reward: -143.641327346, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.07847619057\n",
      "[NOR] Episode: 33450, Length: 302, Avg Reward: -206.759834438, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 186.395874023\n",
      "[NOR] Episode: 33460, Length: 99, Avg Reward: -215.709223471, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 130.385147095\n",
      "[NOR] Episode: 33470, Length: 262, Avg Reward: -254.585826249, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.55825042725\n",
      "[NOR] Episode: 33480, Length: 144, Avg Reward: -240.167547727, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.693437635899\n",
      "[NOR] Episode: 33490, Length: 330, Avg Reward: -182.405401997, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 136.864349365\n",
      "[NOR] Episode: 33500, Length: 164, Avg Reward: -187.365202064, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.62887859344\n",
      "[NOR] Episode: 33510, Length: 217, Avg Reward: -201.520259842, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 33.9603347778\n",
      "[NOR] Episode: 33520, Length: 218, Avg Reward: -221.225951803, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.1755771637\n",
      "[NOR] Episode: 33530, Length: 177, Avg Reward: -149.481212361, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 54.6096458435\n",
      "[NOR] Episode: 33540, Length: 303, Avg Reward: -163.19580867, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.98059654236\n",
      "[NOR] Episode: 33550, Length: 104, Avg Reward: -116.189278918, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.82596302032\n",
      "[NOR] Episode: 33560, Length: 322, Avg Reward: -140.371247068, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.221835777164\n",
      "[NOR] Episode: 33570, Length: 187, Avg Reward: -136.635152405, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.123069047928\n",
      "[NOR] Episode: 33580, Length: 83, Avg Reward: -123.062915924, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.06715297699\n",
      "[NOR] Episode: 33590, Length: 174, Avg Reward: -146.533660624, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.35234022141\n",
      "[NOR] Episode: 33600, Length: 147, Avg Reward: -162.539204263, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.17993807793\n",
      "[NOR] Episode: 33610, Length: 136, Avg Reward: -134.650003403, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.277658522129\n",
      "[NOR] Episode: 33620, Length: 512, Avg Reward: -108.837125552, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 62.160774231\n",
      "[NOR] Episode: 33630, Length: 184, Avg Reward: -121.751991442, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.46735358238\n",
      "[NOR] Episode: 33640, Length: 141, Avg Reward: -185.283572608, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.48905992508\n",
      "[NOR] Episode: 33650, Length: 171, Avg Reward: -107.519261743, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.02208030224\n",
      "[NOR] Episode: 33660, Length: 93, Avg Reward: -127.750108725, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.6690273285\n",
      "[NOR] Episode: 33670, Length: 120, Avg Reward: -108.920972721, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.34829568863\n",
      "[NOR] Episode: 33680, Length: 267, Avg Reward: -179.139202596, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 175.684585571\n",
      "[NOR] Episode: 33690, Length: 619, Avg Reward: -171.089553477, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.2854886055\n",
      "[NOR] Episode: 33700, Length: 189, Avg Reward: -107.392909579, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.13141965866\n",
      "[NOR] Episode: 33710, Length: 81, Avg Reward: -154.482273109, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.690441310406\n",
      "[NOR] Episode: 33720, Length: 140, Avg Reward: -127.954402356, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.17011213303\n",
      "[NOR] Episode: 33730, Length: 101, Avg Reward: -148.442980039, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.717382431\n",
      "[NOR] Episode: 33740, Length: 119, Avg Reward: -123.425847581, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 245.522537231\n",
      "[NOR] Episode: 33750, Length: 132, Avg Reward: -118.179692221, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 24.6190376282\n",
      "[NOR] Episode: 33760, Length: 87, Avg Reward: -105.830560743, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 32.560760498\n",
      "[NOR] Episode: 33770, Length: 65, Avg Reward: -130.564971405, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.464328527451\n",
      "[NOR] Episode: 33780, Length: 68, Avg Reward: -137.017549581, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.0076913834\n",
      "[NOR] Episode: 33790, Length: 92, Avg Reward: -130.468926048, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 97.6610870361\n",
      "[NOR] Episode: 33800, Length: 208, Avg Reward: -126.412825034, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.47716283798\n",
      "[NOR] Episode: 33810, Length: 153, Avg Reward: -129.897814771, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.86689567566\n",
      "[NOR] Episode: 33820, Length: 71, Avg Reward: -135.856654533, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 175.077957153\n",
      "[NOR] Episode: 33830, Length: 82, Avg Reward: -122.300062436, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 251.104644775\n",
      "[NOR] Episode: 33840, Length: 90, Avg Reward: -147.761413549, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.96455103159\n",
      "[NOR] Episode: 33850, Length: 87, Avg Reward: -149.465892507, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 180.946472168\n",
      "[NOR] Episode: 33860, Length: 79, Avg Reward: -135.361936733, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.0293388367\n",
      "[NOR] Episode: 33870, Length: 58, Avg Reward: -149.812750964, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.31945300102\n",
      "[NOR] Episode: 33880, Length: 72, Avg Reward: -150.705248664, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.31563234329\n",
      "[NOR] Episode: 33890, Length: 107, Avg Reward: -132.661234553, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.8927693367\n",
      "[NOR] Episode: 33900, Length: 67, Avg Reward: -147.715563743, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.19025611877\n",
      "[NOR] Episode: 33910, Length: 70, Avg Reward: -151.81030533, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.83527851105\n",
      "[NOR] Episode: 33920, Length: 72, Avg Reward: -155.959255143, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.70452213287\n",
      "[NOR] Episode: 33930, Length: 82, Avg Reward: -151.507903596, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.81236696243\n",
      "[NOR] Episode: 33940, Length: 79, Avg Reward: -134.044958805, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.7612667084\n",
      "[NOR] Episode: 33950, Length: 69, Avg Reward: -142.158774456, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 110.311553955\n",
      "[NOR] Episode: 33960, Length: 60, Avg Reward: -140.708253115, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.22863197327\n",
      "[NOR] Episode: 33970, Length: 100, Avg Reward: -150.808530593, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 199.700149536\n",
      "[NOR] Episode: 33980, Length: 66, Avg Reward: -133.658490961, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.25946617126\n",
      "[NOR] Episode: 33990, Length: 103, Avg Reward: -139.35115874, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.45625090599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 10:01:58,767] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video034000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 34000, Length: 64, Avg Reward: -135.374107981, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.73990869522\n",
      "[NOR] Episode: 34010, Length: 89, Avg Reward: -132.974560729, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 263.405303955\n",
      "[NOR] Episode: 34020, Length: 57, Avg Reward: -154.349486746, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.27914142609\n",
      "[NOR] Episode: 34030, Length: 89, Avg Reward: -144.817773667, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.0900230408\n",
      "[NOR] Episode: 34040, Length: 71, Avg Reward: -148.95073755, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.178569197655\n",
      "[NOR] Episode: 34050, Length: 85, Avg Reward: -135.511916208, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.4436092377\n",
      "[NOR] Episode: 34060, Length: 63, Avg Reward: -142.121816017, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 118.453346252\n",
      "[NOR] Episode: 34070, Length: 91, Avg Reward: -135.804308566, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 73.7320861816\n",
      "[NOR] Episode: 34080, Length: 91, Avg Reward: -138.809503648, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.6744508743\n",
      "[NOR] Episode: 34090, Length: 55, Avg Reward: -142.751161577, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 36.0205955505\n",
      "[NOR] Episode: 34100, Length: 71, Avg Reward: -149.717604896, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.88559174538\n",
      "[NOR] Episode: 34110, Length: 95, Avg Reward: -131.181299313, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 32.2034988403\n",
      "[NOR] Episode: 34120, Length: 62, Avg Reward: -154.690604059, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.40131187439\n",
      "[NOR] Episode: 34130, Length: 95, Avg Reward: -129.254285932, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 59.7250061035\n",
      "[NOR] Episode: 34140, Length: 82, Avg Reward: -138.784191538, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 232.698974609\n",
      "[NOR] Episode: 34150, Length: 74, Avg Reward: -143.026963437, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.17394351959\n",
      "[NOR] Episode: 34160, Length: 79, Avg Reward: -140.932738135, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 203.878753662\n",
      "[NOR] Episode: 34170, Length: 59, Avg Reward: -140.363962299, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.26064968109\n",
      "[NOR] Episode: 34180, Length: 92, Avg Reward: -134.273397239, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0638786554337\n",
      "[NOR] Episode: 34190, Length: 89, Avg Reward: -146.191078059, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.2441091537\n",
      "[NOR] Episode: 34200, Length: 84, Avg Reward: -130.754203196, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.2826461792\n",
      "[NOR] Episode: 34210, Length: 60, Avg Reward: -124.491562131, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 72.8695373535\n",
      "[NOR] Episode: 34220, Length: 77, Avg Reward: -142.546436781, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.37401378155\n",
      "[NOR] Episode: 34230, Length: 69, Avg Reward: -135.183588092, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 25.2025680542\n",
      "[NOR] Episode: 34240, Length: 91, Avg Reward: -154.793428389, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.0821529626846\n",
      "[NOR] Episode: 34250, Length: 98, Avg Reward: -156.057676394, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 251.626968384\n",
      "[NOR] Episode: 34260, Length: 80, Avg Reward: -139.534469033, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.69781017303\n",
      "[NOR] Episode: 34270, Length: 113, Avg Reward: -143.950401987, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.624641954899\n",
      "[NOR] Episode: 34280, Length: 85, Avg Reward: -131.826836145, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.45698308945\n",
      "[NOR] Episode: 34290, Length: 60, Avg Reward: -138.728802301, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 59.7733039856\n",
      "[NOR] Episode: 34300, Length: 90, Avg Reward: -142.940369581, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.979516983\n",
      "[NOR] Episode: 34310, Length: 79, Avg Reward: -138.450947917, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.25475883484\n",
      "[NOR] Episode: 34320, Length: 139, Avg Reward: -131.287741188, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.39694786072\n",
      "[NOR] Episode: 34330, Length: 77, Avg Reward: -125.927273641, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.71229171753\n",
      "[NOR] Episode: 34340, Length: 89, Avg Reward: -117.56541867, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.90117836\n",
      "[NOR] Episode: 34350, Length: 80, Avg Reward: -127.677548615, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.077703476\n",
      "[NOR] Episode: 34360, Length: 74, Avg Reward: -147.198841028, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.12032461166\n",
      "[NOR] Episode: 34370, Length: 113, Avg Reward: -150.497832614, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 23.1847839355\n",
      "[NOR] Episode: 34380, Length: 67, Avg Reward: -130.535283993, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 138.879852295\n",
      "[NOR] Episode: 34390, Length: 96, Avg Reward: -137.038110961, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 54.9883956909\n",
      "[NOR] Episode: 34400, Length: 60, Avg Reward: -142.025970998, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.21616840363\n",
      "[NOR] Episode: 34410, Length: 83, Avg Reward: -139.96963748, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -34.6109619141\n",
      "[NOR] Episode: 34420, Length: 66, Avg Reward: -132.604069194, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.26789689064\n",
      "[NOR] Episode: 34430, Length: 121, Avg Reward: -124.233716567, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.15482234955\n",
      "[NOR] Episode: 34440, Length: 110, Avg Reward: -118.082107275, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.432120263577\n",
      "[NOR] Episode: 34450, Length: 86, Avg Reward: -130.43801499, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 89.3343811035\n",
      "[NOR] Episode: 34460, Length: 96, Avg Reward: -134.298806856, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.18405723572\n",
      "[NOR] Episode: 34470, Length: 96, Avg Reward: -121.753362759, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.2029743195\n",
      "[NOR] Episode: 34480, Length: 103, Avg Reward: -121.468834146, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.47229242325\n",
      "[NOR] Episode: 34490, Length: 62, Avg Reward: -165.112299826, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.7673091888\n",
      "[NOR] Episode: 34500, Length: 91, Avg Reward: -136.968501423, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.6721663475\n",
      "[NOR] Episode: 34510, Length: 65, Avg Reward: -132.200553066, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 12.1922836304\n",
      "[NOR] Episode: 34520, Length: 97, Avg Reward: -144.010617417, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.15791702271\n",
      "[NOR] Episode: 34530, Length: 86, Avg Reward: -133.999944933, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.50995016098\n",
      "[NOR] Episode: 34540, Length: 69, Avg Reward: -144.346441528, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.7728614807\n",
      "[NOR] Episode: 34550, Length: 97, Avg Reward: -128.070330652, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 73.2956619263\n",
      "[NOR] Episode: 34560, Length: 57, Avg Reward: -136.467072412, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.22116422653\n",
      "[NOR] Episode: 34570, Length: 78, Avg Reward: -119.895702838, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.42611563206\n",
      "[NOR] Episode: 34580, Length: 102, Avg Reward: -133.314878648, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 35.2744750977\n",
      "[NOR] Episode: 34590, Length: 75, Avg Reward: -127.492920815, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -2.77794742584\n",
      "[NOR] Episode: 34600, Length: 70, Avg Reward: -128.338532505, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.48097991943\n",
      "[NOR] Episode: 34610, Length: 82, Avg Reward: -130.636778557, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.19143247604\n",
      "[NOR] Episode: 34620, Length: 72, Avg Reward: -118.664816657, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.1618013382\n",
      "[NOR] Episode: 34630, Length: 87, Avg Reward: -141.532776272, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.9965744019\n",
      "[NOR] Episode: 34640, Length: 119, Avg Reward: -130.409954556, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 122.211669922\n",
      "[NOR] Episode: 34650, Length: 74, Avg Reward: -129.759397365, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.0844841003\n",
      "[NOR] Episode: 34660, Length: 100, Avg Reward: -157.873791727, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.70473098755\n",
      "[NOR] Episode: 34670, Length: 92, Avg Reward: -154.432922732, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.03000307083\n",
      "[NOR] Episode: 34680, Length: 81, Avg Reward: -129.202244233, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.28490209579\n",
      "[NOR] Episode: 34690, Length: 103, Avg Reward: -123.046828419, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 123.902870178\n",
      "[NOR] Episode: 34700, Length: 130, Avg Reward: -134.785401782, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 178.66583252\n",
      "[NOR] Episode: 34710, Length: 96, Avg Reward: -154.310335718, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 49.22240448\n",
      "[NOR] Episode: 34720, Length: 85, Avg Reward: -135.502241659, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.7839546204\n",
      "[NOR] Episode: 34730, Length: 60, Avg Reward: -138.454060933, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.55736994743\n",
      "[NOR] Episode: 34740, Length: 94, Avg Reward: -145.356096324, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.47074794769\n",
      "[NOR] Episode: 34750, Length: 66, Avg Reward: -136.255536762, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 103.128707886\n",
      "[NOR] Episode: 34760, Length: 76, Avg Reward: -168.422024353, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 39.0600814819\n",
      "[NOR] Episode: 34770, Length: 128, Avg Reward: -155.295824796, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.44957685471\n",
      "[NOR] Episode: 34780, Length: 89, Avg Reward: -168.054213472, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.33809232712\n",
      "[NOR] Episode: 34790, Length: 143, Avg Reward: -160.75675074, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.393196106\n",
      "[NOR] Episode: 34800, Length: 129, Avg Reward: -158.228520784, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.81026697159\n",
      "[NOR] Episode: 34810, Length: 96, Avg Reward: -174.387209537, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.52690982819\n",
      "[NOR] Episode: 34820, Length: 133, Avg Reward: -162.535781319, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.58278512955\n",
      "[NOR] Episode: 34830, Length: 197, Avg Reward: -204.478074604, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 41.3322029114\n",
      "[NOR] Episode: 34840, Length: 222, Avg Reward: -207.526910703, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 37.6075706482\n",
      "[NOR] Episode: 34850, Length: 198, Avg Reward: -232.17145072, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 247.003036499\n",
      "[NOR] Episode: 34860, Length: 435, Avg Reward: -220.838380045, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 18.8593788147\n",
      "[NOR] Episode: 34870, Length: 122, Avg Reward: -253.216482289, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.11107933521\n",
      "[NOR] Episode: 34880, Length: 132, Avg Reward: -256.565779403, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.7824845314\n",
      "[NOR] Episode: 34890, Length: 95, Avg Reward: -219.988039162, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.82899153233\n",
      "[NOR] Episode: 34900, Length: 89, Avg Reward: -243.954043874, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.662738859653\n",
      "[NOR] Episode: 34910, Length: 86, Avg Reward: -345.655347408, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.60254669189\n",
      "[NOR] Episode: 34920, Length: 152, Avg Reward: -325.358802972, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -4.45089864731\n",
      "[NOR] Episode: 34930, Length: 205, Avg Reward: -298.755875603, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.795293331146\n",
      "[NOR] Episode: 34940, Length: 121, Avg Reward: -257.072145167, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.01775455475\n",
      "[NOR] Episode: 34950, Length: 93, Avg Reward: -296.248469866, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.06884050369\n",
      "[NOR] Episode: 34960, Length: 105, Avg Reward: -149.349843666, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.78717136383\n",
      "[NOR] Episode: 34970, Length: 150, Avg Reward: -93.2166844572, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 86.1562652588\n",
      "[NOR] Episode: 34980, Length: 134, Avg Reward: -84.4807525424, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.53421545029\n",
      "[NOR] Episode: 34990, Length: 158, Avg Reward: -88.5571276893, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.15330934525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 10:08:25,640] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video035000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 35000, Length: 132, Avg Reward: -88.7955336749, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.37106800079\n",
      "[NOR] Episode: 35010, Length: 275, Avg Reward: -114.089726342, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 176.16784668\n",
      "[NOR] Episode: 35020, Length: 123, Avg Reward: -111.307636352, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.04912829399\n",
      "[NOR] Episode: 35030, Length: 207, Avg Reward: -89.649277904, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.23860836029\n",
      "[NOR] Episode: 35040, Length: 118, Avg Reward: -98.7779294848, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.34817767143\n",
      "[NOR] Episode: 35050, Length: 216, Avg Reward: -94.9367386668, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.0819587708\n",
      "[NOR] Episode: 35060, Length: 205, Avg Reward: -78.1378979216, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -3.94248795509\n",
      "[NOR] Episode: 35070, Length: 130, Avg Reward: -60.8689523812, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 103.975692749\n",
      "[NOR] Episode: 35080, Length: 125, Avg Reward: -86.9475288486, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.0255355835\n",
      "[NOR] Episode: 35090, Length: 119, Avg Reward: -79.7471179102, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.86293387413\n",
      "[NOR] Episode: 35100, Length: 179, Avg Reward: -86.0464782748, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.45715475082\n",
      "[NOR] Episode: 35110, Length: 125, Avg Reward: -83.4281318072, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.00313472748\n",
      "[NOR] Episode: 35120, Length: 137, Avg Reward: -63.6032179852, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.08095264435\n",
      "[NOR] Episode: 35130, Length: 170, Avg Reward: -47.8608259881, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 199.390640259\n",
      "[NOR] Episode: 35140, Length: 102, Avg Reward: -36.6368131088, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.16332834959\n",
      "[NOR] Episode: 35150, Length: 113, Avg Reward: -79.9155932649, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.59482288361\n",
      "[NOR] Episode: 35160, Length: 210, Avg Reward: -74.5665379564, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.0811862946\n",
      "[NOR] Episode: 35170, Length: 341, Avg Reward: -75.1196196672, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.60856389999\n",
      "[NOR] Episode: 35180, Length: 134, Avg Reward: -32.7416219271, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.98812496662\n",
      "[NOR] Episode: 35190, Length: 604, Avg Reward: -35.856294474, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.70640850067\n",
      "[NOR] Episode: 35200, Length: 205, Avg Reward: -59.5581910993, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 149.049499512\n",
      "[NOR] Episode: 35210, Length: 230, Avg Reward: -67.5217017529, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 19.4510650635\n",
      "[NOR] Episode: 35220, Length: 376, Avg Reward: -74.7436650682, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.941250264645\n",
      "[NOR] Episode: 35230, Length: 215, Avg Reward: -103.738750532, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.0243597031\n",
      "[NOR] Episode: 35240, Length: 534, Avg Reward: -16.2764151363, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.18490314484\n",
      "[NOR] Episode: 35250, Length: 1000, Avg Reward: -31.5627169324, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.6679289341\n",
      "[NOR] Episode: 35260, Length: 185, Avg Reward: -68.1781024334, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.77063512802\n",
      "[NOR] Episode: 35270, Length: 1000, Avg Reward: 30.8649121455, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 34.1061859131\n",
      "[NOR] Episode: 35280, Length: 239, Avg Reward: -62.6813274182, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.21219944954\n",
      "[NOR] Episode: 35290, Length: 645, Avg Reward: -40.1582814615, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.27962970734\n",
      "[NOR] Episode: 35300, Length: 568, Avg Reward: -97.6763895437, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.732402205467\n",
      "[NOR] Episode: 35310, Length: 401, Avg Reward: -61.9643891308, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 69.4053192139\n",
      "[NOR] Episode: 35320, Length: 500, Avg Reward: -94.5524119072, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.43495297432\n",
      "[NOR] Episode: 35330, Length: 931, Avg Reward: -104.445144462, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.626434922218\n",
      "[NOR] Episode: 35340, Length: 725, Avg Reward: -72.6020272632, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.24679613113\n",
      "[NOR] Episode: 35350, Length: 259, Avg Reward: -67.1289717324, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.10775089264\n",
      "[NOR] Episode: 35360, Length: 270, Avg Reward: -117.504733703, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.64092063904\n",
      "[NOR] Episode: 35370, Length: 1000, Avg Reward: -105.70113523, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.987241268158\n",
      "[NOR] Episode: 35380, Length: 309, Avg Reward: -85.3007087023, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.24129164219\n",
      "[NOR] Episode: 35390, Length: 214, Avg Reward: -149.908291846, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.5970659256\n",
      "[NOR] Episode: 35400, Length: 194, Avg Reward: -114.342954151, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 36.0696563721\n",
      "[NOR] Episode: 35410, Length: 280, Avg Reward: -84.8149986688, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.9244453907\n",
      "[NOR] Episode: 35420, Length: 284, Avg Reward: -124.773765327, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.48964500427\n",
      "[NOR] Episode: 35430, Length: 128, Avg Reward: -93.6864458405, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.29931163788\n",
      "[NOR] Episode: 35440, Length: 274, Avg Reward: -102.982329528, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 63.1859512329\n",
      "[NOR] Episode: 35450, Length: 227, Avg Reward: -150.742622989, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.39358258247\n",
      "[NOR] Episode: 35460, Length: 220, Avg Reward: -154.632787357, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 110.071289062\n",
      "[NOR] Episode: 35470, Length: 213, Avg Reward: -60.9556360396, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.639443695545\n",
      "[NOR] Episode: 35480, Length: 1000, Avg Reward: -82.7645211734, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.934513092\n",
      "[NOR] Episode: 35490, Length: 320, Avg Reward: -101.080298327, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.712857067585\n",
      "[NOR] Episode: 35500, Length: 447, Avg Reward: -45.4010150742, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -9.10599899292\n",
      "[NOR] Episode: 35510, Length: 504, Avg Reward: -45.7546996055, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.9425525665\n",
      "[NOR] Episode: 35520, Length: 185, Avg Reward: -93.9811483203, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -1.62538576126\n",
      "[NOR] Episode: 35530, Length: 175, Avg Reward: -61.8565863603, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.2285041809\n",
      "[NOR] Episode: 35540, Length: 233, Avg Reward: -71.9553313917, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.94040775299\n",
      "[NOR] Episode: 35550, Length: 1000, Avg Reward: -66.6825999173, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 114.242797852\n",
      "[NOR] Episode: 35560, Length: 270, Avg Reward: -91.6563096943, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 250.61932373\n",
      "[NOR] Episode: 35570, Length: 158, Avg Reward: -53.3486750395, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 82.8948669434\n",
      "[NOR] Episode: 35580, Length: 156, Avg Reward: -74.1241145026, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.62167167664\n",
      "[NOR] Episode: 35590, Length: 108, Avg Reward: -70.5175797359, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.11395645142\n",
      "[NOR] Episode: 35600, Length: 118, Avg Reward: -109.332682866, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.53701257706\n",
      "[NOR] Episode: 35610, Length: 129, Avg Reward: -82.0669460029, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.61283302307\n",
      "[NOR] Episode: 35620, Length: 370, Avg Reward: -96.7631762888, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.2228717804\n",
      "[NOR] Episode: 35630, Length: 144, Avg Reward: -47.0612386375, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.17713332176\n",
      "[NOR] Episode: 35640, Length: 147, Avg Reward: -64.4291943071, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.1758852005\n",
      "[NOR] Episode: 35650, Length: 148, Avg Reward: -42.6827934564, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.792050242424\n",
      "[NOR] Episode: 35660, Length: 101, Avg Reward: -68.1744787611, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.26079893112\n",
      "[NOR] Episode: 35670, Length: 201, Avg Reward: -84.0431779995, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.8272352219\n",
      "[NOR] Episode: 35680, Length: 100, Avg Reward: -67.3231917223, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.45640087128\n",
      "[NOR] Episode: 35690, Length: 162, Avg Reward: -26.1231939904, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.148275494576\n",
      "[NOR] Episode: 35700, Length: 153, Avg Reward: -61.1946342836, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.043598175\n",
      "[NOR] Episode: 35710, Length: 140, Avg Reward: -80.5108334008, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 22.9846134186\n",
      "[NOR] Episode: 35720, Length: 131, Avg Reward: -54.3727139366, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 122.965454102\n",
      "[NOR] Episode: 35730, Length: 172, Avg Reward: -60.0315538183, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.17848396301\n",
      "[NOR] Episode: 35740, Length: 564, Avg Reward: -38.0575024485, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.85684585571\n",
      "[NOR] Episode: 35750, Length: 129, Avg Reward: -46.400388959, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.041387558\n",
      "[NOR] Episode: 35760, Length: 150, Avg Reward: -73.4837901359, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.1783800125\n",
      "[NOR] Episode: 35770, Length: 116, Avg Reward: -97.4094971794, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.954465985298\n",
      "[NOR] Episode: 35780, Length: 202, Avg Reward: -59.5416837635, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.5625190735\n",
      "[NOR] Episode: 35790, Length: 168, Avg Reward: -86.2827739967, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.02946710587\n",
      "[NOR] Episode: 35800, Length: 174, Avg Reward: -88.8019575431, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.75049591064\n",
      "[NOR] Episode: 35810, Length: 138, Avg Reward: -26.1538830607, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 6.67683601379\n",
      "[NOR] Episode: 35820, Length: 185, Avg Reward: -88.0721042112, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.8861579895\n",
      "[NOR] Episode: 35830, Length: 1000, Avg Reward: -48.7476402988, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 141.659469604\n",
      "[NOR] Episode: 35840, Length: 156, Avg Reward: -84.2962693534, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 21.2335319519\n",
      "[NOR] Episode: 35850, Length: 159, Avg Reward: -103.979602492, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 241.721969604\n",
      "[NOR] Episode: 35860, Length: 165, Avg Reward: -85.512107928, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 30.5187664032\n",
      "[NOR] Episode: 35870, Length: 176, Avg Reward: -124.419286283, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 254.338043213\n",
      "[NOR] Episode: 35880, Length: 1000, Avg Reward: -71.1218856916, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.155315816402\n",
      "[NOR] Episode: 35890, Length: 618, Avg Reward: -42.250993864, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.32742500305\n",
      "[NOR] Episode: 35900, Length: 160, Avg Reward: -20.0833778086, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.19195652008\n",
      "[NOR] Episode: 35910, Length: 206, Avg Reward: -69.518903864, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.11957645416\n",
      "[NOR] Episode: 35920, Length: 91, Avg Reward: -70.1315831572, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 36.5601654053\n",
      "[NOR] Episode: 35930, Length: 191, Avg Reward: -83.1640687344, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.29703360796\n",
      "[NOR] Episode: 35940, Length: 382, Avg Reward: -2.15020098508, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.239681720734\n",
      "[NOR] Episode: 35950, Length: 186, Avg Reward: -55.4416941359, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.28023529053\n",
      "[NOR] Episode: 35960, Length: 442, Avg Reward: -2.8545695186, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.3507976532\n",
      "[NOR] Episode: 35970, Length: 158, Avg Reward: -15.012591659, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 27.7607345581\n",
      "[NOR] Episode: 35980, Length: 156, Avg Reward: 43.3923763708, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.84844207764\n",
      "[NOR] Episode: 35990, Length: 943, Avg Reward: 23.7169959319, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 16.7129878998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 10:26:48,526] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/reward-task/monitor/21/openaigym.video.21.20184.video036000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 36000, Length: 130, Avg Reward: -99.1670860506, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 9.17866325378\n",
      "[NOR] Episode: 36010, Length: 176, Avg Reward: -25.3429787403, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.23073768616\n",
      "[NOR] Episode: 36020, Length: 324, Avg Reward: 14.228374527, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.90224552155\n",
      "[NOR] Episode: 36030, Length: 173, Avg Reward: -0.0574611458138, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 148.171020508\n",
      "[NOR] Episode: 36040, Length: 363, Avg Reward: 36.2952381732, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 8.69233036041\n",
      "[NOR] Episode: 36050, Length: 235, Avg Reward: -63.2415639042, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.37870121\n",
      "[NOR] Episode: 36060, Length: 1000, Avg Reward: -26.4817633636, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 15.0110015869\n",
      "[NOR] Episode: 36070, Length: 299, Avg Reward: 55.4505006636, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.91641139984\n",
      "[NOR] Episode: 36080, Length: 127, Avg Reward: -21.7048210888, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.98001813889\n",
      "[NOR] Episode: 36090, Length: 229, Avg Reward: 18.6382381459, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 1.21119892597\n",
      "[NOR] Episode: 36100, Length: 104, Avg Reward: -17.0515393188, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 2.61888694763\n",
      "[NOR] Episode: 36110, Length: 246, Avg Reward: 49.6643651983, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.0508183240891\n",
      "[NOR] Episode: 36120, Length: 307, Avg Reward: -3.13755877056, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 7.59494686127\n",
      "[NOR] Episode: 36130, Length: 117, Avg Reward: -21.7687390242, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 226.483108521\n",
      "[NOR] Episode: 36140, Length: 228, Avg Reward: -0.55848788836, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 4.27558803558\n",
      "[NOR] Episode: 36150, Length: 96, Avg Reward: -59.3867859819, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -0.831950187683\n",
      "[NOR] Episode: 36160, Length: 117, Avg Reward: -5.77592736713, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 53.5132789612\n",
      "[NOR] Episode: 36170, Length: 197, Avg Reward: 10.6550114786, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.64579057693\n",
      "[NOR] Episode: 36180, Length: 270, Avg Reward: -15.2381719751, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.69344425201\n",
      "[NOR] Episode: 36190, Length: 743, Avg Reward: 8.64397013734, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 10.7212562561\n",
      "[NOR] Episode: 36200, Length: 255, Avg Reward: 6.01174394844, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.26589441299\n",
      "[NOR] Episode: 36210, Length: 285, Avg Reward: 85.2527910796, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 13.2966003418\n",
      "[NOR] Episode: 36220, Length: 379, Avg Reward: 136.136086733, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 14.7260446548\n",
      "[NOR] Episode: 36230, Length: 291, Avg Reward: 59.0921128816, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.09134817123\n",
      "[NOR] Episode: 36240, Length: 337, Avg Reward: -23.425410487, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.75678396225\n",
      "[NOR] Episode: 36250, Length: 841, Avg Reward: 51.2487377165, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 5.21962070465\n",
      "[NOR] Episode: 36260, Length: 1000, Avg Reward: 32.812938634, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 11.9767894745\n",
      "[NOR] Episode: 36270, Length: 1000, Avg Reward: -88.0523456163, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 30.3789710999\n",
      "[NOR] Episode: 36280, Length: 1000, Avg Reward: -109.02624928, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 3.38141727448\n",
      "[NOR] Episode: 36290, Length: 1000, Avg Reward: -89.9230171593, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 0.609552741051\n",
      "[NOR] Episode: 36300, Length: 123, Avg Reward: -84.0511428623, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: 17.7960624695\n",
      "[NOR] Episode: 36310, Length: 1000, Avg Reward: -77.7119558104, e: 0.05, Learning Rate: 0.01, buffer_len: 500000\n",
      "Loss: -5.5770368576\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-8b4226ca4c2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcoconut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__coconut__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40000.\u001b[0m  \u001b[0;31m# line 1: k = 40000.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episode_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterp1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_target_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# line 2: model.fit(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-06687c429e06>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, keep_prob, e, learning_rate, print_step, update_target_step, episodes, max_episode_length, batch_size)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# line 81:                 a = self.predict(s, e = _e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# line 82:                 s1, r, done, info = env.step(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0mr_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m  \u001b[0;31m# line 83:                 r_total += r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mep_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m  \u001b[0;31m# line 84:                 ep_reward += r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/data/cristian/tfinterface/tfinterface/reinforcement/expanded_state_env.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhorizontal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/wrappers/monitoring.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/wrappers/time_limit.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyLinearImpulse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpulse_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36mBeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mcontactListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mBeginContact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 40000.\n",
    "model.fit(\n",
    "    env, print_step=10, \n",
    "    episodes=int(1e5), max_episode_length=10000, batch_size=32,\n",
    "    learning_rate = 0.01, # lambda t: 0.05 * k / (k + t)\n",
    "    e = interp1d([0, 300000], [0.4, 0.05], fill_value=0.05, bounds_error=False),\n",
    "    keep_prob = 0.5,\n",
    "    update_target_step = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 10:41:45,516] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243.797470956\n",
      "-32.8488014114\n",
      "219.353111093\n",
      "30.568509689\n",
      "176.156562556\n",
      "-38.8324403062\n",
      "114.753748804\n",
      "218.049416314\n",
      "239.012542657\n",
      "-21.1754624372\n",
      "204.824257629\n",
      "221.1510406\n",
      "233.165809066\n",
      "190.769837155\n",
      "226.111052744\n",
      "232.375507618\n",
      "246.158753518\n",
      "201.513484877\n",
      "260.02294592\n",
      "202.344976616\n",
      "225.880992423\n",
      "-9.64755026126\n",
      "-6.03692961905\n",
      "234.001375633\n",
      "209.467160545\n",
      "87.5170965186\n",
      "-28.0624531548\n",
      "251.780547564\n",
      "6.11093998915\n",
      "234.962878443\n",
      "243.457889913\n",
      "217.791354451\n",
      "231.393275445\n",
      "242.334538746\n",
      "233.551304121\n",
      "201.214691061\n",
      "184.507148375\n",
      "176.875103386\n",
      "203.221645247\n",
      "206.039553326\n",
      "228.494334658\n",
      "183.623170261\n",
      "233.926858399\n",
      "233.831669087\n",
      "183.859952167\n",
      "220.885013382\n",
      "229.316361555\n",
      "247.971327615\n",
      "225.934850965\n",
      "250.040910851\n",
      "111.452411384\n",
      "214.949280811\n",
      "114.053585001\n",
      "255.138792085\n",
      "130.970860274\n",
      "223.51669256\n",
      "-12.0294004791\n",
      "259.521591465\n",
      "245.417253086\n",
      "95.523710089\n",
      "201.21619177\n",
      "195.542195408\n",
      "261.715082604\n",
      "237.888645878\n",
      "216.372327063\n",
      "154.560204699\n",
      "234.574622526\n",
      "226.822058493\n",
      "230.557572324\n",
      "210.545263243\n",
      "-19.5257988617\n",
      "245.677615795\n",
      "235.218379635\n",
      "-51.5268928117\n",
      "256.153950838\n",
      "192.467943834\n",
      "255.041207821\n",
      "189.544641109\n",
      "229.740225993\n",
      "217.723790545\n",
      "225.865116975\n",
      "203.086340541\n",
      "228.619039665\n",
      "161.649911023\n",
      "151.696836273\n",
      "-9.70853073756\n",
      "223.557381133\n",
      "235.18404056\n",
      "238.353438659\n",
      "-9.97769998946\n",
      "231.876334748\n",
      "236.134019174\n",
      "240.126965695\n",
      "232.395970114\n",
      "187.008914982\n",
      "109.76120705\n",
      "-58.1153534808\n",
      "239.877052239\n",
      "239.342227791\n",
      "208.946912736\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/models/21.272.691146069\".format(path = os.getcwd(), name = name)\n",
    "logs_path = \"{path}/logs/\".format(path = os.getcwd(), name = name)\n",
    "\n",
    "\n",
    "model_run = LunarLander(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path,\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total = 0.\n",
    "    ep = 0\n",
    "    while not done and ep < 700:\n",
    "        ep += 1\n",
    "        a = model_run.predict(s, 0.0)\n",
    "        s, r, done, info = env.step(a)\n",
    "        total += r\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "    \n",
    "    print(total)\n",
    "    \n",
    "env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coconut",
   "language": "coconut",
   "name": "coconut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3.6
   },
   "file_extension": ".coco",
   "mimetype": "text/x-python3",
   "name": "coconut",
   "pygments_lexer": "coconut"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
