{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tfinterface.model_base import ModelBase\n",
    "from tfinterface.reinforcement import ExperienceReplay\n",
    "from tfinterface.utils import select_columns, soft_if, get_run\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from tfinterface.reinforcement import ExpandedStateEnv\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "name = \"actor-critic-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def shifted_log_loss(x, alfa=0.05):\n",
    "    return - tf.log(x + alfa * (1.0 - x))\n",
    "\n",
    "class Inputs(object):\n",
    "    def __init__(self, n_states, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.episode_length = tf.placeholder(tf.int64, [], name='episode_length')\n",
    "\n",
    "            self.s = tf.placeholder(tf.float32, [None, n_states], name='s')\n",
    "            self.a = tf.placeholder(tf.int32, [None], name='a')\n",
    "            self.r = tf.placeholder(tf.float32, [None], name='r')\n",
    "            self.v1 = tf.placeholder(tf.float32, [None], name='V1')\n",
    "            self.done = tf.placeholder(tf.float32, [None], name='done')\n",
    "            \n",
    "            self.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            self.keep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\n",
    "            self.training = tf.placeholder(tf.bool, [], name='training')\n",
    "            \n",
    "            self.pi = tf.placeholder(tf.float32, [], name='pi')\n",
    "            \n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, base_model, inputs, n_actions, n_states, y, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            \n",
    "            ops = dict(\n",
    "                trainable=True,\n",
    "                kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01)\n",
    "            )\n",
    "\n",
    "            net = inputs.s\n",
    "\n",
    "            net = tf.layers.dense(net, 64, activation=tf.nn.relu, name=\"relu_layer\", **ops)        \n",
    "            \n",
    "            self.V = tf.layers.dense(net, n_actions, name='V', **ops)[:, 0]\n",
    "\n",
    "            self.target = soft_if(inputs.done, inputs.r,  inputs.r + y * inputs.v1)\n",
    "\n",
    "            self.error = self.target - self.V\n",
    "            self.loss = Pipe(self.error, tf.nn.l2_loss, tf.reduce_mean)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "            self.update = tf.train.AdamOptimizer(inputs.learning_rate).minimize(self.loss, var_list=self.variables)\n",
    "\n",
    "            avg_error, std_error = tf.nn.moments(self.error, [0])\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('avg_target', tf.reduce_mean(self.target)),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.scalar('avg_error', avg_error),\n",
    "                tf.summary.scalar('std_error', std_error),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n",
    "class Actor(object):\n",
    "    def __init__(self, base_model, inputs, critic, n_actions, n_states, y, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            ops = dict(\n",
    "                trainable=True,\n",
    "                kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01)\n",
    "            )\n",
    "\n",
    "            net = inputs.s\n",
    "\n",
    "            net = tf.layers.dense(net, 128, activation=tf.nn.relu, name=\"relu_layer\", use_bias=True, **ops)\n",
    "            net = tf.nn.dropout(net, inputs.keep_prob)\n",
    "            net = tf.layers.dense(net, 64, activation=tf.nn.relu, name=\"relu_layer2\", use_bias=True, **ops)\n",
    "            net = tf.nn.dropout(net, inputs.keep_prob)\n",
    "\n",
    "            \n",
    "            self.logits = tf.layers.dense(net, n_actions, name='P', use_bias=False, **ops)\n",
    "            self.P = tf.nn.softmax(self.logits)\n",
    "            \n",
    "            self.Pa = select_columns(self.P, inputs.a)\n",
    "\n",
    "#             self.loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "#                 logits = self.logits, \n",
    "#                 labels = tf.one_hot(inputs.a, n_actions)\n",
    "#             )\n",
    "#             print self.loss\n",
    "#             self.loss = select_columns(self.loss, inputs.a)\n",
    "            self.loss = shifted_log_loss(self.Pa) * critic.error\n",
    "            self.loss = tf.reduce_mean(self.loss)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "            self.update = tf.train.AdamOptimizer(inputs.learning_rate).minimize(self.loss, var_list=self.variables)\n",
    "\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LunarLander(ModelBase):\n",
    "    \n",
    "    def define_model(self, n_actions, n_states, y=0.98, buffer_length=50000, pi=0.1):\n",
    "        self.global_max = float('-inf')\n",
    "\n",
    "        self.replay_buffer = ExperienceReplay(max_length=buffer_length)\n",
    "\n",
    "\n",
    "        with self.graph.as_default(), tf.device(\"cpu:0\"):\n",
    "\n",
    "            self.inputs = Inputs(n_states, \"inputs\")\n",
    "\n",
    "            self.critic = Critic(self, self.inputs, n_actions, n_states, y, \"critic\")\n",
    "            self.target_critic = Critic(self, self.inputs, n_actions, n_states, y, \"target_critic\")\n",
    "            self.actor = Actor(self, self.inputs, self.target_critic, n_actions, n_states, y, \"actor\")\n",
    "\n",
    "            self.update = tf.group(self.critic.update, self.actor.update)\n",
    "\n",
    "            self.episode_length_summary = tf.summary.scalar('episode_length', self.inputs.episode_length)\n",
    "\n",
    "            self.summaries = tf.summary.merge([self.actor.summaries, self.critic.summaries, self.target_critic.summaries])\n",
    "\n",
    "            self.update_target = tf.group(*[\n",
    "                t.assign_add(pi * (a - t)) for t, a in zip(self.target_critic.variables, self.critic.variables)\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    def predict_feed(self, S):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.keep_prob: 1.0,\n",
    "            self.inputs.training: False\n",
    "        }\n",
    "    \n",
    "    def predict(self, state, e = 0.0):\n",
    "        predict_feed = self.predict_feed([state])\n",
    "        actions = self.sess.run(self.actor.P, feed_dict=predict_feed)\n",
    "        actions = actions[0]\n",
    "        n = len(actions)\n",
    "\n",
    "        if random.random() < e:\n",
    "            return random.randint(0, n-1)\n",
    "        else:\n",
    "            return np.random.choice(n, p=actions)\n",
    "    \n",
    "    def fit_feed(self, S, A, R, V1, Done, learning_rate, keep_prob):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.a: A,\n",
    "            self.inputs.r: R,\n",
    "            self.inputs.v1: V1,\n",
    "            self.inputs.done: Done,\n",
    "            self.inputs.learning_rate: learning_rate,\n",
    "            self.inputs.keep_prob: keep_prob,\n",
    "            self.inputs.training: True\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def fit(self, env, keep_prob=0.5, e=0.01, learning_rate=0.01, print_step=10, \n",
    "            update_target_step = 32, episodes=100000, max_episode_length=float('inf'), batch_size=32):\n",
    "        \n",
    "        r_total = 0.\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            ep_step = 0\n",
    "            s = env.reset()\n",
    "            episode_length = 0\n",
    "            ep_reward = 0.\n",
    "            \n",
    "            while not done and ep_step <= max_episode_length:\n",
    "                self.global_step += 1\n",
    "                episode_length += 1\n",
    "                ep_step += 1\n",
    "                \n",
    "                \n",
    "                _learning_rate = learning_rate(self.global_step) if hasattr(learning_rate, '__call__') else learning_rate\n",
    "                _e = e(self.global_step) if hasattr(e, '__call__') else e\n",
    "                \n",
    "                \n",
    "                a = self.predict(s, e = _e)\n",
    "                s1, r, done, info = env.step(a)\n",
    "                r_total += r\n",
    "                ep_reward += r\n",
    "                \n",
    "                \n",
    "                self.replay_buffer.append((s, a, r, s1, float(done)))\n",
    "                \n",
    "                \n",
    "                S, A, R, S1, Done = self.replay_buffer.random_batch(batch_size).unzip()\n",
    "                predict_feed = self.predict_feed(S1)\n",
    "                V1 = self.sess.run(self.target_critic.V, feed_dict=predict_feed)\n",
    "\n",
    "                \n",
    "                fit_feed = self.fit_feed(S, A, R, V1, Done, _learning_rate, keep_prob)\n",
    "                _, summaries = self.sess.run([self.update, self.summaries], feed_dict=fit_feed)\n",
    "                self.writer.add_summary(summaries, self.global_step)\n",
    "                \n",
    "                \n",
    "                if self.global_step % update_target_step == 0:\n",
    "                    self.sess.run(self.update_target)\n",
    "                \n",
    "                \n",
    "                s = s1\n",
    "                \n",
    "            \n",
    "            episode_length_summary = self.sess.run(self.episode_length_summary,\n",
    "                                                   feed_dict={self.inputs.episode_length: episode_length})\n",
    "            self.writer.add_summary(episode_length_summary, self.global_step)\n",
    "\n",
    "\n",
    "            if ep_reward >= self.global_max:\n",
    "                print(\"[MAX] Episode: {}, Length: {}, Reward: {}, buffer_len: {}\".format(episode, episode_length, ep_reward, len(self.replay_buffer)))\n",
    "                self.save(model_path = self.model_path + \".{score}\".format(score = ep_reward))\n",
    "                self.global_max = ep_reward\n",
    "\n",
    "\n",
    "            if episode % print_step == 0 and episode > 0:\n",
    "                avg_r = r_total / print_step\n",
    "                actor_loss = self.sess.run(self.actor.loss, feed_dict=fit_feed)\n",
    "                print(\"[NOR] Episode: {}, Length: {}, Avg Reward: {}, e: {}, Learning Rate: {}, buffer_len: {}\".format(episode, episode_length, avg_r, _e, _learning_rate, len(self.replay_buffer)))\n",
    "                print(\"Loss: {}\".format(actor_loss))\n",
    "                self.save()\n",
    "                r_total = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 01:13:00,377] Making new env: LunarLander-v2\n",
      "[2017-03-23 01:13:00,379] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-23 01:13:00,380] Creating monitor directory monitor/24\n"
     ]
    }
   ],
   "source": [
    "run = get_run()\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = wrappers.Monitor(env, \"monitor/{run}\".format(run = run))\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/models/{run}\".format(path = os.getcwd(), run = run)\n",
    "logs_path = \"{path}/logs/{run}\".format(path = os.getcwd(), run = run)\n",
    "\n",
    "\n",
    "model = LunarLander(\n",
    "    n_actions, n_states, y=0.9999, \n",
    "    buffer_length=500000,\n",
    "    model_path = model_path,\n",
    "    logs_path = logs_path,\n",
    "    restore = False,\n",
    "    pi = 0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 01:13:01,454] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video000000.mp4\n",
      "[2017-03-23 01:13:08,466] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 0, Length: 276, Reward: -110.734014186, buffer_len: 276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 01:13:13,542] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 10, Length: 102, Avg Reward: -485.148865628, e: 0.397859166667, Learning Rate: 0.002, buffer_len: 1836\n",
      "Loss: -6.88629245758\n",
      "[MAX] Episode: 14, Length: 1000, Reward: -42.7222007203, buffer_len: 3177\n",
      "[MAX] Episode: 16, Length: 1000, Reward: 61.3215828042, buffer_len: 4293\n",
      "[NOR] Episode: 20, Length: 287, Avg Reward: -144.908075334, e: 0.3928215, Learning Rate: 0.002, buffer_len: 6154\n",
      "Loss: -4.51028299332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 01:13:41,989] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 30, Length: 1000, Avg Reward: -93.2477725664, e: 0.387566833333, Learning Rate: 0.002, buffer_len: 10658\n",
      "Loss: -5.41270160675\n",
      "[NOR] Episode: 40, Length: 1000, Avg Reward: -117.776512161, e: 0.383902333333, Learning Rate: 0.002, buffer_len: 13799\n",
      "Loss: -1.59926497936\n",
      "[NOR] Episode: 50, Length: 263, Avg Reward: -106.539652408, e: 0.380281, Learning Rate: 0.002, buffer_len: 16903\n",
      "Loss: -3.91002941132\n",
      "[NOR] Episode: 60, Length: 141, Avg Reward: -41.2669486018, e: 0.371097, Learning Rate: 0.002, buffer_len: 24775\n",
      "Loss: -0.949034571648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 01:14:56,724] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 69, Length: 1000, Reward: 70.6373755308, buffer_len: 29620\n",
      "[NOR] Episode: 70, Length: 332, Avg Reward: -101.721203357, e: 0.365057166667, Learning Rate: 0.002, buffer_len: 29952\n",
      "Loss: 0.498363405466\n",
      "[MAX] Episode: 79, Length: 1000, Reward: 73.8736000134, buffer_len: 33553\n",
      "[NOR] Episode: 80, Length: 1000, Avg Reward: -49.1995556742, e: 0.359689333333, Learning Rate: 0.002, buffer_len: 34553\n",
      "Loss: -1.63813722134\n",
      "[MAX] Episode: 85, Length: 1000, Reward: 105.662860026, buffer_len: 37160\n",
      "[NOR] Episode: 90, Length: 282, Avg Reward: -64.4683891573, e: 0.355449666667, Learning Rate: 0.002, buffer_len: 38187\n",
      "Loss: -8.08340740204\n",
      "[NOR] Episode: 100, Length: 338, Avg Reward: -71.0201901791, e: 0.350092333333, Learning Rate: 0.002, buffer_len: 42779\n",
      "Loss: -2.569617033\n",
      "[NOR] Episode: 110, Length: 1000, Avg Reward: -30.4682065516, e: 0.342908, Learning Rate: 0.002, buffer_len: 48937\n",
      "Loss: -1.32207274437\n",
      "[NOR] Episode: 120, Length: 1000, Avg Reward: -21.9435946196, e: 0.336949833333, Learning Rate: 0.002, buffer_len: 54044\n",
      "Loss: -1.55298733711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 01:16:54,061] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 130, Length: 384, Avg Reward: -14.5336454289, e: 0.327503333333, Learning Rate: 0.002, buffer_len: 62141\n",
      "Loss: 4.77951622009\n",
      "[NOR] Episode: 140, Length: 1000, Avg Reward: -21.9957870161, e: 0.317149166667, Learning Rate: 0.002, buffer_len: 71016\n",
      "Loss: -0.397850781679\n",
      "[NOR] Episode: 150, Length: 1000, Avg Reward: -13.3092830504, e: 0.307553333333, Learning Rate: 0.002, buffer_len: 79241\n",
      "Loss: -2.29038476944\n",
      "[MAX] Episode: 153, Length: 990, Reward: 117.899109947, buffer_len: 82118\n",
      "[NOR] Episode: 160, Length: 1000, Avg Reward: 22.5284431271, e: 0.296477, Learning Rate: 0.002, buffer_len: 88735\n",
      "Loss: 0.0906465649605\n",
      "[NOR] Episode: 170, Length: 1000, Avg Reward: -2.01178325789, e: 0.287490166667, Learning Rate: 0.002, buffer_len: 96438\n",
      "Loss: -2.67467069626\n",
      "[NOR] Episode: 180, Length: 98, Avg Reward: 2.15837839238, e: 0.2779095, Learning Rate: 0.002, buffer_len: 104650\n",
      "Loss: -2.7666156292\n",
      "[MAX] Episode: 184, Length: 534, Reward: 206.82874657, buffer_len: 108184\n",
      "[NOR] Episode: 190, Length: 1000, Avg Reward: 38.6617262975, e: 0.2677315, Learning Rate: 0.002, buffer_len: 113374\n",
      "Loss: -1.64189291\n",
      "[NOR] Episode: 200, Length: 1000, Avg Reward: 52.3616626749, e: 0.256950333333, Learning Rate: 0.002, buffer_len: 122615\n",
      "Loss: -1.84048759937\n",
      "[MAX] Episode: 208, Length: 527, Reward: 220.546465703, buffer_len: 128355\n",
      "[NOR] Episode: 210, Length: 501, Avg Reward: 67.33572522, e: 0.2485025, Learning Rate: 0.002, buffer_len: 129856\n",
      "Loss: -2.01158761978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 01:22:09,335] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 220, Length: 935, Avg Reward: 61.9113441283, e: 0.239548333333, Learning Rate: 0.002, buffer_len: 137531\n",
      "Loss: -0.793254256248\n",
      "[MAX] Episode: 222, Length: 549, Reward: 235.530647789, buffer_len: 139080\n",
      "[NOR] Episode: 230, Length: 490, Avg Reward: 96.9677895858, e: 0.2292175, Learning Rate: 0.002, buffer_len: 146386\n",
      "Loss: -1.93373680115\n",
      "[NOR] Episode: 240, Length: 501, Avg Reward: 107.340951331, e: 0.218791, Learning Rate: 0.002, buffer_len: 155323\n",
      "Loss: -3.35356998444\n",
      "[NOR] Episode: 250, Length: 105, Avg Reward: 77.6931485819, e: 0.210458666667, Learning Rate: 0.002, buffer_len: 162465\n",
      "Loss: -0.959419071674\n",
      "[NOR] Episode: 260, Length: 919, Avg Reward: 27.6069264486, e: 0.205005666667, Learning Rate: 0.002, buffer_len: 167139\n",
      "Loss: -1.6795232296\n",
      "[NOR] Episode: 270, Length: 504, Avg Reward: 10.1686769874, e: 0.201751833333, Learning Rate: 0.002, buffer_len: 169928\n",
      "Loss: -2.14257383347\n",
      "[NOR] Episode: 280, Length: 110, Avg Reward: 98.3108000542, e: 0.196375833333, Learning Rate: 0.002, buffer_len: 174536\n",
      "Loss: -0.919072628021\n",
      "[NOR] Episode: 290, Length: 229, Avg Reward: 41.2287726802, e: 0.191533, Learning Rate: 0.002, buffer_len: 178687\n",
      "Loss: -0.107482910156\n",
      "[NOR] Episode: 300, Length: 1000, Avg Reward: 53.032055132, e: 0.186358833333, Learning Rate: 0.002, buffer_len: 183122\n",
      "Loss: -2.16341471672\n",
      "[NOR] Episode: 310, Length: 426, Avg Reward: 48.8353997059, e: 0.181581333333, Learning Rate: 0.002, buffer_len: 187217\n",
      "Loss: -1.00580835342\n",
      "[NOR] Episode: 320, Length: 470, Avg Reward: 97.003963171, e: 0.1755065, Learning Rate: 0.002, buffer_len: 192424\n",
      "Loss: -1.30122613907\n",
      "[NOR] Episode: 330, Length: 162, Avg Reward: 64.1471966959, e: 0.169049, Learning Rate: 0.002, buffer_len: 197959\n",
      "Loss: -1.887165308\n",
      "[NOR] Episode: 340, Length: 237, Avg Reward: 87.7963628569, e: 0.1627735, Learning Rate: 0.002, buffer_len: 203338\n",
      "Loss: -1.08497595787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 01:26:23,396] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 350, Length: 555, Avg Reward: 69.3352999502, e: 0.1593645, Learning Rate: 0.002, buffer_len: 206260\n",
      "Loss: -0.802798271179\n",
      "[NOR] Episode: 360, Length: 316, Avg Reward: 74.9446583008, e: 0.155075833333, Learning Rate: 0.002, buffer_len: 209936\n",
      "Loss: -0.302848160267\n",
      "[NOR] Episode: 370, Length: 374, Avg Reward: 93.4154837075, e: 0.148183166667, Learning Rate: 0.002, buffer_len: 215844\n",
      "Loss: -1.46220266819\n",
      "[NOR] Episode: 380, Length: 132, Avg Reward: 68.9704364647, e: 0.1439295, Learning Rate: 0.002, buffer_len: 219490\n",
      "Loss: -0.0121326446533\n",
      "[MAX] Episode: 382, Length: 300, Reward: 236.468349315, buffer_len: 220621\n",
      "[NOR] Episode: 390, Length: 586, Avg Reward: 96.3660434412, e: 0.139380666667, Learning Rate: 0.002, buffer_len: 223389\n",
      "Loss: -0.34280538559\n",
      "[NOR] Episode: 400, Length: 541, Avg Reward: 75.0311199044, e: 0.1356975, Learning Rate: 0.002, buffer_len: 226546\n",
      "Loss: 0.0892119407654\n",
      "[NOR] Episode: 410, Length: 85, Avg Reward: 97.1850769197, e: 0.131659666667, Learning Rate: 0.002, buffer_len: 230007\n",
      "Loss: -3.23237276077\n",
      "[NOR] Episode: 420, Length: 368, Avg Reward: 118.25055063, e: 0.1273045, Learning Rate: 0.002, buffer_len: 233740\n",
      "Loss: -0.975674450397\n",
      "[MAX] Episode: 429, Length: 224, Reward: 260.356090002, buffer_len: 236098\n",
      "[NOR] Episode: 430, Length: 333, Avg Reward: 111.332493945, e: 0.124165, Learning Rate: 0.002, buffer_len: 236431\n",
      "Loss: -3.47184991837\n",
      "[NOR] Episode: 440, Length: 683, Avg Reward: 166.107024329, e: 0.118968666667, Learning Rate: 0.002, buffer_len: 240885\n",
      "Loss: 0.550193846226\n",
      "[NOR] Episode: 450, Length: 299, Avg Reward: 105.063490396, e: 0.1147745, Learning Rate: 0.002, buffer_len: 244480\n",
      "Loss: -1.31029093266\n",
      "[NOR] Episode: 460, Length: 102, Avg Reward: 94.8642492517, e: 0.110740166667, Learning Rate: 0.002, buffer_len: 247938\n",
      "Loss: -1.03165376186\n",
      "[NOR] Episode: 470, Length: 570, Avg Reward: 87.0099581994, e: 0.1047365, Learning Rate: 0.002, buffer_len: 253084\n",
      "Loss: -1.31708073616\n",
      "[NOR] Episode: 480, Length: 322, Avg Reward: 84.6391847044, e: 0.100526, Learning Rate: 0.002, buffer_len: 256693\n",
      "Loss: 0.127690911293\n",
      "[NOR] Episode: 490, Length: 118, Avg Reward: 125.383184089, e: 0.0957776666667, Learning Rate: 0.002, buffer_len: 260763\n",
      "Loss: -2.27154707909\n",
      "[NOR] Episode: 500, Length: 245, Avg Reward: 109.010550228, e: 0.0889141666667, Learning Rate: 0.002, buffer_len: 266646\n",
      "Loss: -0.249225348234\n",
      "[NOR] Episode: 510, Length: 516, Avg Reward: 134.310968249, e: 0.0843245, Learning Rate: 0.002, buffer_len: 270580\n",
      "Loss: -2.30751752853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 01:30:21,402] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 520, Length: 524, Avg Reward: 125.649604513, e: 0.0804056666667, Learning Rate: 0.002, buffer_len: 273939\n",
      "Loss: -0.329622209072\n",
      "[NOR] Episode: 530, Length: 320, Avg Reward: 97.5111650268, e: 0.0762768333333, Learning Rate: 0.002, buffer_len: 277478\n",
      "Loss: -1.62040007114\n",
      "[NOR] Episode: 540, Length: 75, Avg Reward: 81.2694240828, e: 0.0726216666667, Learning Rate: 0.002, buffer_len: 280611\n",
      "Loss: -0.0692533254623\n",
      "[NOR] Episode: 550, Length: 1000, Avg Reward: 111.273605193, e: 0.0672188333333, Learning Rate: 0.002, buffer_len: 285242\n",
      "Loss: -1.74000835419\n",
      "[NOR] Episode: 560, Length: 353, Avg Reward: 61.6392953386, e: 0.0639206666667, Learning Rate: 0.002, buffer_len: 288069\n",
      "Loss: -0.15679910779\n",
      "[NOR] Episode: 570, Length: 591, Avg Reward: 81.2559800146, e: 0.0594908333333, Learning Rate: 0.002, buffer_len: 291866\n",
      "Loss: -1.89224755764\n",
      "[NOR] Episode: 580, Length: 216, Avg Reward: 83.4367117545, e: 0.0561028333333, Learning Rate: 0.002, buffer_len: 294770\n",
      "Loss: -2.2788438797\n",
      "[NOR] Episode: 590, Length: 140, Avg Reward: 14.3314888454, e: 0.0536936666667, Learning Rate: 0.002, buffer_len: 296835\n",
      "Loss: -2.1568980217\n",
      "[NOR] Episode: 600, Length: 97, Avg Reward: 80.0897413165, e: 0.0504631666667, Learning Rate: 0.002, buffer_len: 299604\n",
      "Loss: -1.21531200409\n",
      "[NOR] Episode: 610, Length: 353, Avg Reward: 29.1575254005, e: 0.05, Learning Rate: 0.002, buffer_len: 302322\n",
      "Loss: -0.94443488121\n",
      "[NOR] Episode: 620, Length: 105, Avg Reward: 83.8025893677, e: 0.05, Learning Rate: 0.002, buffer_len: 305989\n",
      "Loss: -0.332617402077\n",
      "[NOR] Episode: 630, Length: 89, Avg Reward: 67.3484612733, e: 0.05, Learning Rate: 0.002, buffer_len: 308234\n",
      "Loss: -0.219612568617\n",
      "[NOR] Episode: 640, Length: 282, Avg Reward: 37.2834708645, e: 0.05, Learning Rate: 0.002, buffer_len: 311100\n",
      "Loss: -0.997579395771\n",
      "[NOR] Episode: 650, Length: 290, Avg Reward: 51.519846452, e: 0.05, Learning Rate: 0.002, buffer_len: 314112\n",
      "Loss: -0.0465407371521\n",
      "[NOR] Episode: 660, Length: 94, Avg Reward: -16.0011200919, e: 0.05, Learning Rate: 0.002, buffer_len: 317302\n",
      "Loss: -1.93948233128\n",
      "[NOR] Episode: 670, Length: 351, Avg Reward: 15.9714454091, e: 0.05, Learning Rate: 0.002, buffer_len: 320215\n",
      "Loss: -2.5501730442\n",
      "[NOR] Episode: 680, Length: 78, Avg Reward: 13.4243714305, e: 0.05, Learning Rate: 0.002, buffer_len: 323449\n",
      "Loss: -3.12169981003\n",
      "[NOR] Episode: 690, Length: 392, Avg Reward: 45.0940753375, e: 0.05, Learning Rate: 0.002, buffer_len: 327534\n",
      "Loss: -1.1049683094\n",
      "[NOR] Episode: 700, Length: 376, Avg Reward: 62.1399879496, e: 0.05, Learning Rate: 0.002, buffer_len: 332904\n",
      "Loss: -3.30423021317\n",
      "[NOR] Episode: 710, Length: 147, Avg Reward: 55.1493501497, e: 0.05, Learning Rate: 0.002, buffer_len: 338127\n",
      "Loss: -1.57804238796\n",
      "[NOR] Episode: 720, Length: 977, Avg Reward: 135.742583506, e: 0.05, Learning Rate: 0.002, buffer_len: 343521\n",
      "Loss: -0.919355690479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 01:34:49,766] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 730, Length: 286, Avg Reward: 103.85104574, e: 0.05, Learning Rate: 0.002, buffer_len: 347598\n",
      "Loss: -0.425585031509\n",
      "[NOR] Episode: 740, Length: 381, Avg Reward: 105.606009282, e: 0.05, Learning Rate: 0.002, buffer_len: 351287\n",
      "Loss: 1.17494595051\n",
      "[NOR] Episode: 750, Length: 366, Avg Reward: 161.404563968, e: 0.05, Learning Rate: 0.002, buffer_len: 355339\n",
      "Loss: -1.02347683907\n",
      "[NOR] Episode: 760, Length: 442, Avg Reward: 175.152166839, e: 0.05, Learning Rate: 0.002, buffer_len: 359028\n",
      "Loss: -0.312816441059\n",
      "[NOR] Episode: 770, Length: 228, Avg Reward: 113.00647592, e: 0.05, Learning Rate: 0.002, buffer_len: 363202\n",
      "Loss: -0.612167298794\n",
      "[NOR] Episode: 780, Length: 383, Avg Reward: 173.938103966, e: 0.05, Learning Rate: 0.002, buffer_len: 366960\n",
      "Loss: 0.177760094404\n",
      "[NOR] Episode: 790, Length: 279, Avg Reward: 120.685691378, e: 0.05, Learning Rate: 0.002, buffer_len: 371381\n",
      "Loss: -0.781987369061\n",
      "[NOR] Episode: 800, Length: 254, Avg Reward: 161.620636341, e: 0.05, Learning Rate: 0.002, buffer_len: 375948\n",
      "Loss: -0.201489269733\n",
      "[NOR] Episode: 810, Length: 315, Avg Reward: 149.265454452, e: 0.05, Learning Rate: 0.002, buffer_len: 378989\n",
      "Loss: -1.09743762016\n",
      "[NOR] Episode: 820, Length: 394, Avg Reward: 159.724561548, e: 0.05, Learning Rate: 0.002, buffer_len: 384259\n",
      "Loss: 0.0298486053944\n",
      "[NOR] Episode: 830, Length: 226, Avg Reward: 150.219613872, e: 0.05, Learning Rate: 0.002, buffer_len: 388117\n",
      "Loss: -2.76935815811\n",
      "[NOR] Episode: 840, Length: 255, Avg Reward: 141.867869903, e: 0.05, Learning Rate: 0.002, buffer_len: 393286\n",
      "Loss: -0.446010023355\n",
      "[NOR] Episode: 850, Length: 1000, Avg Reward: 84.7887941533, e: 0.05, Learning Rate: 0.002, buffer_len: 399840\n",
      "Loss: -0.592943549156\n",
      "[NOR] Episode: 860, Length: 319, Avg Reward: 187.063523178, e: 0.05, Learning Rate: 0.002, buffer_len: 403887\n",
      "Loss: -0.379167914391\n",
      "[NOR] Episode: 870, Length: 606, Avg Reward: 85.2731982445, e: 0.05, Learning Rate: 0.002, buffer_len: 409030\n",
      "Loss: 2.0933713913\n",
      "[NOR] Episode: 880, Length: 446, Avg Reward: 127.610879629, e: 0.05, Learning Rate: 0.002, buffer_len: 414201\n",
      "Loss: -2.33113765717\n",
      "[NOR] Episode: 890, Length: 348, Avg Reward: 153.61931213, e: 0.05, Learning Rate: 0.002, buffer_len: 418541\n",
      "Loss: -0.815934360027\n",
      "[MAX] Episode: 891, Length: 249, Reward: 262.327807987, buffer_len: 418790\n",
      "[NOR] Episode: 900, Length: 1000, Avg Reward: 102.145857391, e: 0.05, Learning Rate: 0.002, buffer_len: 426061\n",
      "Loss: -0.899403274059\n",
      "[NOR] Episode: 910, Length: 369, Avg Reward: 111.213380965, e: 0.05, Learning Rate: 0.002, buffer_len: 432654\n",
      "Loss: -0.71600997448\n",
      "[NOR] Episode: 920, Length: 275, Avg Reward: 190.11095335, e: 0.05, Learning Rate: 0.002, buffer_len: 437298\n",
      "Loss: -1.91741240025\n",
      "[NOR] Episode: 930, Length: 1000, Avg Reward: 116.90594628, e: 0.05, Learning Rate: 0.002, buffer_len: 442696\n",
      "Loss: -2.24288368225\n",
      "[NOR] Episode: 940, Length: 642, Avg Reward: 86.234997964, e: 0.05, Learning Rate: 0.002, buffer_len: 449630\n",
      "Loss: 0.137232899666\n",
      "[NOR] Episode: 950, Length: 1000, Avg Reward: -9.01069966403, e: 0.05, Learning Rate: 0.002, buffer_len: 458405\n",
      "Loss: -2.60629701614\n",
      "[NOR] Episode: 960, Length: 1000, Avg Reward: 49.0052199412, e: 0.05, Learning Rate: 0.002, buffer_len: 466414\n",
      "Loss: -0.936759471893\n",
      "[NOR] Episode: 970, Length: 1000, Avg Reward: -59.021475978, e: 0.05, Learning Rate: 0.002, buffer_len: 476414\n",
      "Loss: -1.86913228035\n",
      "[NOR] Episode: 980, Length: 611, Avg Reward: 52.3445138177, e: 0.05, Learning Rate: 0.002, buffer_len: 484632\n",
      "Loss: -1.6341817379\n",
      "[NOR] Episode: 990, Length: 552, Avg Reward: 159.361679849, e: 0.05, Learning Rate: 0.002, buffer_len: 490351\n",
      "Loss: 1.66797924042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 01:43:57,831] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 1000, Length: 402, Avg Reward: 176.276595591, e: 0.05, Learning Rate: 0.002, buffer_len: 495783\n",
      "Loss: -2.33706784248\n",
      "[NOR] Episode: 1010, Length: 588, Avg Reward: 135.89198285, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -4.38565301895\n",
      "[NOR] Episode: 1020, Length: 720, Avg Reward: 144.284986457, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.54419279099\n",
      "[NOR] Episode: 1030, Length: 372, Avg Reward: 126.909941185, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.00876140594482\n",
      "[NOR] Episode: 1040, Length: 1000, Avg Reward: 115.631675221, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.509763002396\n",
      "[NOR] Episode: 1050, Length: 1000, Avg Reward: -45.6617702296, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.539056301117\n",
      "[NOR] Episode: 1060, Length: 1000, Avg Reward: 24.278846772, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.26029330492\n",
      "[NOR] Episode: 1070, Length: 1000, Avg Reward: 115.668029346, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.20339047909\n",
      "[NOR] Episode: 1080, Length: 1000, Avg Reward: -20.3929722784, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.144913926721\n",
      "[NOR] Episode: 1090, Length: 1000, Avg Reward: 25.4738094693, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0888934731483\n",
      "[NOR] Episode: 1100, Length: 737, Avg Reward: 94.6348683536, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.317023932934\n",
      "[NOR] Episode: 1110, Length: 849, Avg Reward: 110.647455643, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.70213675499\n",
      "[NOR] Episode: 1120, Length: 791, Avg Reward: 140.137952367, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.31425213814\n",
      "[MAX] Episode: 1125, Length: 196, Reward: 265.243033965, buffer_len: 500000\n",
      "[NOR] Episode: 1130, Length: 520, Avg Reward: 190.55202392, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.253890872\n",
      "[NOR] Episode: 1140, Length: 914, Avg Reward: 154.807609657, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.44422775507\n",
      "[NOR] Episode: 1150, Length: 366, Avg Reward: 171.483322954, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.08176374435\n",
      "[MAX] Episode: 1153, Length: 291, Reward: 265.792943672, buffer_len: 500000\n",
      "[NOR] Episode: 1160, Length: 595, Avg Reward: 156.224379373, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.891331076622\n",
      "[NOR] Episode: 1170, Length: 165, Avg Reward: 130.824711663, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.346410989761\n",
      "[NOR] Episode: 1180, Length: 648, Avg Reward: 129.818595521, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.689012885094\n",
      "[NOR] Episode: 1190, Length: 871, Avg Reward: 152.184262999, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.73906230927\n",
      "[NOR] Episode: 1200, Length: 493, Avg Reward: 139.84089558, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.218317657709\n",
      "[NOR] Episode: 1210, Length: 588, Avg Reward: 105.426096263, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0931287109852\n",
      "[NOR] Episode: 1220, Length: 427, Avg Reward: 159.66525979, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.702249646187\n",
      "[NOR] Episode: 1230, Length: 523, Avg Reward: 113.851358175, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.35631346703\n",
      "[NOR] Episode: 1240, Length: 337, Avg Reward: 84.4611783852, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.596947073936\n",
      "[NOR] Episode: 1250, Length: 495, Avg Reward: 63.1358007106, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.931055366993\n",
      "[NOR] Episode: 1260, Length: 194, Avg Reward: 96.4220974315, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.914163887501\n",
      "[NOR] Episode: 1270, Length: 586, Avg Reward: 112.923339891, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.760988533497\n",
      "[NOR] Episode: 1280, Length: 642, Avg Reward: 104.03819036, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.64090871811\n",
      "[NOR] Episode: 1290, Length: 542, Avg Reward: 144.869936507, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.287040919065\n",
      "[NOR] Episode: 1300, Length: 734, Avg Reward: 55.9597927626, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 2.55138444901\n",
      "[NOR] Episode: 1310, Length: 708, Avg Reward: 114.199543936, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.209620639682\n",
      "[NOR] Episode: 1320, Length: 798, Avg Reward: 125.62852367, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.632687866688\n",
      "[NOR] Episode: 1330, Length: 705, Avg Reward: 143.05359846, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.9798848629\n",
      "[NOR] Episode: 1340, Length: 634, Avg Reward: 147.804287682, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -8.16618251801\n",
      "[NOR] Episode: 1350, Length: 550, Avg Reward: 200.28446663, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.79050385952\n",
      "[NOR] Episode: 1360, Length: 382, Avg Reward: 192.72035438, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.294460833073\n",
      "[NOR] Episode: 1370, Length: 789, Avg Reward: 201.83397776, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.270080089569\n",
      "[NOR] Episode: 1380, Length: 341, Avg Reward: 183.839235756, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.20745718479\n",
      "[NOR] Episode: 1390, Length: 319, Avg Reward: 105.369060356, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.406965970993\n",
      "[NOR] Episode: 1400, Length: 387, Avg Reward: 179.494994904, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.194416642189\n",
      "[NOR] Episode: 1410, Length: 303, Avg Reward: 121.447662514, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.961205005646\n",
      "[NOR] Episode: 1420, Length: 335, Avg Reward: 149.341683731, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.735792040825\n",
      "[NOR] Episode: 1430, Length: 383, Avg Reward: 173.822090694, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.48889696598\n",
      "[NOR] Episode: 1440, Length: 288, Avg Reward: 202.889965189, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.65612387657\n",
      "[NOR] Episode: 1450, Length: 530, Avg Reward: 194.841648172, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.753309130669\n",
      "[NOR] Episode: 1460, Length: 337, Avg Reward: 170.878085043, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.251536369324\n",
      "[NOR] Episode: 1470, Length: 1000, Avg Reward: 156.54234963, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.2673586607\n",
      "[NOR] Episode: 1480, Length: 265, Avg Reward: 177.856817215, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.25428438187\n",
      "[NOR] Episode: 1490, Length: 322, Avg Reward: 143.691562803, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.406531393528\n",
      "[NOR] Episode: 1500, Length: 236, Avg Reward: 158.85217241, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.487067401409\n",
      "[NOR] Episode: 1510, Length: 410, Avg Reward: 148.595956405, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.816815018654\n",
      "[NOR] Episode: 1520, Length: 289, Avg Reward: 159.248564967, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.42339897156\n",
      "[NOR] Episode: 1530, Length: 387, Avg Reward: 167.614149337, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.39357018471\n",
      "[NOR] Episode: 1540, Length: 404, Avg Reward: 190.314251055, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.14385342598\n",
      "[NOR] Episode: 1550, Length: 1000, Avg Reward: 178.959508322, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.35141217709\n",
      "[NOR] Episode: 1560, Length: 358, Avg Reward: 202.709038349, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.529236197472\n",
      "[NOR] Episode: 1570, Length: 488, Avg Reward: 150.836393901, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.415727585554\n",
      "[NOR] Episode: 1580, Length: 249, Avg Reward: 212.817315044, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.241526141763\n",
      "[NOR] Episode: 1590, Length: 428, Avg Reward: 212.267300159, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.261650979519\n",
      "[NOR] Episode: 1600, Length: 290, Avg Reward: 204.821295121, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.258838653564\n",
      "[NOR] Episode: 1610, Length: 299, Avg Reward: 204.62373855, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.61099338531\n",
      "[NOR] Episode: 1620, Length: 269, Avg Reward: 217.825648886, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.05004072189\n",
      "[NOR] Episode: 1630, Length: 348, Avg Reward: 210.617891035, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0661809444427\n",
      "[NOR] Episode: 1640, Length: 285, Avg Reward: 212.528583283, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.11113333702\n",
      "[NOR] Episode: 1650, Length: 243, Avg Reward: 227.813185866, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.519165456295\n",
      "[NOR] Episode: 1660, Length: 475, Avg Reward: 214.727964299, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.0968760252\n",
      "[NOR] Episode: 1670, Length: 250, Avg Reward: 216.525617354, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.622902750969\n",
      "[NOR] Episode: 1680, Length: 301, Avg Reward: 226.651161223, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.53351902962\n",
      "[NOR] Episode: 1690, Length: 307, Avg Reward: 221.054640991, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.30722296238\n",
      "[NOR] Episode: 1700, Length: 380, Avg Reward: 205.598929983, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.284604758024\n",
      "[NOR] Episode: 1710, Length: 113, Avg Reward: 183.423028386, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.730485737324\n",
      "[NOR] Episode: 1720, Length: 232, Avg Reward: 214.911776033, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -3.53330254555\n",
      "[NOR] Episode: 1730, Length: 307, Avg Reward: 237.158254894, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.07560610771\n",
      "[NOR] Episode: 1740, Length: 340, Avg Reward: 165.743163013, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.15638780594\n",
      "[NOR] Episode: 1750, Length: 370, Avg Reward: 204.724501796, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.53830909729\n",
      "[NOR] Episode: 1760, Length: 466, Avg Reward: 206.824814201, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.55661416054\n",
      "[NOR] Episode: 1770, Length: 313, Avg Reward: 225.112707222, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.10933732986\n",
      "[NOR] Episode: 1780, Length: 340, Avg Reward: 219.263199198, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.58327519894\n",
      "[NOR] Episode: 1790, Length: 293, Avg Reward: 187.957331165, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.126397132874\n",
      "[MAX] Episode: 1792, Length: 248, Reward: 267.396625107, buffer_len: 500000\n",
      "[NOR] Episode: 1800, Length: 228, Avg Reward: 226.815807511, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.69339132309\n",
      "[NOR] Episode: 1810, Length: 241, Avg Reward: 200.056590008, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.624474465847\n",
      "[NOR] Episode: 1820, Length: 320, Avg Reward: 223.669225086, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.877872169018\n",
      "[NOR] Episode: 1830, Length: 287, Avg Reward: 213.580201196, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.272877573967\n",
      "[NOR] Episode: 1840, Length: 306, Avg Reward: 204.773447127, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.8866969347\n",
      "[NOR] Episode: 1850, Length: 662, Avg Reward: 217.428702991, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.33897256851\n",
      "[NOR] Episode: 1860, Length: 216, Avg Reward: 222.265970746, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0935165733099\n",
      "[NOR] Episode: 1870, Length: 238, Avg Reward: 227.633667289, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.279627203941\n",
      "[NOR] Episode: 1880, Length: 231, Avg Reward: 229.079049874, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.155771970749\n",
      "[NOR] Episode: 1890, Length: 236, Avg Reward: 214.383486178, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0530204772949\n",
      "[NOR] Episode: 1900, Length: 318, Avg Reward: 201.883258577, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.156265854836\n",
      "[NOR] Episode: 1910, Length: 283, Avg Reward: 228.717047704, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.04000997543\n",
      "[NOR] Episode: 1920, Length: 226, Avg Reward: 182.975733343, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.394839465618\n",
      "[NOR] Episode: 1930, Length: 255, Avg Reward: 217.402060782, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.219222158194\n",
      "[NOR] Episode: 1940, Length: 319, Avg Reward: 182.50405613, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.60833394527\n",
      "[NOR] Episode: 1950, Length: 349, Avg Reward: 166.504471839, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.91003370285\n",
      "[NOR] Episode: 1960, Length: 252, Avg Reward: 190.200310982, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.53327429295\n",
      "[NOR] Episode: 1970, Length: 212, Avg Reward: 127.426120213, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.78396344185\n",
      "[NOR] Episode: 1980, Length: 256, Avg Reward: 175.511903886, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.398419767618\n",
      "[NOR] Episode: 1990, Length: 131, Avg Reward: 181.691018094, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.505242705345\n",
      "[MAX] Episode: 1997, Length: 348, Reward: 271.110556239, buffer_len: 500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 02:12:43,866] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video002000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 2000, Length: 105, Avg Reward: 74.8528093931, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.42925632\n",
      "[NOR] Episode: 2010, Length: 330, Avg Reward: 202.118730405, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.61160850525\n",
      "[NOR] Episode: 2020, Length: 237, Avg Reward: 183.332002496, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.939157903194\n",
      "[NOR] Episode: 2030, Length: 281, Avg Reward: 192.565243475, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.328345358372\n",
      "[NOR] Episode: 2040, Length: 269, Avg Reward: 222.145410345, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.397533714771\n",
      "[NOR] Episode: 2050, Length: 334, Avg Reward: 189.040629324, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 3.76170086861\n",
      "[NOR] Episode: 2060, Length: 347, Avg Reward: 219.696205163, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.67498338223\n",
      "[NOR] Episode: 2070, Length: 229, Avg Reward: 193.72001469, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.8037314415\n",
      "[NOR] Episode: 2080, Length: 269, Avg Reward: 214.94899266, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.196292966604\n",
      "[MAX] Episode: 2085, Length: 248, Reward: 275.105155254, buffer_len: 500000\n",
      "[NOR] Episode: 2090, Length: 227, Avg Reward: 210.354657057, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.04712033272\n",
      "[NOR] Episode: 2100, Length: 253, Avg Reward: 220.060321868, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.41528356075\n",
      "[NOR] Episode: 2110, Length: 229, Avg Reward: 229.800286468, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.588634610176\n",
      "[NOR] Episode: 2120, Length: 275, Avg Reward: 218.041924729, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.017096221447\n",
      "[NOR] Episode: 2130, Length: 365, Avg Reward: 185.564377287, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.499419093132\n",
      "[NOR] Episode: 2140, Length: 220, Avg Reward: 191.092513658, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.359162956476\n",
      "[NOR] Episode: 2150, Length: 210, Avg Reward: 177.992606196, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.848812401295\n",
      "[NOR] Episode: 2160, Length: 255, Avg Reward: 191.631233998, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.740983605385\n",
      "[NOR] Episode: 2170, Length: 195, Avg Reward: 228.518385598, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.56435477734\n",
      "[NOR] Episode: 2180, Length: 208, Avg Reward: 228.962350435, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.555628716946\n",
      "[NOR] Episode: 2190, Length: 389, Avg Reward: 223.0518863, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.726885199547\n",
      "[NOR] Episode: 2200, Length: 252, Avg Reward: 214.22957735, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.103863075376\n",
      "[NOR] Episode: 2210, Length: 281, Avg Reward: 220.851951553, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0846808254719\n",
      "[NOR] Episode: 2220, Length: 435, Avg Reward: 222.142356532, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.50076770782\n",
      "[NOR] Episode: 2230, Length: 321, Avg Reward: 200.729268675, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.5672352314\n",
      "[NOR] Episode: 2240, Length: 365, Avg Reward: 228.810909259, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.664081215858\n",
      "[NOR] Episode: 2250, Length: 303, Avg Reward: 230.557819946, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.739560067654\n",
      "[NOR] Episode: 2260, Length: 275, Avg Reward: 215.371925953, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.832236647606\n",
      "[NOR] Episode: 2270, Length: 208, Avg Reward: 179.909313461, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.901009738445\n",
      "[NOR] Episode: 2280, Length: 306, Avg Reward: 194.256796759, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.715234160423\n",
      "[NOR] Episode: 2290, Length: 240, Avg Reward: 198.157084758, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.89362740517\n",
      "[NOR] Episode: 2300, Length: 268, Avg Reward: 229.784587435, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.16860413551\n",
      "[NOR] Episode: 2310, Length: 247, Avg Reward: 226.894625705, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.191683977842\n",
      "[NOR] Episode: 2320, Length: 239, Avg Reward: 223.53175008, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.70461189747\n",
      "[NOR] Episode: 2330, Length: 280, Avg Reward: 219.781533642, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.502858161926\n",
      "[NOR] Episode: 2340, Length: 277, Avg Reward: 227.399440108, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.318967998028\n",
      "[NOR] Episode: 2350, Length: 228, Avg Reward: 223.338565328, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.478942930698\n",
      "[NOR] Episode: 2360, Length: 230, Avg Reward: 220.904572234, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.453588545322\n",
      "[NOR] Episode: 2370, Length: 317, Avg Reward: 222.783575645, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.185019060969\n",
      "[NOR] Episode: 2380, Length: 210, Avg Reward: 208.927489989, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.478673219681\n",
      "[NOR] Episode: 2390, Length: 344, Avg Reward: 218.992403366, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.374618798494\n",
      "[NOR] Episode: 2400, Length: 366, Avg Reward: 217.721964395, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.114670097828\n",
      "[NOR] Episode: 2410, Length: 264, Avg Reward: 217.364255131, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.15699517727\n",
      "[NOR] Episode: 2420, Length: 267, Avg Reward: 223.192446411, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.330148249865\n",
      "[NOR] Episode: 2430, Length: 281, Avg Reward: 225.006941859, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.461020320654\n",
      "[NOR] Episode: 2440, Length: 279, Avg Reward: 215.541734839, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.667422950268\n",
      "[NOR] Episode: 2450, Length: 201, Avg Reward: 222.580714743, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.76019620895\n",
      "[NOR] Episode: 2460, Length: 290, Avg Reward: 191.89059943, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.414920151234\n",
      "[NOR] Episode: 2470, Length: 312, Avg Reward: 200.428808632, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.854783654213\n",
      "[NOR] Episode: 2480, Length: 235, Avg Reward: 186.87764177, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.632298231125\n",
      "[NOR] Episode: 2490, Length: 294, Avg Reward: 171.567418558, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.2155097723\n",
      "[NOR] Episode: 2500, Length: 193, Avg Reward: 132.953234687, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.45939040184\n",
      "[NOR] Episode: 2510, Length: 308, Avg Reward: 173.093409651, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.29026928544\n",
      "[NOR] Episode: 2520, Length: 294, Avg Reward: 167.386783032, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.658098340034\n",
      "[NOR] Episode: 2530, Length: 396, Avg Reward: 138.660619561, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.67256724834\n",
      "[NOR] Episode: 2540, Length: 333, Avg Reward: 73.9838166756, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.00837254524231\n",
      "[NOR] Episode: 2550, Length: 236, Avg Reward: 123.924801973, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0760105550289\n",
      "[NOR] Episode: 2560, Length: 218, Avg Reward: 191.158962507, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.175359785557\n",
      "[NOR] Episode: 2570, Length: 197, Avg Reward: 164.813070857, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.34651207924\n",
      "[NOR] Episode: 2580, Length: 275, Avg Reward: 200.585979207, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.814261674881\n",
      "[NOR] Episode: 2590, Length: 374, Avg Reward: 177.04766755, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.397024214268\n",
      "[NOR] Episode: 2600, Length: 311, Avg Reward: 207.334785512, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.02406215668\n",
      "[NOR] Episode: 2610, Length: 251, Avg Reward: 173.315001396, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.00688999891281\n",
      "[NOR] Episode: 2620, Length: 429, Avg Reward: 208.662718932, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.830569505692\n",
      "[NOR] Episode: 2630, Length: 278, Avg Reward: 209.894416268, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.802213788033\n",
      "[NOR] Episode: 2640, Length: 256, Avg Reward: 191.49279132, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.47893249989\n",
      "[NOR] Episode: 2650, Length: 212, Avg Reward: 207.377502987, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.39710640907\n",
      "[NOR] Episode: 2660, Length: 256, Avg Reward: 229.489572841, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.334278941154\n",
      "[NOR] Episode: 2670, Length: 283, Avg Reward: 204.542037788, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.03969621658\n",
      "[NOR] Episode: 2680, Length: 272, Avg Reward: 201.125825567, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.511992394924\n",
      "[NOR] Episode: 2690, Length: 395, Avg Reward: 204.554403143, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.105587393045\n",
      "[NOR] Episode: 2700, Length: 295, Avg Reward: 222.08529378, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.996250987053\n",
      "[NOR] Episode: 2710, Length: 381, Avg Reward: 202.966461803, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.658422708511\n",
      "[NOR] Episode: 2720, Length: 414, Avg Reward: 197.393120634, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.413592517376\n",
      "[NOR] Episode: 2730, Length: 397, Avg Reward: 208.806321227, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.33170509338\n",
      "[NOR] Episode: 2740, Length: 328, Avg Reward: 207.826970329, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.92831748724\n",
      "[NOR] Episode: 2750, Length: 365, Avg Reward: 203.818565645, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.00591814518\n",
      "[NOR] Episode: 2760, Length: 341, Avg Reward: 125.979511355, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.67192673683\n",
      "[NOR] Episode: 2770, Length: 340, Avg Reward: 203.866865148, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.625218451023\n",
      "[NOR] Episode: 2780, Length: 314, Avg Reward: 163.985102021, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.553079485893\n",
      "[NOR] Episode: 2790, Length: 246, Avg Reward: 122.685829139, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.381576687098\n",
      "[NOR] Episode: 2800, Length: 334, Avg Reward: 141.024751447, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.471479386091\n",
      "[NOR] Episode: 2810, Length: 1000, Avg Reward: 88.5985295449, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.9503890276\n",
      "[NOR] Episode: 2820, Length: 753, Avg Reward: 188.633996472, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.526077270508\n",
      "[NOR] Episode: 2830, Length: 514, Avg Reward: 192.612788665, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.189093142748\n",
      "[NOR] Episode: 2840, Length: 394, Avg Reward: 184.493328212, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.35962080956\n",
      "[NOR] Episode: 2850, Length: 384, Avg Reward: 208.186418366, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.61726975441\n",
      "[NOR] Episode: 2860, Length: 447, Avg Reward: 196.424279512, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.295299977064\n",
      "[NOR] Episode: 2870, Length: 425, Avg Reward: 192.395198421, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.124450877309\n",
      "[NOR] Episode: 2880, Length: 568, Avg Reward: 166.238206945, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.873496949673\n",
      "[NOR] Episode: 2890, Length: 507, Avg Reward: 140.470506303, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.497276902199\n",
      "[NOR] Episode: 2900, Length: 545, Avg Reward: 167.061406292, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.847539544106\n",
      "[NOR] Episode: 2910, Length: 585, Avg Reward: 181.733733489, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.781464457512\n",
      "[NOR] Episode: 2920, Length: 665, Avg Reward: 163.498162892, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.67194890976\n",
      "[NOR] Episode: 2930, Length: 437, Avg Reward: 163.981400041, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.665223658085\n",
      "[NOR] Episode: 2940, Length: 888, Avg Reward: 157.39706801, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.517507135868\n",
      "[NOR] Episode: 2950, Length: 602, Avg Reward: 149.114101642, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -3.13684248924\n",
      "[NOR] Episode: 2960, Length: 766, Avg Reward: 159.261524955, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.53914427757\n",
      "[NOR] Episode: 2970, Length: 631, Avg Reward: 156.999732755, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.138077273965\n",
      "[NOR] Episode: 2980, Length: 602, Avg Reward: 148.232564373, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.448685854673\n",
      "[NOR] Episode: 2990, Length: 594, Avg Reward: 137.926798435, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.881546378136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 02:34:39,251] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video003000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 3000, Length: 605, Avg Reward: 165.118672331, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.915148615837\n",
      "[NOR] Episode: 3010, Length: 681, Avg Reward: 160.581038683, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0497445575893\n",
      "[NOR] Episode: 3020, Length: 511, Avg Reward: 123.571426859, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.670706152916\n",
      "[NOR] Episode: 3030, Length: 496, Avg Reward: 157.657445991, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.712878465652\n",
      "[NOR] Episode: 3040, Length: 734, Avg Reward: 148.606211654, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.478569447994\n",
      "[NOR] Episode: 3050, Length: 999, Avg Reward: 141.376530958, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.263548433781\n",
      "[NOR] Episode: 3060, Length: 551, Avg Reward: 110.372215424, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.399818629026\n",
      "[NOR] Episode: 3070, Length: 403, Avg Reward: 169.476588211, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.130305469036\n",
      "[NOR] Episode: 3080, Length: 532, Avg Reward: 208.617417564, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.201479017735\n",
      "[NOR] Episode: 3090, Length: 551, Avg Reward: 190.126249819, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.794112920761\n",
      "[NOR] Episode: 3100, Length: 551, Avg Reward: 192.417724028, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.272394776344\n",
      "[MAX] Episode: 3103, Length: 229, Reward: 277.291109134, buffer_len: 500000\n",
      "[NOR] Episode: 3110, Length: 275, Avg Reward: 204.69188467, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.11679267883\n",
      "[NOR] Episode: 3120, Length: 382, Avg Reward: 209.559750828, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.172401070595\n",
      "[NOR] Episode: 3130, Length: 442, Avg Reward: 199.593633911, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.719915032387\n",
      "[NOR] Episode: 3140, Length: 350, Avg Reward: 206.429854226, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.69090199471\n",
      "[NOR] Episode: 3150, Length: 372, Avg Reward: 204.6637645, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.23957872391\n",
      "[NOR] Episode: 3160, Length: 133, Avg Reward: 174.186058731, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.03865873814\n",
      "[NOR] Episode: 3170, Length: 555, Avg Reward: 200.330414147, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.68392133713\n",
      "[NOR] Episode: 3180, Length: 384, Avg Reward: 197.383419404, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.52758872509\n",
      "[NOR] Episode: 3190, Length: 457, Avg Reward: 179.670107166, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.882771730423\n",
      "[NOR] Episode: 3200, Length: 272, Avg Reward: 196.4157197, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.893608391285\n",
      "[NOR] Episode: 3210, Length: 310, Avg Reward: 188.004539921, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0691428482533\n",
      "[NOR] Episode: 3220, Length: 411, Avg Reward: 197.03795348, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.93271946907\n",
      "[NOR] Episode: 3230, Length: 417, Avg Reward: 215.735720549, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.0754185915\n",
      "[NOR] Episode: 3240, Length: 236, Avg Reward: 202.000272098, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.344727098942\n",
      "[NOR] Episode: 3250, Length: 325, Avg Reward: 196.377728886, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.541304647923\n",
      "[NOR] Episode: 3260, Length: 287, Avg Reward: 209.116431573, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.743384599686\n",
      "[NOR] Episode: 3270, Length: 694, Avg Reward: 183.605745738, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.727637827396\n",
      "[NOR] Episode: 3280, Length: 312, Avg Reward: 197.793098512, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.83893507719\n",
      "[NOR] Episode: 3290, Length: 275, Avg Reward: 210.743334544, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.298377096653\n",
      "[NOR] Episode: 3300, Length: 227, Avg Reward: 219.036787464, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.960598647594\n",
      "[NOR] Episode: 3310, Length: 529, Avg Reward: 208.935575198, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.48085379601\n",
      "[NOR] Episode: 3320, Length: 340, Avg Reward: 210.989532374, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.14794206619\n",
      "[NOR] Episode: 3330, Length: 269, Avg Reward: 212.208937783, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.00745439529\n",
      "[NOR] Episode: 3340, Length: 275, Avg Reward: 203.787944, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.543701767921\n",
      "[NOR] Episode: 3350, Length: 291, Avg Reward: 215.209737219, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.847908854485\n",
      "[NOR] Episode: 3360, Length: 282, Avg Reward: 220.72427368, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.58746522665\n",
      "[NOR] Episode: 3370, Length: 342, Avg Reward: 226.534454344, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.557696580887\n",
      "[NOR] Episode: 3380, Length: 337, Avg Reward: 198.398347768, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.31334662437\n",
      "[NOR] Episode: 3390, Length: 253, Avg Reward: 220.938221457, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.178388088942\n",
      "[NOR] Episode: 3400, Length: 319, Avg Reward: 228.830358993, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.26842999458\n",
      "[NOR] Episode: 3410, Length: 360, Avg Reward: 220.36430846, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.82799744606\n",
      "[NOR] Episode: 3420, Length: 227, Avg Reward: 220.772258383, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.15835309029\n",
      "[NOR] Episode: 3430, Length: 226, Avg Reward: 203.975116582, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.778312027454\n",
      "[NOR] Episode: 3440, Length: 258, Avg Reward: 235.76373521, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.206785172224\n",
      "[NOR] Episode: 3450, Length: 247, Avg Reward: 221.513819309, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.234875112772\n",
      "[NOR] Episode: 3460, Length: 283, Avg Reward: 234.171983429, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.746542572975\n",
      "[NOR] Episode: 3470, Length: 331, Avg Reward: 220.134063869, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0159796476364\n",
      "[NOR] Episode: 3480, Length: 249, Avg Reward: 228.578859757, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.702274680138\n",
      "[NOR] Episode: 3490, Length: 234, Avg Reward: 225.687845426, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.416912078857\n",
      "[NOR] Episode: 3500, Length: 316, Avg Reward: 224.383560389, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.955079078674\n",
      "[NOR] Episode: 3510, Length: 232, Avg Reward: 215.862282045, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.874174356461\n",
      "[NOR] Episode: 3520, Length: 284, Avg Reward: 223.545608377, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.625799775124\n",
      "[NOR] Episode: 3530, Length: 301, Avg Reward: 205.453782285, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.430779635906\n",
      "[NOR] Episode: 3540, Length: 126, Avg Reward: 198.518700078, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.903587937355\n",
      "[NOR] Episode: 3550, Length: 352, Avg Reward: 218.284424444, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.49073982239\n",
      "[NOR] Episode: 3560, Length: 233, Avg Reward: 212.933170803, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.433496654034\n",
      "[NOR] Episode: 3570, Length: 255, Avg Reward: 219.692613322, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.30415302515\n",
      "[NOR] Episode: 3580, Length: 249, Avg Reward: 205.679501729, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.546706914902\n",
      "[NOR] Episode: 3590, Length: 287, Avg Reward: 214.253877575, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.43744540215\n",
      "[NOR] Episode: 3600, Length: 373, Avg Reward: 216.606608215, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.46795719862\n",
      "[NOR] Episode: 3610, Length: 270, Avg Reward: 217.620104439, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0376082360744\n",
      "[NOR] Episode: 3620, Length: 372, Avg Reward: 190.123852157, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.497161954641\n",
      "[NOR] Episode: 3630, Length: 367, Avg Reward: 198.492418861, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.372316420078\n",
      "[NOR] Episode: 3640, Length: 303, Avg Reward: 219.028503245, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.2037062645\n",
      "[NOR] Episode: 3650, Length: 270, Avg Reward: 219.703579129, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.362643808126\n",
      "[NOR] Episode: 3660, Length: 334, Avg Reward: 224.358990039, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.26928496361\n",
      "[NOR] Episode: 3670, Length: 218, Avg Reward: 221.516696546, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.20780736208\n",
      "[NOR] Episode: 3680, Length: 306, Avg Reward: 191.412168226, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.288970947266\n",
      "[NOR] Episode: 3690, Length: 311, Avg Reward: 229.41192221, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.570274949074\n",
      "[NOR] Episode: 3700, Length: 262, Avg Reward: 172.382350032, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.26718640327\n",
      "[NOR] Episode: 3710, Length: 318, Avg Reward: 221.90721451, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.144091784954\n",
      "[NOR] Episode: 3720, Length: 228, Avg Reward: 203.821788229, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.301779270172\n",
      "[NOR] Episode: 3730, Length: 211, Avg Reward: 188.214492457, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.285686999559\n",
      "[NOR] Episode: 3740, Length: 359, Avg Reward: 174.630289201, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.18805897236\n",
      "[NOR] Episode: 3750, Length: 268, Avg Reward: 194.284090463, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.418561428785\n",
      "[NOR] Episode: 3760, Length: 263, Avg Reward: 180.3934997, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.464683383703\n",
      "[NOR] Episode: 3770, Length: 296, Avg Reward: 220.887579986, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0973585397005\n",
      "[NOR] Episode: 3780, Length: 384, Avg Reward: 189.397482376, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.425494670868\n",
      "[NOR] Episode: 3790, Length: 351, Avg Reward: 213.181203388, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0774405822158\n",
      "[NOR] Episode: 3800, Length: 206, Avg Reward: 192.805296953, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.45056653023\n",
      "[NOR] Episode: 3810, Length: 322, Avg Reward: 217.302268509, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.378451883793\n",
      "[NOR] Episode: 3820, Length: 273, Avg Reward: 224.990149399, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.833487689495\n",
      "[NOR] Episode: 3830, Length: 278, Avg Reward: 196.797847985, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.94643998146\n",
      "[NOR] Episode: 3840, Length: 295, Avg Reward: 228.917120263, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.08791613579\n",
      "[NOR] Episode: 3850, Length: 268, Avg Reward: 218.415077502, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.454524159431\n",
      "[NOR] Episode: 3860, Length: 308, Avg Reward: 225.814047069, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.21865022182\n",
      "[NOR] Episode: 3870, Length: 297, Avg Reward: 242.096420822, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.62843298912\n",
      "[NOR] Episode: 3880, Length: 268, Avg Reward: 222.742952076, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.52239668369\n",
      "[NOR] Episode: 3890, Length: 234, Avg Reward: 215.46696398, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.810633301735\n",
      "[NOR] Episode: 3900, Length: 300, Avg Reward: 222.04066497, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.412963986397\n",
      "[NOR] Episode: 3910, Length: 315, Avg Reward: 196.801032386, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.03867721558\n",
      "[NOR] Episode: 3920, Length: 237, Avg Reward: 184.521516359, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.361625224352\n",
      "[NOR] Episode: 3930, Length: 330, Avg Reward: 190.130729757, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.04954147339\n",
      "[NOR] Episode: 3940, Length: 254, Avg Reward: 199.53751559, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.151046425104\n",
      "[NOR] Episode: 3950, Length: 221, Avg Reward: 126.964386739, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.35387444496\n",
      "[NOR] Episode: 3960, Length: 279, Avg Reward: 178.427532944, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.477948248386\n",
      "[NOR] Episode: 3970, Length: 270, Avg Reward: 134.127751042, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.61192333698\n",
      "[NOR] Episode: 3980, Length: 256, Avg Reward: 171.047547795, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.230045691133\n",
      "[NOR] Episode: 3990, Length: 293, Avg Reward: 144.534488374, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.245584383607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 02:56:57,418] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video004000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 4000, Length: 233, Avg Reward: 176.658699281, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.21478867531\n",
      "[NOR] Episode: 4010, Length: 120, Avg Reward: 174.377655181, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.56213474274\n",
      "[NOR] Episode: 4020, Length: 243, Avg Reward: 162.380363589, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.769235193729\n",
      "[NOR] Episode: 4030, Length: 344, Avg Reward: 143.792913857, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.102557569742\n",
      "[NOR] Episode: 4040, Length: 366, Avg Reward: 192.278477142, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.65838599205\n",
      "[NOR] Episode: 4050, Length: 189, Avg Reward: 148.310425734, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.40101480484\n",
      "[NOR] Episode: 4060, Length: 264, Avg Reward: 77.7165886792, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.756788432598\n",
      "[NOR] Episode: 4070, Length: 303, Avg Reward: 203.336939419, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.148239910603\n",
      "[NOR] Episode: 4080, Length: 257, Avg Reward: 202.582896778, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.988606452942\n",
      "[NOR] Episode: 4090, Length: 315, Avg Reward: 197.45361804, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.767759382725\n",
      "[NOR] Episode: 4100, Length: 285, Avg Reward: 219.053258209, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.951543450356\n",
      "[NOR] Episode: 4110, Length: 269, Avg Reward: 212.348646196, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.493097215891\n",
      "[NOR] Episode: 4120, Length: 385, Avg Reward: 224.840660542, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.887570381165\n",
      "[NOR] Episode: 4130, Length: 315, Avg Reward: 208.423955445, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.239012107253\n",
      "[NOR] Episode: 4140, Length: 273, Avg Reward: 200.674762171, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0811696201563\n",
      "[NOR] Episode: 4150, Length: 284, Avg Reward: 215.225399915, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.675669431686\n",
      "[NOR] Episode: 4160, Length: 298, Avg Reward: 223.072370534, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.855040967464\n",
      "[NOR] Episode: 4170, Length: 315, Avg Reward: 208.688022961, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.381082296371\n",
      "[NOR] Episode: 4180, Length: 286, Avg Reward: 219.234949231, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.672426760197\n",
      "[NOR] Episode: 4190, Length: 351, Avg Reward: 209.595825306, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.636594831944\n",
      "[NOR] Episode: 4200, Length: 354, Avg Reward: 223.471706708, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.733718395233\n",
      "[NOR] Episode: 4210, Length: 281, Avg Reward: 227.042992151, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.490255236626\n",
      "[NOR] Episode: 4220, Length: 281, Avg Reward: 218.256821428, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0616723895073\n",
      "[NOR] Episode: 4230, Length: 316, Avg Reward: 213.019163151, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0861749947071\n",
      "[NOR] Episode: 4240, Length: 291, Avg Reward: 210.473596183, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.062775708735\n",
      "[NOR] Episode: 4250, Length: 383, Avg Reward: 215.954719848, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.524645864964\n",
      "[NOR] Episode: 4260, Length: 269, Avg Reward: 216.234274607, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.390985488892\n",
      "[NOR] Episode: 4270, Length: 261, Avg Reward: 230.669621198, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0594838857651\n",
      "[NOR] Episode: 4280, Length: 279, Avg Reward: 227.169230152, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.106414921582\n",
      "[NOR] Episode: 4290, Length: 323, Avg Reward: 223.901401665, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.650831699371\n",
      "[NOR] Episode: 4300, Length: 292, Avg Reward: 222.350505529, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.22826191783\n",
      "[NOR] Episode: 4310, Length: 248, Avg Reward: 226.17585766, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.332643449306\n",
      "[NOR] Episode: 4320, Length: 260, Avg Reward: 226.439208548, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.285089671612\n",
      "[NOR] Episode: 4330, Length: 306, Avg Reward: 198.361987373, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.570491790771\n",
      "[NOR] Episode: 4340, Length: 333, Avg Reward: 195.650016575, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.821690380573\n",
      "[NOR] Episode: 4350, Length: 271, Avg Reward: 211.497993124, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0645271539688\n",
      "[NOR] Episode: 4360, Length: 282, Avg Reward: 222.539342614, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.697772264481\n",
      "[NOR] Episode: 4370, Length: 368, Avg Reward: 184.415878326, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.38686561584\n",
      "[NOR] Episode: 4380, Length: 447, Avg Reward: 196.114605831, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.26905274391\n",
      "[NOR] Episode: 4390, Length: 232, Avg Reward: 206.015140029, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.698324859142\n",
      "[NOR] Episode: 4400, Length: 599, Avg Reward: 170.388129119, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.138840988278\n",
      "[NOR] Episode: 4410, Length: 300, Avg Reward: 206.566522277, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0582311302423\n",
      "[NOR] Episode: 4420, Length: 427, Avg Reward: 213.742031398, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.931258380413\n",
      "[NOR] Episode: 4430, Length: 310, Avg Reward: 213.388624146, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.00572812557\n",
      "[NOR] Episode: 4440, Length: 264, Avg Reward: 219.258074253, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.397699981928\n",
      "[NOR] Episode: 4450, Length: 263, Avg Reward: 202.191047625, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.270601809025\n",
      "[NOR] Episode: 4460, Length: 292, Avg Reward: 200.768238281, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.090551123023\n",
      "[NOR] Episode: 4470, Length: 237, Avg Reward: 220.421127456, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.257502168417\n",
      "[NOR] Episode: 4480, Length: 269, Avg Reward: 225.136012047, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.248898372054\n",
      "[NOR] Episode: 4490, Length: 236, Avg Reward: 207.01433103, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.107320874929\n",
      "[NOR] Episode: 4500, Length: 278, Avg Reward: 167.429240467, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.467522889376\n",
      "[NOR] Episode: 4510, Length: 290, Avg Reward: 228.616060558, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.50659352541\n",
      "[NOR] Episode: 4520, Length: 430, Avg Reward: 216.198372081, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.64429050684\n",
      "[NOR] Episode: 4530, Length: 473, Avg Reward: 206.443736338, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.498326182365\n",
      "[NOR] Episode: 4540, Length: 270, Avg Reward: 200.702373091, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.173125609756\n",
      "[NOR] Episode: 4550, Length: 300, Avg Reward: 223.626801007, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 2.27748394012\n",
      "[NOR] Episode: 4560, Length: 339, Avg Reward: 214.008100664, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.799166917801\n",
      "[NOR] Episode: 4570, Length: 267, Avg Reward: 194.031733274, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.27090263367\n",
      "[NOR] Episode: 4580, Length: 315, Avg Reward: 228.246283043, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.676954269409\n",
      "[NOR] Episode: 4590, Length: 258, Avg Reward: 218.143969908, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.199725747108\n",
      "[NOR] Episode: 4600, Length: 294, Avg Reward: 204.863078843, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.53493732214\n",
      "[NOR] Episode: 4610, Length: 239, Avg Reward: 195.854905139, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.388585448265\n",
      "[NOR] Episode: 4620, Length: 298, Avg Reward: 198.590272709, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.00673741102219\n",
      "[NOR] Episode: 4630, Length: 295, Avg Reward: 227.0755277, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.097092717886\n",
      "[NOR] Episode: 4640, Length: 295, Avg Reward: 221.792094505, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.310750067234\n",
      "[NOR] Episode: 4650, Length: 233, Avg Reward: 221.974025874, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.931510210037\n",
      "[NOR] Episode: 4660, Length: 256, Avg Reward: 218.419289308, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.541366815567\n",
      "[NOR] Episode: 4670, Length: 284, Avg Reward: 214.848470231, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.656154215336\n",
      "[NOR] Episode: 4680, Length: 570, Avg Reward: 187.516804978, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.13682629168\n",
      "[NOR] Episode: 4690, Length: 426, Avg Reward: 185.781547959, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.07032883167\n",
      "[NOR] Episode: 4700, Length: 500, Avg Reward: 172.824369193, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.300883352757\n",
      "[NOR] Episode: 4710, Length: 508, Avg Reward: 162.069165439, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0549749135971\n",
      "[NOR] Episode: 4720, Length: 325, Avg Reward: 210.113833667, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.107580006123\n",
      "[NOR] Episode: 4730, Length: 322, Avg Reward: 205.509531362, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.741759955883\n",
      "[NOR] Episode: 4740, Length: 410, Avg Reward: 193.12674389, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.704119682312\n",
      "[NOR] Episode: 4750, Length: 549, Avg Reward: 181.838216695, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.794825196266\n",
      "[NOR] Episode: 4760, Length: 474, Avg Reward: 192.732534841, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.251947581768\n",
      "[NOR] Episode: 4770, Length: 350, Avg Reward: 190.098910484, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.0556833744\n",
      "[NOR] Episode: 4780, Length: 403, Avg Reward: 209.184086496, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.762599110603\n",
      "[NOR] Episode: 4790, Length: 471, Avg Reward: 191.07044273, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.633713543415\n",
      "[NOR] Episode: 4800, Length: 309, Avg Reward: 223.853953648, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.795853614807\n",
      "[NOR] Episode: 4810, Length: 270, Avg Reward: 164.798061224, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.49551564455\n",
      "[NOR] Episode: 4820, Length: 524, Avg Reward: 187.448360022, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.466191411018\n",
      "[NOR] Episode: 4830, Length: 831, Avg Reward: 180.645257562, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.108112111688\n",
      "[NOR] Episode: 4840, Length: 365, Avg Reward: 183.988790339, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0972312018275\n",
      "[NOR] Episode: 4850, Length: 319, Avg Reward: 212.562979635, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.70941567421\n",
      "[NOR] Episode: 4860, Length: 490, Avg Reward: 186.243272673, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.703597903252\n",
      "[NOR] Episode: 4870, Length: 570, Avg Reward: 125.183975828, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0391538441181\n",
      "[NOR] Episode: 4880, Length: 335, Avg Reward: 113.674544637, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.251286417246\n",
      "[NOR] Episode: 4890, Length: 510, Avg Reward: 109.261398971, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.181449085474\n",
      "[NOR] Episode: 4900, Length: 672, Avg Reward: 146.726897452, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.384334534407\n",
      "[NOR] Episode: 4910, Length: 468, Avg Reward: 159.924947187, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.00520604848862\n",
      "[NOR] Episode: 4920, Length: 1000, Avg Reward: 36.876165737, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 2.07920956612\n",
      "[NOR] Episode: 4930, Length: 993, Avg Reward: 114.61142635, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.248696774244\n",
      "[NOR] Episode: 4940, Length: 299, Avg Reward: 111.098564077, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.678157806396\n",
      "[NOR] Episode: 4950, Length: 515, Avg Reward: 153.8685051, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.243080303073\n",
      "[NOR] Episode: 4960, Length: 360, Avg Reward: 67.0602157945, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.210505187511\n",
      "[NOR] Episode: 4970, Length: 235, Avg Reward: 153.197292693, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.350838124752\n",
      "[NOR] Episode: 4980, Length: 228, Avg Reward: 138.277887426, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.174116283655\n",
      "[NOR] Episode: 4990, Length: 449, Avg Reward: 207.963237324, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.32749176025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 03:19:27,958] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video005000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 5000, Length: 307, Avg Reward: 154.023435977, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.55990344286\n",
      "[NOR] Episode: 5010, Length: 711, Avg Reward: 171.04682546, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0824528485537\n",
      "[NOR] Episode: 5020, Length: 316, Avg Reward: 116.730321648, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.524769425392\n",
      "[NOR] Episode: 5030, Length: 457, Avg Reward: 199.843368775, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.426777362823\n",
      "[NOR] Episode: 5040, Length: 459, Avg Reward: 190.58298995, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.21442848444\n",
      "[NOR] Episode: 5050, Length: 383, Avg Reward: 149.232729199, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.447280943394\n",
      "[NOR] Episode: 5060, Length: 404, Avg Reward: 178.318789531, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.288961470127\n",
      "[NOR] Episode: 5070, Length: 1000, Avg Reward: 171.251713109, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.84737360477\n",
      "[NOR] Episode: 5080, Length: 1000, Avg Reward: 139.026656388, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.469760864973\n",
      "[NOR] Episode: 5090, Length: 560, Avg Reward: 174.258337949, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.580827832222\n",
      "[NOR] Episode: 5100, Length: 532, Avg Reward: 164.306642561, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0639591068029\n",
      "[NOR] Episode: 5110, Length: 562, Avg Reward: 143.906538086, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0355904698372\n",
      "[NOR] Episode: 5120, Length: 635, Avg Reward: 176.1222244, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.494983553886\n",
      "[NOR] Episode: 5130, Length: 349, Avg Reward: 132.349979935, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.64429974556\n",
      "[NOR] Episode: 5140, Length: 599, Avg Reward: 133.886070308, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.125665187836\n",
      "[NOR] Episode: 5150, Length: 393, Avg Reward: 178.256014821, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.27537882328\n",
      "[NOR] Episode: 5160, Length: 392, Avg Reward: 105.228583124, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.532596349716\n",
      "[NOR] Episode: 5170, Length: 531, Avg Reward: 125.614194733, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.305762559175\n",
      "[NOR] Episode: 5180, Length: 364, Avg Reward: 144.505145523, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.466674745083\n",
      "[NOR] Episode: 5190, Length: 355, Avg Reward: 204.620749965, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.361881673336\n",
      "[NOR] Episode: 5200, Length: 459, Avg Reward: 173.387081432, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.735199689865\n",
      "[NOR] Episode: 5210, Length: 286, Avg Reward: 118.482568913, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.186282634735\n",
      "[NOR] Episode: 5220, Length: 224, Avg Reward: 117.278520862, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.315344035625\n",
      "[NOR] Episode: 5230, Length: 215, Avg Reward: 127.54786064, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.319891542196\n",
      "[NOR] Episode: 5240, Length: 347, Avg Reward: 73.1539542854, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.44277203083\n",
      "[NOR] Episode: 5250, Length: 942, Avg Reward: 18.6467338114, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0296744760126\n",
      "[NOR] Episode: 5260, Length: 424, Avg Reward: 135.071142088, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0129234455526\n",
      "[NOR] Episode: 5270, Length: 411, Avg Reward: 84.0291838714, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0853786021471\n",
      "[NOR] Episode: 5280, Length: 225, Avg Reward: 32.073576777, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.171338111162\n",
      "[NOR] Episode: 5290, Length: 480, Avg Reward: 144.610439836, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.360462486744\n",
      "[NOR] Episode: 5300, Length: 321, Avg Reward: 199.318619222, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.421074450016\n",
      "[NOR] Episode: 5310, Length: 322, Avg Reward: 173.117239461, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.837395787239\n",
      "[NOR] Episode: 5320, Length: 391, Avg Reward: 203.482048126, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.475988954306\n",
      "[NOR] Episode: 5330, Length: 322, Avg Reward: 172.66939867, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.48572665453\n",
      "[NOR] Episode: 5340, Length: 578, Avg Reward: 135.794525518, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.30540394783\n",
      "[NOR] Episode: 5350, Length: 495, Avg Reward: 179.133444011, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.30854010582\n",
      "[NOR] Episode: 5360, Length: 592, Avg Reward: 181.865942414, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.982850432396\n",
      "[NOR] Episode: 5370, Length: 585, Avg Reward: 166.439270036, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.281998723745\n",
      "[NOR] Episode: 5380, Length: 649, Avg Reward: 176.978217789, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.01704573631\n",
      "[NOR] Episode: 5390, Length: 597, Avg Reward: 178.1516246, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.453063517809\n",
      "[NOR] Episode: 5400, Length: 568, Avg Reward: 173.24951087, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.494039952755\n",
      "[NOR] Episode: 5410, Length: 564, Avg Reward: 167.42266451, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.534239113331\n",
      "[NOR] Episode: 5420, Length: 667, Avg Reward: 186.623604176, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0537378042936\n",
      "[NOR] Episode: 5430, Length: 594, Avg Reward: 167.064944043, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.857608735561\n",
      "[NOR] Episode: 5440, Length: 660, Avg Reward: 156.038169027, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.167085319757\n",
      "[NOR] Episode: 5450, Length: 414, Avg Reward: 155.044244818, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.729355752468\n",
      "[NOR] Episode: 5460, Length: 430, Avg Reward: 197.211361402, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.108331367373\n",
      "[NOR] Episode: 5470, Length: 455, Avg Reward: 200.367099547, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.569587290287\n",
      "[NOR] Episode: 5480, Length: 272, Avg Reward: 176.421022601, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.14132989943\n",
      "[NOR] Episode: 5490, Length: 832, Avg Reward: 161.623764079, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.752177834511\n",
      "[NOR] Episode: 5500, Length: 164, Avg Reward: 161.764714015, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.232715174556\n",
      "[NOR] Episode: 5510, Length: 908, Avg Reward: 166.38641815, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.43758440018\n",
      "[NOR] Episode: 5520, Length: 470, Avg Reward: 130.171210218, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.30695676804\n",
      "[NOR] Episode: 5530, Length: 286, Avg Reward: 178.968190378, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.706613361835\n",
      "[NOR] Episode: 5540, Length: 1000, Avg Reward: 137.491124576, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.171537846327\n",
      "[NOR] Episode: 5550, Length: 1000, Avg Reward: 35.0493129965, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.264872759581\n",
      "[NOR] Episode: 5560, Length: 325, Avg Reward: 67.0487643384, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.841014862061\n",
      "[NOR] Episode: 5570, Length: 465, Avg Reward: 85.9796634033, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.369967460632\n",
      "[NOR] Episode: 5580, Length: 152, Avg Reward: 88.5738733319, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.076437741518\n",
      "[NOR] Episode: 5590, Length: 1000, Avg Reward: 95.9317073969, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.30646312237\n",
      "[NOR] Episode: 5600, Length: 1000, Avg Reward: 121.424927555, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.400992333889\n",
      "[NOR] Episode: 5610, Length: 1000, Avg Reward: 114.458245998, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.876362204552\n",
      "[NOR] Episode: 5620, Length: 1000, Avg Reward: 16.6915822226, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.599943220615\n",
      "[NOR] Episode: 5630, Length: 928, Avg Reward: 101.819012509, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.504790782928\n",
      "[NOR] Episode: 5640, Length: 816, Avg Reward: 147.316387253, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.106196701527\n",
      "[NOR] Episode: 5650, Length: 543, Avg Reward: 177.414192702, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.986447572708\n",
      "[NOR] Episode: 5660, Length: 629, Avg Reward: 140.636351544, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.48656702042\n",
      "[NOR] Episode: 5670, Length: 343, Avg Reward: 172.663548929, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.972585320473\n",
      "[NOR] Episode: 5680, Length: 224, Avg Reward: 193.6357574, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.313222765923\n",
      "[NOR] Episode: 5690, Length: 616, Avg Reward: 155.043274356, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.00201427937\n",
      "[NOR] Episode: 5700, Length: 630, Avg Reward: 160.828189044, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.12161457539\n",
      "[NOR] Episode: 5710, Length: 566, Avg Reward: 169.247037318, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.145939230919\n",
      "[NOR] Episode: 5720, Length: 498, Avg Reward: 178.334930759, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.583927571774\n",
      "[NOR] Episode: 5730, Length: 479, Avg Reward: 189.95373247, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.758607387543\n",
      "[NOR] Episode: 5740, Length: 430, Avg Reward: 191.65281105, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0163041949272\n",
      "[NOR] Episode: 5750, Length: 477, Avg Reward: 179.56152856, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.37659418583\n",
      "[NOR] Episode: 5760, Length: 428, Avg Reward: 186.686143659, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.549336910248\n",
      "[NOR] Episode: 5770, Length: 327, Avg Reward: 202.276130331, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.65647947788\n",
      "[NOR] Episode: 5780, Length: 388, Avg Reward: 216.197070904, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.216846302152\n",
      "[NOR] Episode: 5790, Length: 481, Avg Reward: 189.942747942, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.844948470592\n",
      "[NOR] Episode: 5800, Length: 400, Avg Reward: 216.588983552, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -16.9475898743\n",
      "[NOR] Episode: 5810, Length: 466, Avg Reward: 209.06026238, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0491835474968\n",
      "[NOR] Episode: 5820, Length: 405, Avg Reward: 193.867460675, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0709181427956\n",
      "[NOR] Episode: 5830, Length: 345, Avg Reward: 204.836206479, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.88931375742\n",
      "[NOR] Episode: 5840, Length: 316, Avg Reward: 204.004355708, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.398412972689\n",
      "[NOR] Episode: 5850, Length: 358, Avg Reward: 209.412977897, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.162162348628\n",
      "[NOR] Episode: 5860, Length: 388, Avg Reward: 208.586374952, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.83487272263\n",
      "[NOR] Episode: 5870, Length: 275, Avg Reward: 215.971207168, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.523189485073\n",
      "[NOR] Episode: 5880, Length: 318, Avg Reward: 210.017489074, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.03324627876\n",
      "[NOR] Episode: 5890, Length: 282, Avg Reward: 221.559430552, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.480802357197\n",
      "[NOR] Episode: 5900, Length: 354, Avg Reward: 215.788687965, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.447563022375\n",
      "[NOR] Episode: 5910, Length: 345, Avg Reward: 222.089098435, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.13472747803\n",
      "[NOR] Episode: 5920, Length: 273, Avg Reward: 214.045216065, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.92309081554\n",
      "[NOR] Episode: 5930, Length: 330, Avg Reward: 214.850841053, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.277436554432\n",
      "[NOR] Episode: 5940, Length: 270, Avg Reward: 201.107691684, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -17.9290981293\n",
      "[NOR] Episode: 5950, Length: 379, Avg Reward: 181.942249585, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.20336651802\n",
      "[NOR] Episode: 5960, Length: 302, Avg Reward: 215.570511709, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.677670478821\n",
      "[NOR] Episode: 5970, Length: 323, Avg Reward: 206.725597569, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.14936363697\n",
      "[NOR] Episode: 5980, Length: 319, Avg Reward: 171.880382239, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.27423250675\n",
      "[NOR] Episode: 5990, Length: 249, Avg Reward: 192.566419081, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.597236335278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 03:52:02,647] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video006000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 6000, Length: 164, Avg Reward: 158.018954451, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.337623983622\n",
      "[NOR] Episode: 6010, Length: 431, Avg Reward: 206.344613971, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.389557719231\n",
      "[NOR] Episode: 6020, Length: 277, Avg Reward: 161.106274687, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.75872451067\n",
      "[NOR] Episode: 6030, Length: 310, Avg Reward: 201.376333989, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.45432555676\n",
      "[NOR] Episode: 6040, Length: 318, Avg Reward: 220.77635926, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.253580302\n",
      "[NOR] Episode: 6050, Length: 309, Avg Reward: 204.055070355, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.691224694252\n",
      "[NOR] Episode: 6060, Length: 384, Avg Reward: 213.892085491, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0146597623825\n",
      "[NOR] Episode: 6070, Length: 400, Avg Reward: 209.223471892, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.888020277023\n",
      "[NOR] Episode: 6080, Length: 346, Avg Reward: 205.710082022, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.215509384871\n",
      "[NOR] Episode: 6090, Length: 328, Avg Reward: 216.526273122, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.660261392593\n",
      "[NOR] Episode: 6100, Length: 323, Avg Reward: 215.683415191, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.56861627102\n",
      "[NOR] Episode: 6110, Length: 418, Avg Reward: 204.555083977, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.14408040047\n",
      "[NOR] Episode: 6120, Length: 408, Avg Reward: 202.929871442, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.111934006214\n",
      "[NOR] Episode: 6130, Length: 338, Avg Reward: 209.995822786, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.570964217186\n",
      "[NOR] Episode: 6140, Length: 397, Avg Reward: 198.837596063, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.546055853367\n",
      "[NOR] Episode: 6150, Length: 336, Avg Reward: 209.215095072, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.02441966534\n",
      "[NOR] Episode: 6160, Length: 385, Avg Reward: 222.453788217, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.45642614365\n",
      "[NOR] Episode: 6170, Length: 417, Avg Reward: 178.26220188, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 2.07350158691\n",
      "[NOR] Episode: 6180, Length: 413, Avg Reward: 201.942562961, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.01704132557\n",
      "[NOR] Episode: 6190, Length: 871, Avg Reward: 200.277390853, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.886206805706\n",
      "[NOR] Episode: 6200, Length: 283, Avg Reward: 205.316898337, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.09094202518\n",
      "[NOR] Episode: 6210, Length: 318, Avg Reward: 171.184006747, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.1776060462\n",
      "[NOR] Episode: 6220, Length: 446, Avg Reward: 159.605382109, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.840991258621\n",
      "[NOR] Episode: 6230, Length: 426, Avg Reward: 167.250839625, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.842981934547\n",
      "[NOR] Episode: 6240, Length: 746, Avg Reward: 158.276293905, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.193840697408\n",
      "[NOR] Episode: 6250, Length: 349, Avg Reward: 134.232324929, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.628591775894\n",
      "[NOR] Episode: 6260, Length: 392, Avg Reward: 180.702538353, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.86720764637\n",
      "[NOR] Episode: 6270, Length: 307, Avg Reward: 117.944621376, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.482119381428\n",
      "[NOR] Episode: 6280, Length: 291, Avg Reward: 113.604448005, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.371436715126\n",
      "[NOR] Episode: 6290, Length: 251, Avg Reward: 122.344969087, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.28862500191\n",
      "[NOR] Episode: 6300, Length: 317, Avg Reward: 133.000585792, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.78186583519\n",
      "[NOR] Episode: 6310, Length: 159, Avg Reward: 98.4788309111, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.450329989195\n",
      "[NOR] Episode: 6320, Length: 207, Avg Reward: 36.9770525128, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.628742098808\n",
      "[NOR] Episode: 6330, Length: 191, Avg Reward: -31.663895961, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0850793495774\n",
      "[NOR] Episode: 6340, Length: 123, Avg Reward: 125.522748536, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.153496235609\n",
      "[NOR] Episode: 6350, Length: 176, Avg Reward: 18.8041830339, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.869066476822\n",
      "[NOR] Episode: 6360, Length: 305, Avg Reward: 91.7756064136, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.298459708691\n",
      "[NOR] Episode: 6370, Length: 283, Avg Reward: 21.4194789372, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.13611900806\n",
      "[NOR] Episode: 6380, Length: 174, Avg Reward: 26.1541983341, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.650147080421\n",
      "[NOR] Episode: 6390, Length: 110, Avg Reward: 8.85000120357, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.159656360745\n",
      "[NOR] Episode: 6400, Length: 207, Avg Reward: 38.2710381727, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.592337369919\n",
      "[NOR] Episode: 6410, Length: 337, Avg Reward: 77.7418577379, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.9061820507\n",
      "[NOR] Episode: 6420, Length: 216, Avg Reward: 28.0359247516, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.11568593979\n",
      "[NOR] Episode: 6430, Length: 212, Avg Reward: 4.02905059376, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.392942458391\n",
      "[NOR] Episode: 6440, Length: 424, Avg Reward: 100.254535737, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0674983859062\n",
      "[NOR] Episode: 6450, Length: 331, Avg Reward: 137.443659175, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.406438142061\n",
      "[NOR] Episode: 6460, Length: 425, Avg Reward: 166.560667179, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.247305706143\n",
      "[NOR] Episode: 6470, Length: 414, Avg Reward: 173.856580424, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.260226309299\n",
      "[NOR] Episode: 6480, Length: 427, Avg Reward: 183.039940493, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.817099988461\n",
      "[NOR] Episode: 6490, Length: 354, Avg Reward: 204.078232073, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -3.19451999664\n",
      "[NOR] Episode: 6500, Length: 430, Avg Reward: 196.237970548, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.29799604416\n",
      "[NOR] Episode: 6510, Length: 366, Avg Reward: 203.935899736, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.25622808933\n",
      "[NOR] Episode: 6520, Length: 417, Avg Reward: 211.606911017, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.490681320429\n",
      "[NOR] Episode: 6530, Length: 352, Avg Reward: 207.578588393, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.539099872112\n",
      "[NOR] Episode: 6540, Length: 355, Avg Reward: 199.362713896, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.907496809959\n",
      "[NOR] Episode: 6550, Length: 339, Avg Reward: 220.312065557, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.362860947847\n",
      "[NOR] Episode: 6560, Length: 351, Avg Reward: 212.089471676, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.39810281992\n",
      "[NOR] Episode: 6570, Length: 301, Avg Reward: 213.66320016, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.649608969688\n",
      "[NOR] Episode: 6580, Length: 320, Avg Reward: 220.202331912, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.389815807343\n",
      "[NOR] Episode: 6590, Length: 360, Avg Reward: 202.719913777, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.287068784237\n",
      "[NOR] Episode: 6600, Length: 316, Avg Reward: 217.763166177, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.916429579258\n",
      "[NOR] Episode: 6610, Length: 359, Avg Reward: 207.485475496, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.12857496738\n",
      "[NOR] Episode: 6620, Length: 305, Avg Reward: 224.173046946, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.36088144779\n",
      "[NOR] Episode: 6630, Length: 435, Avg Reward: 219.857408168, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.675885438919\n",
      "[NOR] Episode: 6640, Length: 351, Avg Reward: 219.88561019, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.941904485226\n",
      "[NOR] Episode: 6650, Length: 295, Avg Reward: 213.306637385, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.29108351469\n",
      "[NOR] Episode: 6660, Length: 278, Avg Reward: 224.453863501, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.907357096672\n",
      "[NOR] Episode: 6670, Length: 345, Avg Reward: 232.4318638, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.651695251465\n",
      "[NOR] Episode: 6680, Length: 304, Avg Reward: 221.127153638, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.652323126793\n",
      "[NOR] Episode: 6690, Length: 299, Avg Reward: 225.533358583, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.58710950613\n",
      "[NOR] Episode: 6700, Length: 433, Avg Reward: 220.562254235, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -17.2090816498\n",
      "[NOR] Episode: 6710, Length: 332, Avg Reward: 221.154977178, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.58444714546\n",
      "[NOR] Episode: 6720, Length: 266, Avg Reward: 221.512963219, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 2.61765646935\n",
      "[NOR] Episode: 6730, Length: 316, Avg Reward: 197.29372643, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.7449914217\n",
      "[NOR] Episode: 6740, Length: 327, Avg Reward: 225.104394559, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.78513932228\n",
      "[NOR] Episode: 6750, Length: 389, Avg Reward: 216.725100951, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.40497875214\n",
      "[NOR] Episode: 6760, Length: 377, Avg Reward: 209.73346558, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.781148731709\n",
      "[NOR] Episode: 6770, Length: 322, Avg Reward: 206.389211868, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.08830761909\n",
      "[NOR] Episode: 6780, Length: 576, Avg Reward: 205.065376224, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0206283628941\n",
      "[NOR] Episode: 6790, Length: 269, Avg Reward: 194.36718422, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.677002847195\n",
      "[NOR] Episode: 6800, Length: 309, Avg Reward: 219.65734661, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.3521323204\n",
      "[NOR] Episode: 6810, Length: 330, Avg Reward: 216.839379672, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.481228768826\n",
      "[NOR] Episode: 6820, Length: 384, Avg Reward: 214.058690083, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.690633416176\n",
      "[NOR] Episode: 6830, Length: 398, Avg Reward: 198.51306043, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.570122122765\n",
      "[NOR] Episode: 6840, Length: 364, Avg Reward: 191.498408998, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.866221189499\n",
      "[NOR] Episode: 6850, Length: 306, Avg Reward: 218.01843963, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.14321541786\n",
      "[NOR] Episode: 6860, Length: 319, Avg Reward: 192.870516167, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.416502326727\n",
      "[NOR] Episode: 6870, Length: 598, Avg Reward: 171.883962691, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.965306818485\n",
      "[NOR] Episode: 6880, Length: 304, Avg Reward: 201.952160723, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.46408188343\n",
      "[NOR] Episode: 6890, Length: 311, Avg Reward: 176.089041202, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.694886744022\n",
      "[NOR] Episode: 6900, Length: 431, Avg Reward: 217.984973164, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.12548983097\n",
      "[NOR] Episode: 6910, Length: 447, Avg Reward: 192.932773774, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0391995459795\n",
      "[NOR] Episode: 6920, Length: 665, Avg Reward: 190.927538085, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.348362326622\n",
      "[NOR] Episode: 6930, Length: 328, Avg Reward: 203.049350432, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.180920183659\n",
      "[NOR] Episode: 6940, Length: 557, Avg Reward: 179.557321284, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 3.10453367233\n",
      "[NOR] Episode: 6950, Length: 420, Avg Reward: 190.157411067, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.61690092087\n",
      "[NOR] Episode: 6960, Length: 348, Avg Reward: 181.496991444, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.981690168381\n",
      "[NOR] Episode: 6970, Length: 316, Avg Reward: 193.160085393, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.564701020718\n",
      "[NOR] Episode: 6980, Length: 295, Avg Reward: 210.680024924, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.322477132082\n",
      "[NOR] Episode: 6990, Length: 329, Avg Reward: 178.629115882, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.82338738441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 04:14:48,691] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video007000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 7000, Length: 317, Avg Reward: 198.351995221, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.70604014397\n",
      "[NOR] Episode: 7010, Length: 505, Avg Reward: 156.885776462, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.684836566448\n",
      "[NOR] Episode: 7020, Length: 353, Avg Reward: 185.102464804, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.23024463654\n",
      "[NOR] Episode: 7030, Length: 197, Avg Reward: 160.9582096, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.619274556637\n",
      "[NOR] Episode: 7040, Length: 341, Avg Reward: 183.859810617, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.186781138182\n",
      "[NOR] Episode: 7050, Length: 413, Avg Reward: 180.210002317, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.256481766701\n",
      "[NOR] Episode: 7060, Length: 514, Avg Reward: 203.908873315, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.568216502666\n",
      "[NOR] Episode: 7070, Length: 341, Avg Reward: 197.190153905, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.37903308868\n",
      "[NOR] Episode: 7080, Length: 757, Avg Reward: 185.785993999, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.71657657623\n",
      "[NOR] Episode: 7090, Length: 296, Avg Reward: 141.960994431, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.584122955799\n",
      "[NOR] Episode: 7100, Length: 363, Avg Reward: 217.542481045, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.34670472145\n",
      "[NOR] Episode: 7110, Length: 329, Avg Reward: 196.973420102, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.668075561523\n",
      "[NOR] Episode: 7120, Length: 347, Avg Reward: 183.593502783, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0602616071701\n",
      "[NOR] Episode: 7130, Length: 1000, Avg Reward: 156.883162318, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.147293180227\n",
      "[NOR] Episode: 7140, Length: 372, Avg Reward: 196.315332876, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.45772838593\n",
      "[NOR] Episode: 7150, Length: 276, Avg Reward: 201.751495977, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.70336651802\n",
      "[NOR] Episode: 7160, Length: 360, Avg Reward: 152.230851762, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.810612559319\n",
      "[NOR] Episode: 7170, Length: 292, Avg Reward: 135.557184753, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.120585620403\n",
      "[NOR] Episode: 7180, Length: 244, Avg Reward: 162.925944125, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.0197353363\n",
      "[NOR] Episode: 7190, Length: 238, Avg Reward: 156.038407941, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.657872617245\n",
      "[NOR] Episode: 7200, Length: 319, Avg Reward: 156.676634391, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.683185517788\n",
      "[NOR] Episode: 7210, Length: 316, Avg Reward: 167.768088955, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.624347805977\n",
      "[NOR] Episode: 7220, Length: 461, Avg Reward: 136.144051313, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0396754741669\n",
      "[NOR] Episode: 7230, Length: 326, Avg Reward: 81.3737509449, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.794432640076\n",
      "[NOR] Episode: 7240, Length: 539, Avg Reward: 131.246742755, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.44648134708\n",
      "[NOR] Episode: 7250, Length: 552, Avg Reward: 140.010478526, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.1379878521\n",
      "[NOR] Episode: 7260, Length: 584, Avg Reward: 112.842977299, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.03959178925\n",
      "[NOR] Episode: 7270, Length: 647, Avg Reward: 144.797761639, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.412151902914\n",
      "[NOR] Episode: 7280, Length: 955, Avg Reward: 141.542548876, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.39725920558\n",
      "[NOR] Episode: 7290, Length: 681, Avg Reward: 79.4153216198, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.00141188502312\n",
      "[NOR] Episode: 7300, Length: 849, Avg Reward: 129.825735115, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.35629367828\n",
      "[NOR] Episode: 7310, Length: 552, Avg Reward: 133.191885475, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.595930397511\n",
      "[NOR] Episode: 7320, Length: 528, Avg Reward: 163.749794964, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.00686238706112\n",
      "[NOR] Episode: 7330, Length: 580, Avg Reward: 160.360487645, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.16054964066\n",
      "[NOR] Episode: 7340, Length: 529, Avg Reward: 159.969769975, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -3.20426726341\n",
      "[NOR] Episode: 7350, Length: 605, Avg Reward: 185.338891416, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.78543770313\n",
      "[NOR] Episode: 7360, Length: 543, Avg Reward: 182.471141023, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.26764631271\n",
      "[NOR] Episode: 7370, Length: 413, Avg Reward: 186.262299556, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.43846213818\n",
      "[NOR] Episode: 7380, Length: 666, Avg Reward: 184.06921469, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.537810623646\n",
      "[NOR] Episode: 7390, Length: 329, Avg Reward: 202.827491162, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.765303492546\n",
      "[NOR] Episode: 7400, Length: 486, Avg Reward: 209.515056421, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.794719338417\n",
      "[NOR] Episode: 7410, Length: 296, Avg Reward: 211.762702307, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.753886222839\n",
      "[NOR] Episode: 7420, Length: 308, Avg Reward: 206.760180321, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.207101941109\n",
      "[NOR] Episode: 7430, Length: 315, Avg Reward: 200.449932255, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.000896751880646\n",
      "[NOR] Episode: 7440, Length: 277, Avg Reward: 209.668835839, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.24786400795\n",
      "[NOR] Episode: 7450, Length: 364, Avg Reward: 180.705734384, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.233064576983\n",
      "[NOR] Episode: 7460, Length: 376, Avg Reward: 207.94371912, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.173182100058\n",
      "[NOR] Episode: 7470, Length: 473, Avg Reward: 204.460504155, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.216935575008\n",
      "[NOR] Episode: 7480, Length: 387, Avg Reward: 208.622938317, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.27371501923\n",
      "[NOR] Episode: 7490, Length: 310, Avg Reward: 215.329283515, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 3.13158464432\n",
      "[NOR] Episode: 7500, Length: 340, Avg Reward: 203.684041318, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.543526947498\n",
      "[NOR] Episode: 7510, Length: 422, Avg Reward: 198.261364886, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.01449608803\n",
      "[NOR] Episode: 7520, Length: 318, Avg Reward: 205.649053594, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.15844368935\n",
      "[NOR] Episode: 7530, Length: 376, Avg Reward: 199.945276764, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.24076294899\n",
      "[NOR] Episode: 7540, Length: 1000, Avg Reward: 158.432374744, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 2.08013820648\n",
      "[NOR] Episode: 7550, Length: 824, Avg Reward: 167.943178177, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.33245849609\n",
      "[NOR] Episode: 7560, Length: 1000, Avg Reward: 173.728131913, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.88086628914\n",
      "[NOR] Episode: 7570, Length: 717, Avg Reward: 179.842824387, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.159827083349\n",
      "[NOR] Episode: 7580, Length: 760, Avg Reward: 186.094753945, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.197722047567\n",
      "[NOR] Episode: 7590, Length: 271, Avg Reward: 208.767839527, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.14390575886\n",
      "[NOR] Episode: 7600, Length: 390, Avg Reward: 214.917572095, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0986824631691\n",
      "[NOR] Episode: 7610, Length: 314, Avg Reward: 216.507382054, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.033927038312\n",
      "[NOR] Episode: 7620, Length: 302, Avg Reward: 214.035712748, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.50940704346\n",
      "[NOR] Episode: 7630, Length: 268, Avg Reward: 216.329362925, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.27650296688\n",
      "[NOR] Episode: 7640, Length: 330, Avg Reward: 211.512966333, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.165107905865\n",
      "[NOR] Episode: 7650, Length: 422, Avg Reward: 205.861443854, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.418961465359\n",
      "[NOR] Episode: 7660, Length: 383, Avg Reward: 206.318009226, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.027613312006\n",
      "[NOR] Episode: 7670, Length: 333, Avg Reward: 220.549892284, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.208916649222\n",
      "[NOR] Episode: 7680, Length: 312, Avg Reward: 217.410430443, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.329420953989\n",
      "[NOR] Episode: 7690, Length: 330, Avg Reward: 204.04134552, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.18003165722\n",
      "[NOR] Episode: 7700, Length: 384, Avg Reward: 213.930398044, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.491086125374\n",
      "[NOR] Episode: 7710, Length: 366, Avg Reward: 208.060185951, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.543669223785\n",
      "[NOR] Episode: 7720, Length: 285, Avg Reward: 210.504108015, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.204795688391\n",
      "[NOR] Episode: 7730, Length: 301, Avg Reward: 214.977466451, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.010804772377\n",
      "[NOR] Episode: 7740, Length: 240, Avg Reward: 227.375742701, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.769015192986\n",
      "[NOR] Episode: 7750, Length: 322, Avg Reward: 223.691074982, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.968207657337\n",
      "[NOR] Episode: 7760, Length: 272, Avg Reward: 213.701040464, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.278345167637\n",
      "[NOR] Episode: 7770, Length: 376, Avg Reward: 186.501659813, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.550602555275\n",
      "[NOR] Episode: 7780, Length: 341, Avg Reward: 234.458029571, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.204499334097\n",
      "[NOR] Episode: 7790, Length: 253, Avg Reward: 207.061113221, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.101341232657\n",
      "[NOR] Episode: 7800, Length: 247, Avg Reward: 217.242874628, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.862944126129\n",
      "[NOR] Episode: 7810, Length: 380, Avg Reward: 216.510807007, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.42031240463\n",
      "[NOR] Episode: 7820, Length: 360, Avg Reward: 220.438902563, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.880784332752\n",
      "[NOR] Episode: 7830, Length: 349, Avg Reward: 209.268414773, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.58596765995\n",
      "[NOR] Episode: 7840, Length: 321, Avg Reward: 202.926228911, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.775403499603\n",
      "[NOR] Episode: 7850, Length: 410, Avg Reward: 186.905377558, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.00990292429924\n",
      "[NOR] Episode: 7860, Length: 472, Avg Reward: 193.88325484, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.349271804094\n",
      "[NOR] Episode: 7870, Length: 289, Avg Reward: 184.69497738, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0588329732418\n",
      "[NOR] Episode: 7880, Length: 490, Avg Reward: 214.640403414, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.863882958889\n",
      "[NOR] Episode: 7890, Length: 319, Avg Reward: 216.270162085, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.840397715569\n",
      "[NOR] Episode: 7900, Length: 400, Avg Reward: 210.371917405, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.769843876362\n",
      "[NOR] Episode: 7910, Length: 416, Avg Reward: 193.553255223, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.106038220227\n",
      "[NOR] Episode: 7920, Length: 381, Avg Reward: 184.567984205, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.445593833923\n",
      "[NOR] Episode: 7930, Length: 415, Avg Reward: 191.393059462, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.00777971745\n",
      "[NOR] Episode: 7940, Length: 599, Avg Reward: 175.019209861, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.225943982601\n",
      "[NOR] Episode: 7950, Length: 598, Avg Reward: 162.553463098, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.15226984024\n",
      "[NOR] Episode: 7960, Length: 537, Avg Reward: 174.831539871, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.732319235802\n",
      "[NOR] Episode: 7970, Length: 403, Avg Reward: 193.403107546, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.847088217735\n",
      "[NOR] Episode: 7980, Length: 412, Avg Reward: 196.50206995, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.830355405807\n",
      "[NOR] Episode: 7990, Length: 377, Avg Reward: 199.154897193, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.396078646183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 04:43:23,446] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video008000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 8000, Length: 322, Avg Reward: 193.255776059, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.27429035306\n",
      "[NOR] Episode: 8010, Length: 312, Avg Reward: 204.967344088, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.783667206764\n",
      "[NOR] Episode: 8020, Length: 328, Avg Reward: 207.874787064, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.542711019516\n",
      "[NOR] Episode: 8030, Length: 529, Avg Reward: 212.622782263, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.21141338348\n",
      "[NOR] Episode: 8040, Length: 649, Avg Reward: 208.434696595, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -8.94071674347\n",
      "[NOR] Episode: 8050, Length: 319, Avg Reward: 180.965328985, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.0081486702\n",
      "[NOR] Episode: 8060, Length: 276, Avg Reward: 215.494580145, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0888994038105\n",
      "[NOR] Episode: 8070, Length: 283, Avg Reward: 219.791781153, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.629849910736\n",
      "[NOR] Episode: 8080, Length: 293, Avg Reward: 172.498307466, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0801903605461\n",
      "[NOR] Episode: 8090, Length: 257, Avg Reward: 134.06909894, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.870555400848\n",
      "[NOR] Episode: 8100, Length: 282, Avg Reward: 184.726112651, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.113929465413\n",
      "[NOR] Episode: 8110, Length: 327, Avg Reward: 206.966669097, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.397770732641\n",
      "[NOR] Episode: 8120, Length: 491, Avg Reward: 149.438737049, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.25388753414\n",
      "[NOR] Episode: 8130, Length: 355, Avg Reward: 204.009442328, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.490583091974\n",
      "[NOR] Episode: 8140, Length: 302, Avg Reward: 216.602169191, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.604401826859\n",
      "[NOR] Episode: 8150, Length: 383, Avg Reward: 214.740262807, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0382460281253\n",
      "[NOR] Episode: 8160, Length: 419, Avg Reward: 197.405698747, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.675217270851\n",
      "[NOR] Episode: 8170, Length: 326, Avg Reward: 202.518485932, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.12267971039\n",
      "[NOR] Episode: 8180, Length: 336, Avg Reward: 212.518000069, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.458998024464\n",
      "[NOR] Episode: 8190, Length: 313, Avg Reward: 212.730700264, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.741640329361\n",
      "[NOR] Episode: 8200, Length: 280, Avg Reward: 210.183446033, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.947142481804\n",
      "[NOR] Episode: 8210, Length: 293, Avg Reward: 219.5816426, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0473768264055\n",
      "[NOR] Episode: 8220, Length: 279, Avg Reward: 204.311759318, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.696040987968\n",
      "[NOR] Episode: 8230, Length: 313, Avg Reward: 202.336513572, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.359356164932\n",
      "[NOR] Episode: 8240, Length: 299, Avg Reward: 208.811376132, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.821797370911\n",
      "[NOR] Episode: 8250, Length: 355, Avg Reward: 201.101107237, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0441749468446\n",
      "[NOR] Episode: 8260, Length: 409, Avg Reward: 180.259235444, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.647992610931\n",
      "[NOR] Episode: 8270, Length: 394, Avg Reward: 190.818587732, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.775740206242\n",
      "[NOR] Episode: 8280, Length: 452, Avg Reward: 196.292646118, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.655073523521\n",
      "[NOR] Episode: 8290, Length: 300, Avg Reward: 190.279621598, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.147216677666\n",
      "[NOR] Episode: 8300, Length: 530, Avg Reward: 200.823741477, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.638804674149\n",
      "[NOR] Episode: 8310, Length: 413, Avg Reward: 201.925468207, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.481947273016\n",
      "[NOR] Episode: 8320, Length: 314, Avg Reward: 198.7910183, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.302450537682\n",
      "[NOR] Episode: 8330, Length: 281, Avg Reward: 204.291938529, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.467486798763\n",
      "[NOR] Episode: 8340, Length: 380, Avg Reward: 152.765067449, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.297596991062\n",
      "[NOR] Episode: 8350, Length: 354, Avg Reward: 211.184393895, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.879063665867\n",
      "[NOR] Episode: 8360, Length: 252, Avg Reward: 214.055730638, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.571409702301\n",
      "[NOR] Episode: 8370, Length: 467, Avg Reward: 185.609642854, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 3.71425008774\n",
      "[NOR] Episode: 8380, Length: 282, Avg Reward: 179.573294243, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.6409329772\n",
      "[NOR] Episode: 8390, Length: 422, Avg Reward: 181.83063003, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.753107130527\n",
      "[NOR] Episode: 8400, Length: 339, Avg Reward: 141.972374829, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.24372947216\n",
      "[NOR] Episode: 8410, Length: 288, Avg Reward: 185.040419231, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.806844234467\n",
      "[NOR] Episode: 8420, Length: 639, Avg Reward: 165.727504458, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.690804064274\n",
      "[NOR] Episode: 8430, Length: 636, Avg Reward: 161.96536344, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.986515164375\n",
      "[NOR] Episode: 8440, Length: 588, Avg Reward: 192.26003783, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.278169393539\n",
      "[NOR] Episode: 8450, Length: 663, Avg Reward: 139.201510293, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.775536298752\n",
      "[NOR] Episode: 8460, Length: 560, Avg Reward: 160.998457171, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.387234985828\n",
      "[NOR] Episode: 8470, Length: 654, Avg Reward: 185.654299815, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.180517286062\n",
      "[NOR] Episode: 8480, Length: 349, Avg Reward: 180.860543543, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.120425567031\n",
      "[NOR] Episode: 8490, Length: 588, Avg Reward: 194.242043725, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.692636907101\n",
      "[NOR] Episode: 8500, Length: 445, Avg Reward: 200.383920365, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.338383227587\n",
      "[NOR] Episode: 8510, Length: 713, Avg Reward: 176.137299174, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.387533932924\n",
      "[NOR] Episode: 8520, Length: 950, Avg Reward: 160.452420695, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.214809656143\n",
      "[NOR] Episode: 8530, Length: 510, Avg Reward: 168.221874751, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.210840672255\n",
      "[NOR] Episode: 8540, Length: 438, Avg Reward: 174.253349438, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.324030578136\n",
      "[NOR] Episode: 8550, Length: 444, Avg Reward: 143.573024092, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.531476795673\n",
      "[NOR] Episode: 8560, Length: 560, Avg Reward: 171.512764132, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.135657399893\n",
      "[NOR] Episode: 8570, Length: 893, Avg Reward: 151.104835244, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.212390169501\n",
      "[NOR] Episode: 8580, Length: 748, Avg Reward: 160.727976549, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.6391210556\n",
      "[NOR] Episode: 8590, Length: 589, Avg Reward: 153.71005986, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.298960149288\n",
      "[NOR] Episode: 8600, Length: 445, Avg Reward: 148.916170987, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.188161075115\n",
      "[NOR] Episode: 8610, Length: 722, Avg Reward: 143.006841585, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.37237846851\n",
      "[NOR] Episode: 8620, Length: 772, Avg Reward: 49.34625915, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.217886209488\n",
      "[NOR] Episode: 8630, Length: 842, Avg Reward: 109.582684851, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0112930387259\n",
      "[NOR] Episode: 8640, Length: 1000, Avg Reward: 149.241167311, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.4834446311\n",
      "[NOR] Episode: 8650, Length: 325, Avg Reward: 98.7911808357, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -7.88205432892\n",
      "[NOR] Episode: 8660, Length: 178, Avg Reward: 153.978130486, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.449778676033\n",
      "[NOR] Episode: 8670, Length: 422, Avg Reward: 182.232169559, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.156091600657\n",
      "[NOR] Episode: 8680, Length: 362, Avg Reward: 166.560310582, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.267928600311\n",
      "[NOR] Episode: 8690, Length: 303, Avg Reward: 146.101778225, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0639036148787\n",
      "[NOR] Episode: 8700, Length: 498, Avg Reward: 201.521868477, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.835658729076\n",
      "[NOR] Episode: 8710, Length: 472, Avg Reward: 188.131532325, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.627029776573\n",
      "[NOR] Episode: 8720, Length: 339, Avg Reward: 202.353754938, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.24828132987\n",
      "[NOR] Episode: 8730, Length: 567, Avg Reward: 187.044374579, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.635634899139\n",
      "[NOR] Episode: 8740, Length: 393, Avg Reward: 172.24207373, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.916448831558\n",
      "[NOR] Episode: 8750, Length: 488, Avg Reward: 195.287942281, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0637472420931\n",
      "[NOR] Episode: 8760, Length: 434, Avg Reward: 187.557779106, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.00514640659094\n",
      "[NOR] Episode: 8770, Length: 498, Avg Reward: 184.10421162, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.26417607069\n",
      "[NOR] Episode: 8780, Length: 204, Avg Reward: 154.169697166, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.00335407257\n",
      "[NOR] Episode: 8790, Length: 171, Avg Reward: 176.958412763, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.650207638741\n",
      "[NOR] Episode: 8800, Length: 501, Avg Reward: 189.364399852, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.291287839413\n",
      "[NOR] Episode: 8810, Length: 274, Avg Reward: 133.020937103, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.259802699089\n",
      "[NOR] Episode: 8820, Length: 433, Avg Reward: 183.084241608, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0347665995359\n",
      "[NOR] Episode: 8830, Length: 415, Avg Reward: 190.581231706, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.20773807168\n",
      "[NOR] Episode: 8840, Length: 161, Avg Reward: 171.905257816, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.151919916272\n",
      "[NOR] Episode: 8850, Length: 391, Avg Reward: 214.026903502, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.885688960552\n",
      "[NOR] Episode: 8860, Length: 600, Avg Reward: 206.618986788, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.24440664053\n",
      "[NOR] Episode: 8870, Length: 293, Avg Reward: 173.374951883, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.441494882107\n",
      "[NOR] Episode: 8880, Length: 292, Avg Reward: 180.179747899, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.327992588282\n",
      "[NOR] Episode: 8890, Length: 794, Avg Reward: 160.00312273, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.822198152542\n",
      "[NOR] Episode: 8900, Length: 341, Avg Reward: 163.608995262, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.445907175541\n",
      "[NOR] Episode: 8910, Length: 878, Avg Reward: 137.111253538, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0271489024162\n",
      "[NOR] Episode: 8920, Length: 836, Avg Reward: 127.953950478, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.627621173859\n",
      "[NOR] Episode: 8930, Length: 467, Avg Reward: 108.371858023, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0712352320552\n",
      "[NOR] Episode: 8940, Length: 314, Avg Reward: 158.765232041, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.135852441192\n",
      "[NOR] Episode: 8950, Length: 145, Avg Reward: 82.4278379179, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.26774716377\n",
      "[NOR] Episode: 8960, Length: 412, Avg Reward: 124.390501366, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.766393125057\n",
      "[NOR] Episode: 8970, Length: 395, Avg Reward: 146.225838959, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.173495590687\n",
      "[NOR] Episode: 8980, Length: 458, Avg Reward: 118.526478116, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.222660779953\n",
      "[NOR] Episode: 8990, Length: 237, Avg Reward: 144.09094182, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.596440315247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 05:13:18,838] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video009000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 9000, Length: 787, Avg Reward: 152.481231526, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.606279850006\n",
      "[NOR] Episode: 9010, Length: 297, Avg Reward: 133.314469092, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.145873874426\n",
      "[NOR] Episode: 9020, Length: 205, Avg Reward: 65.5681959452, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.672143936157\n",
      "[NOR] Episode: 9030, Length: 280, Avg Reward: 168.183262082, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.100618287921\n",
      "[NOR] Episode: 9040, Length: 186, Avg Reward: 93.1996258895, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.184693410993\n",
      "[NOR] Episode: 9050, Length: 1000, Avg Reward: 64.6543939753, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.87618303299\n",
      "[NOR] Episode: 9060, Length: 255, Avg Reward: 165.490013234, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.564447045326\n",
      "[NOR] Episode: 9070, Length: 658, Avg Reward: 139.358946674, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.34156447649\n",
      "[NOR] Episode: 9080, Length: 178, Avg Reward: 156.551464666, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.500497460365\n",
      "[NOR] Episode: 9090, Length: 550, Avg Reward: 188.467821304, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.798917651176\n",
      "[NOR] Episode: 9100, Length: 150, Avg Reward: 143.978883113, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.162039220333\n",
      "[NOR] Episode: 9110, Length: 476, Avg Reward: 171.612120903, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.498921096325\n",
      "[NOR] Episode: 9120, Length: 280, Avg Reward: 161.683999399, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.660269200802\n",
      "[NOR] Episode: 9130, Length: 592, Avg Reward: 172.971230112, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.229078441858\n",
      "[NOR] Episode: 9140, Length: 453, Avg Reward: 189.961024982, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0666434168816\n",
      "[NOR] Episode: 9150, Length: 496, Avg Reward: 152.681636241, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.313544481993\n",
      "[NOR] Episode: 9160, Length: 591, Avg Reward: 180.472017738, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.3163985014\n",
      "[NOR] Episode: 9170, Length: 507, Avg Reward: 203.279905522, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0678904950619\n",
      "[NOR] Episode: 9180, Length: 452, Avg Reward: 168.838040799, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0824029296637\n",
      "[NOR] Episode: 9190, Length: 342, Avg Reward: 201.725474139, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.584901809692\n",
      "[NOR] Episode: 9200, Length: 453, Avg Reward: 204.034309488, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0970057845116\n",
      "[NOR] Episode: 9210, Length: 434, Avg Reward: 173.354729428, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.948661208153\n",
      "[NOR] Episode: 9220, Length: 269, Avg Reward: 204.859663438, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.518026232719\n",
      "[NOR] Episode: 9230, Length: 452, Avg Reward: 196.943964335, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.754643082619\n",
      "[NOR] Episode: 9240, Length: 369, Avg Reward: 208.609831278, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.261812627316\n",
      "[NOR] Episode: 9250, Length: 403, Avg Reward: 176.541255714, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.590399682522\n",
      "[NOR] Episode: 9260, Length: 447, Avg Reward: 195.914540374, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.226011395454\n",
      "[NOR] Episode: 9270, Length: 404, Avg Reward: 192.630577738, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.137825965881\n",
      "[NOR] Episode: 9280, Length: 345, Avg Reward: 211.24746108, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.125370025635\n",
      "[NOR] Episode: 9290, Length: 160, Avg Reward: 164.388546991, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.777954697609\n",
      "[NOR] Episode: 9300, Length: 265, Avg Reward: 208.495309578, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.556633710861\n",
      "[NOR] Episode: 9310, Length: 295, Avg Reward: 198.453632478, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.458949506283\n",
      "[NOR] Episode: 9320, Length: 313, Avg Reward: 194.74860295, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.498367130756\n",
      "[NOR] Episode: 9330, Length: 339, Avg Reward: 162.794420045, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0418421328068\n",
      "[NOR] Episode: 9340, Length: 332, Avg Reward: 126.318408729, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.713736176491\n",
      "[NOR] Episode: 9350, Length: 255, Avg Reward: 114.490055886, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.273748964071\n",
      "[NOR] Episode: 9360, Length: 248, Avg Reward: 123.720583593, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.177351117134\n",
      "[NOR] Episode: 9370, Length: 295, Avg Reward: 164.185101333, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.500158905983\n",
      "[NOR] Episode: 9380, Length: 145, Avg Reward: 162.989398222, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.175131917\n",
      "[NOR] Episode: 9390, Length: 129, Avg Reward: 152.371932301, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.04773533344\n",
      "[NOR] Episode: 9400, Length: 282, Avg Reward: 102.113008942, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.172423824668\n",
      "[NOR] Episode: 9410, Length: 496, Avg Reward: 107.826238702, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.78396868706\n",
      "[NOR] Episode: 9420, Length: 284, Avg Reward: 158.628585482, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.820196211338\n",
      "[NOR] Episode: 9430, Length: 255, Avg Reward: 162.785483087, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.23434227705\n",
      "[NOR] Episode: 9440, Length: 292, Avg Reward: 115.707048144, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.47561532259\n",
      "[NOR] Episode: 9450, Length: 131, Avg Reward: 107.200476414, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.79279994965\n",
      "[NOR] Episode: 9460, Length: 293, Avg Reward: 158.468132087, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.551541924477\n",
      "[NOR] Episode: 9470, Length: 267, Avg Reward: 122.30027207, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.297168105841\n",
      "[NOR] Episode: 9480, Length: 368, Avg Reward: 124.65527041, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.44590401649\n",
      "[NOR] Episode: 9490, Length: 151, Avg Reward: 113.814190962, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.399541318417\n",
      "[NOR] Episode: 9500, Length: 293, Avg Reward: 122.374873012, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.05909025669\n",
      "[NOR] Episode: 9510, Length: 294, Avg Reward: 171.11470625, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.496565580368\n",
      "[NOR] Episode: 9520, Length: 295, Avg Reward: 152.790119457, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.254482388496\n",
      "[NOR] Episode: 9530, Length: 132, Avg Reward: 36.0481785245, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.382435262203\n",
      "[NOR] Episode: 9540, Length: 235, Avg Reward: 119.629638463, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.432866185904\n",
      "[NOR] Episode: 9550, Length: 271, Avg Reward: 183.863769628, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.803276240826\n",
      "[NOR] Episode: 9560, Length: 318, Avg Reward: 164.579807736, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.58737885952\n",
      "[NOR] Episode: 9570, Length: 291, Avg Reward: 180.486541713, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.29968857765\n",
      "[NOR] Episode: 9580, Length: 194, Avg Reward: 145.462375942, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.6043959856\n",
      "[NOR] Episode: 9590, Length: 391, Avg Reward: 156.022572091, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -15.9371061325\n",
      "[NOR] Episode: 9600, Length: 544, Avg Reward: 120.529922687, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.711943924427\n",
      "[NOR] Episode: 9610, Length: 292, Avg Reward: 192.735300264, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.572140336037\n",
      "[NOR] Episode: 9620, Length: 243, Avg Reward: 192.74244981, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.716925919056\n",
      "[NOR] Episode: 9630, Length: 623, Avg Reward: 187.065208571, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.20114290714\n",
      "[NOR] Episode: 9640, Length: 138, Avg Reward: 146.134637664, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.922934412956\n",
      "[NOR] Episode: 9650, Length: 284, Avg Reward: 180.317236499, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.564206063747\n",
      "[NOR] Episode: 9660, Length: 272, Avg Reward: 212.761722951, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.41189074516\n",
      "[NOR] Episode: 9670, Length: 122, Avg Reward: 158.367764597, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.719383597374\n",
      "[NOR] Episode: 9680, Length: 150, Avg Reward: 167.995834842, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0175002813339\n",
      "[NOR] Episode: 9690, Length: 227, Avg Reward: 164.768025914, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.442894846201\n",
      "[NOR] Episode: 9700, Length: 183, Avg Reward: 147.653440011, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.76979804039\n",
      "[NOR] Episode: 9710, Length: 589, Avg Reward: 153.531363939, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.594591140747\n",
      "[NOR] Episode: 9720, Length: 121, Avg Reward: 73.268377813, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.596576929092\n",
      "[NOR] Episode: 9730, Length: 261, Avg Reward: 144.914241672, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.406629353762\n",
      "[NOR] Episode: 9740, Length: 312, Avg Reward: 109.13162479, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.803686618805\n",
      "[NOR] Episode: 9750, Length: 142, Avg Reward: 121.020102738, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.698572874069\n",
      "[NOR] Episode: 9760, Length: 330, Avg Reward: 144.198388412, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.943810880184\n",
      "[NOR] Episode: 9770, Length: 367, Avg Reward: 96.3852886458, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.759503483772\n",
      "[NOR] Episode: 9780, Length: 214, Avg Reward: 55.8007848099, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.804105877876\n",
      "[NOR] Episode: 9790, Length: 272, Avg Reward: 43.1908325231, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.202484413981\n",
      "[NOR] Episode: 9800, Length: 493, Avg Reward: 129.3326165, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.06842875481\n",
      "[NOR] Episode: 9810, Length: 211, Avg Reward: 117.128175992, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.162264108658\n",
      "[NOR] Episode: 9820, Length: 1000, Avg Reward: 147.242981793, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.96940779686\n",
      "[NOR] Episode: 9830, Length: 271, Avg Reward: 134.994481164, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.430233538151\n",
      "[NOR] Episode: 9840, Length: 424, Avg Reward: 88.3518107586, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.68781757355\n",
      "[NOR] Episode: 9850, Length: 331, Avg Reward: 167.986673488, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.70661759377\n",
      "[NOR] Episode: 9860, Length: 1000, Avg Reward: 142.270941069, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.260311275721\n",
      "[NOR] Episode: 9870, Length: 247, Avg Reward: 132.400493973, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.25537660718\n",
      "[NOR] Episode: 9880, Length: 208, Avg Reward: 130.907632737, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.197083979845\n",
      "[NOR] Episode: 9890, Length: 387, Avg Reward: 182.682657752, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.208234712481\n",
      "[NOR] Episode: 9900, Length: 332, Avg Reward: 190.885797687, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.864807486534\n",
      "[NOR] Episode: 9910, Length: 369, Avg Reward: 189.750109085, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.294356882572\n",
      "[NOR] Episode: 9920, Length: 264, Avg Reward: 184.369034797, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.743731617928\n",
      "[NOR] Episode: 9930, Length: 197, Avg Reward: 119.470419859, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.466683268547\n",
      "[NOR] Episode: 9940, Length: 338, Avg Reward: 174.46101106, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.23654079437\n",
      "[NOR] Episode: 9950, Length: 248, Avg Reward: 164.531272548, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.804185807705\n",
      "[NOR] Episode: 9960, Length: 397, Avg Reward: 197.085054228, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.585088312626\n",
      "[NOR] Episode: 9970, Length: 337, Avg Reward: 197.009950646, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.35336875916\n",
      "[NOR] Episode: 9980, Length: 319, Avg Reward: 213.541327045, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 3.89543509483\n",
      "[NOR] Episode: 9990, Length: 376, Avg Reward: 199.059915879, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.879423379898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 05:35:54,374] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video010000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 10000, Length: 301, Avg Reward: 215.859526657, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.242112115026\n",
      "[NOR] Episode: 10010, Length: 365, Avg Reward: 219.586125706, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0429403334856\n",
      "[NOR] Episode: 10020, Length: 373, Avg Reward: 211.933235394, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.363117396832\n",
      "[NOR] Episode: 10030, Length: 298, Avg Reward: 220.782512331, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.969698309898\n",
      "[NOR] Episode: 10040, Length: 395, Avg Reward: 218.53817196, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.172413855791\n",
      "[NOR] Episode: 10050, Length: 388, Avg Reward: 212.009448546, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.532916128635\n",
      "[NOR] Episode: 10060, Length: 379, Avg Reward: 220.118396625, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.909634709358\n",
      "[NOR] Episode: 10070, Length: 401, Avg Reward: 209.600969164, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.965042591095\n",
      "[NOR] Episode: 10080, Length: 305, Avg Reward: 202.816786102, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.141829133034\n",
      "[NOR] Episode: 10090, Length: 342, Avg Reward: 220.628950285, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.41435995698\n",
      "[NOR] Episode: 10100, Length: 423, Avg Reward: 192.806093203, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0446627736092\n",
      "[NOR] Episode: 10110, Length: 488, Avg Reward: 196.226456442, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.00627303123\n",
      "[NOR] Episode: 10120, Length: 380, Avg Reward: 201.327446089, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.626383364201\n",
      "[NOR] Episode: 10130, Length: 425, Avg Reward: 193.381161734, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.923202931881\n",
      "[NOR] Episode: 10140, Length: 413, Avg Reward: 198.306027868, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.30872535706\n",
      "[NOR] Episode: 10150, Length: 415, Avg Reward: 194.06119631, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.331177264452\n",
      "[NOR] Episode: 10160, Length: 465, Avg Reward: 195.90735557, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.76536512375\n",
      "[NOR] Episode: 10170, Length: 573, Avg Reward: 194.639291232, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.303351610899\n",
      "[NOR] Episode: 10180, Length: 587, Avg Reward: 137.510320091, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.840230584145\n",
      "[NOR] Episode: 10190, Length: 1000, Avg Reward: 98.3968538635, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.47793972492\n",
      "[NOR] Episode: 10200, Length: 992, Avg Reward: 85.5314417376, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.43469822407\n",
      "[NOR] Episode: 10210, Length: 1000, Avg Reward: 76.8616666234, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.22942507267\n",
      "[NOR] Episode: 10220, Length: 935, Avg Reward: 102.01360345, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.383785218\n",
      "[NOR] Episode: 10230, Length: 542, Avg Reward: 75.7375248767, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.71072912216\n",
      "[NOR] Episode: 10240, Length: 434, Avg Reward: 121.445787896, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.271012336016\n",
      "[NOR] Episode: 10250, Length: 1000, Avg Reward: 21.5413412026, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.484743535519\n",
      "[NOR] Episode: 10260, Length: 1000, Avg Reward: 15.4776710791, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.50565636158\n",
      "[NOR] Episode: 10270, Length: 757, Avg Reward: 49.5442949045, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.832501471043\n",
      "[NOR] Episode: 10280, Length: 1000, Avg Reward: -25.0959869268, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.56916987896\n",
      "[NOR] Episode: 10290, Length: 932, Avg Reward: -74.1880568745, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.76200646162\n",
      "[NOR] Episode: 10300, Length: 732, Avg Reward: 38.0900935295, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.56745219231\n",
      "[NOR] Episode: 10310, Length: 1000, Avg Reward: 59.4080993343, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.44528639317\n",
      "[NOR] Episode: 10320, Length: 767, Avg Reward: 18.4890091713, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.821120381355\n",
      "[NOR] Episode: 10330, Length: 1000, Avg Reward: -57.3984904277, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.50311011076\n",
      "[NOR] Episode: 10340, Length: 640, Avg Reward: -116.727341085, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.689544141293\n",
      "[NOR] Episode: 10350, Length: 837, Avg Reward: -36.7891988405, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.357526957989\n",
      "[NOR] Episode: 10360, Length: 500, Avg Reward: 6.58187495292, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.43063640594\n",
      "[NOR] Episode: 10370, Length: 830, Avg Reward: 21.1463898396, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.505243599415\n",
      "[NOR] Episode: 10380, Length: 576, Avg Reward: 10.997991185, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.560835599899\n",
      "[NOR] Episode: 10390, Length: 1000, Avg Reward: 93.1614722824, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.108907043934\n",
      "[NOR] Episode: 10400, Length: 570, Avg Reward: 100.965523422, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.311337202787\n",
      "[NOR] Episode: 10410, Length: 469, Avg Reward: 145.671373552, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.799654483795\n",
      "[NOR] Episode: 10420, Length: 453, Avg Reward: 136.138221315, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.97536706924\n",
      "[NOR] Episode: 10430, Length: 490, Avg Reward: 162.727476041, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.58511805534\n",
      "[NOR] Episode: 10440, Length: 502, Avg Reward: 174.603825837, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.883332014084\n",
      "[NOR] Episode: 10450, Length: 302, Avg Reward: 192.589191828, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.704114973545\n",
      "[NOR] Episode: 10460, Length: 523, Avg Reward: 131.792360004, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.983729183674\n",
      "[NOR] Episode: 10470, Length: 482, Avg Reward: 153.34749625, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.615217328072\n",
      "[NOR] Episode: 10480, Length: 365, Avg Reward: 195.751196806, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.90612280369\n",
      "[NOR] Episode: 10490, Length: 372, Avg Reward: 194.674684142, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.164475291967\n",
      "[NOR] Episode: 10500, Length: 500, Avg Reward: 197.57853086, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.49807775021\n",
      "[NOR] Episode: 10510, Length: 399, Avg Reward: 201.907868672, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.384831756353\n",
      "[NOR] Episode: 10520, Length: 576, Avg Reward: 193.752827832, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.0838432312\n",
      "[NOR] Episode: 10530, Length: 435, Avg Reward: 210.163730538, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.957641839981\n",
      "[NOR] Episode: 10540, Length: 378, Avg Reward: 204.252695744, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 3.12173247337\n",
      "[NOR] Episode: 10550, Length: 428, Avg Reward: 200.325180954, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.932729780674\n",
      "[NOR] Episode: 10560, Length: 474, Avg Reward: 197.260254332, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.484448730946\n",
      "[NOR] Episode: 10570, Length: 583, Avg Reward: 187.369484456, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.556283175945\n",
      "[NOR] Episode: 10580, Length: 561, Avg Reward: 157.162953919, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.2314299196\n",
      "[NOR] Episode: 10590, Length: 612, Avg Reward: 143.83003871, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.729004025459\n",
      "[NOR] Episode: 10600, Length: 500, Avg Reward: 167.692430895, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0792566239834\n",
      "[NOR] Episode: 10610, Length: 416, Avg Reward: 157.023080599, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0594392567873\n",
      "[NOR] Episode: 10620, Length: 461, Avg Reward: 158.154847962, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.986439466476\n",
      "[NOR] Episode: 10630, Length: 868, Avg Reward: 135.439857941, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.4180482626\n",
      "[NOR] Episode: 10640, Length: 648, Avg Reward: 130.034310093, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.75960469246\n",
      "[NOR] Episode: 10650, Length: 643, Avg Reward: 165.577172353, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.196455672383\n",
      "[NOR] Episode: 10660, Length: 1000, Avg Reward: 108.9265038, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0205998718739\n",
      "[NOR] Episode: 10670, Length: 405, Avg Reward: 150.755070417, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.405263960361\n",
      "[NOR] Episode: 10680, Length: 484, Avg Reward: 190.842260903, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.181547999382\n",
      "[NOR] Episode: 10690, Length: 499, Avg Reward: 192.89936997, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.434401720762\n",
      "[NOR] Episode: 10700, Length: 285, Avg Reward: 196.733122997, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.64973020554\n",
      "[NOR] Episode: 10710, Length: 299, Avg Reward: 201.598143553, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.372884750366\n",
      "[NOR] Episode: 10720, Length: 502, Avg Reward: 180.656812298, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.498505651951\n",
      "[NOR] Episode: 10730, Length: 332, Avg Reward: 177.643146124, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.389383196831\n",
      "[NOR] Episode: 10740, Length: 489, Avg Reward: 180.685386548, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.536769509315\n",
      "[NOR] Episode: 10750, Length: 274, Avg Reward: 210.741711744, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.138943299651\n",
      "[NOR] Episode: 10760, Length: 380, Avg Reward: 206.868907803, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -5.73767805099\n",
      "[NOR] Episode: 10770, Length: 364, Avg Reward: 203.626367123, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.56156373024\n",
      "[NOR] Episode: 10780, Length: 309, Avg Reward: 209.229930015, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.86792361736\n",
      "[NOR] Episode: 10790, Length: 683, Avg Reward: 215.464614726, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.32517242432\n",
      "[NOR] Episode: 10800, Length: 261, Avg Reward: 211.903488072, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.277748525143\n",
      "[NOR] Episode: 10810, Length: 285, Avg Reward: 210.411796323, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.89063310623\n",
      "[NOR] Episode: 10820, Length: 264, Avg Reward: 211.128771292, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.19041377306\n",
      "[NOR] Episode: 10830, Length: 258, Avg Reward: 219.015878, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.720756173134\n",
      "[NOR] Episode: 10840, Length: 303, Avg Reward: 216.986311893, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.43466353416\n",
      "[NOR] Episode: 10850, Length: 293, Avg Reward: 202.399143934, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.932153224945\n",
      "[NOR] Episode: 10860, Length: 399, Avg Reward: 217.737581533, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.69423604012\n",
      "[NOR] Episode: 10870, Length: 329, Avg Reward: 215.907670521, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.566507101059\n",
      "[NOR] Episode: 10880, Length: 289, Avg Reward: 221.661696924, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.17936325073\n",
      "[NOR] Episode: 10890, Length: 328, Avg Reward: 216.426847617, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.594944000244\n",
      "[NOR] Episode: 10900, Length: 334, Avg Reward: 211.097550535, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.839336931705\n",
      "[NOR] Episode: 10910, Length: 323, Avg Reward: 201.151958408, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.239045485854\n",
      "[NOR] Episode: 10920, Length: 352, Avg Reward: 182.777557866, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.652070760727\n",
      "[NOR] Episode: 10930, Length: 384, Avg Reward: 211.78203269, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.58926820755\n",
      "[NOR] Episode: 10940, Length: 370, Avg Reward: 202.876150535, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.686679840088\n",
      "[NOR] Episode: 10950, Length: 386, Avg Reward: 201.980118855, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.845890522\n",
      "[NOR] Episode: 10960, Length: 444, Avg Reward: 202.487042179, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.05036211014\n",
      "[NOR] Episode: 10970, Length: 455, Avg Reward: 197.58865769, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.00804233551\n",
      "[NOR] Episode: 10980, Length: 329, Avg Reward: 193.110777743, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.873515963554\n",
      "[NOR] Episode: 10990, Length: 432, Avg Reward: 169.197844203, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.675102829933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 06:12:11,979] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video011000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 11000, Length: 356, Avg Reward: 185.688603719, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.821406841278\n",
      "[NOR] Episode: 11010, Length: 341, Avg Reward: 171.263262975, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.406763285398\n",
      "[NOR] Episode: 11020, Length: 352, Avg Reward: 198.668548282, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.767876029015\n",
      "[NOR] Episode: 11030, Length: 395, Avg Reward: 195.289554831, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.3298535347\n",
      "[NOR] Episode: 11040, Length: 188, Avg Reward: 173.680038327, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.45583570004\n",
      "[NOR] Episode: 11050, Length: 364, Avg Reward: 195.171161247, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.482598572969\n",
      "[NOR] Episode: 11060, Length: 545, Avg Reward: 196.808728911, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.97969174385\n",
      "[NOR] Episode: 11070, Length: 345, Avg Reward: 217.918479607, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.514051198959\n",
      "[NOR] Episode: 11080, Length: 402, Avg Reward: 199.602429002, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.31484365463\n",
      "[NOR] Episode: 11090, Length: 513, Avg Reward: 207.632475355, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.12389612198\n",
      "[NOR] Episode: 11100, Length: 348, Avg Reward: 195.173781612, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.726341307163\n",
      "[NOR] Episode: 11110, Length: 533, Avg Reward: 167.743412193, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.27174520493\n",
      "[NOR] Episode: 11120, Length: 625, Avg Reward: 137.974953469, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.277040362358\n",
      "[NOR] Episode: 11130, Length: 405, Avg Reward: 191.88096217, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.285949587822\n",
      "[NOR] Episode: 11140, Length: 673, Avg Reward: 156.76629647, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.166744589806\n",
      "[NOR] Episode: 11150, Length: 570, Avg Reward: 174.231903688, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.575830638409\n",
      "[NOR] Episode: 11160, Length: 428, Avg Reward: 154.298913523, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.291045188904\n",
      "[NOR] Episode: 11170, Length: 286, Avg Reward: 202.228176283, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.04049110413\n",
      "[NOR] Episode: 11180, Length: 520, Avg Reward: 177.954560805, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.503800332546\n",
      "[NOR] Episode: 11190, Length: 1000, Avg Reward: 176.179861057, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.103329464793\n",
      "[NOR] Episode: 11200, Length: 503, Avg Reward: 185.887488732, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0655397027731\n",
      "[NOR] Episode: 11210, Length: 618, Avg Reward: 187.706090373, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.251690030098\n",
      "[NOR] Episode: 11220, Length: 976, Avg Reward: 160.649581982, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.965606212616\n",
      "[NOR] Episode: 11230, Length: 572, Avg Reward: 178.642492779, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.59804844856\n",
      "[NOR] Episode: 11240, Length: 392, Avg Reward: 185.272143783, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.33229362965\n",
      "[NOR] Episode: 11250, Length: 433, Avg Reward: 188.881039947, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.66743075848\n",
      "[NOR] Episode: 11260, Length: 381, Avg Reward: 165.859489799, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.44553804398\n",
      "[NOR] Episode: 11270, Length: 483, Avg Reward: 193.459297547, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.29046559334\n",
      "[NOR] Episode: 11280, Length: 528, Avg Reward: 194.105290793, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.942758738995\n",
      "[NOR] Episode: 11290, Length: 267, Avg Reward: 167.46284481, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0423684120178\n",
      "[NOR] Episode: 11300, Length: 914, Avg Reward: 148.941515789, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0947375893593\n",
      "[NOR] Episode: 11310, Length: 1000, Avg Reward: 181.800211085, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.9896235466\n",
      "[NOR] Episode: 11320, Length: 1000, Avg Reward: 180.302401296, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.63978767395\n",
      "[NOR] Episode: 11330, Length: 303, Avg Reward: 113.78285933, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.714583933353\n",
      "[NOR] Episode: 11340, Length: 837, Avg Reward: 122.791800882, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.456900119781\n",
      "[NOR] Episode: 11350, Length: 1000, Avg Reward: 147.116510882, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.33829164505\n",
      "[NOR] Episode: 11360, Length: 597, Avg Reward: 145.311346807, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.133180260658\n",
      "[NOR] Episode: 11370, Length: 1000, Avg Reward: 180.697461112, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.118995308876\n",
      "[NOR] Episode: 11380, Length: 502, Avg Reward: 192.886481658, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.05500137806\n",
      "[NOR] Episode: 11390, Length: 442, Avg Reward: 195.772604713, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.197309732437\n",
      "[NOR] Episode: 11400, Length: 372, Avg Reward: 166.833385376, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.330426871777\n",
      "[NOR] Episode: 11410, Length: 382, Avg Reward: 171.898318848, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.28588855267\n",
      "[NOR] Episode: 11420, Length: 776, Avg Reward: 168.667258066, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.394726991653\n",
      "[NOR] Episode: 11430, Length: 405, Avg Reward: 174.990421702, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.13783660531\n",
      "[NOR] Episode: 11440, Length: 662, Avg Reward: 163.071127966, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.15418510139\n",
      "[NOR] Episode: 11450, Length: 360, Avg Reward: 191.1381919, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.309831559658\n",
      "[NOR] Episode: 11460, Length: 1000, Avg Reward: 152.445821627, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.18783891201\n",
      "[NOR] Episode: 11470, Length: 952, Avg Reward: 174.942711278, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0879203379154\n",
      "[NOR] Episode: 11480, Length: 914, Avg Reward: 153.978868092, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.810174226761\n",
      "[NOR] Episode: 11490, Length: 878, Avg Reward: 176.52014177, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.220616042614\n",
      "[NOR] Episode: 11500, Length: 1000, Avg Reward: 158.604074127, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.100762054324\n",
      "[NOR] Episode: 11510, Length: 783, Avg Reward: 112.486437753, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.112613916397\n",
      "[NOR] Episode: 11520, Length: 933, Avg Reward: 123.606558985, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.495025664568\n",
      "[NOR] Episode: 11530, Length: 790, Avg Reward: 136.49730286, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.691621184349\n",
      "[NOR] Episode: 11540, Length: 625, Avg Reward: 126.48816633, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.189759105444\n",
      "[NOR] Episode: 11550, Length: 777, Avg Reward: 125.96157395, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.443724811077\n",
      "[NOR] Episode: 11560, Length: 406, Avg Reward: 132.288752602, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.23994386196\n",
      "[NOR] Episode: 11570, Length: 616, Avg Reward: 136.100184727, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.02496528625\n",
      "[NOR] Episode: 11580, Length: 549, Avg Reward: 121.36440778, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.833766758442\n",
      "[NOR] Episode: 11590, Length: 663, Avg Reward: 168.262863141, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.090657196939\n",
      "[NOR] Episode: 11600, Length: 608, Avg Reward: 182.416894981, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0954950749874\n",
      "[NOR] Episode: 11610, Length: 726, Avg Reward: 132.68137749, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.452485769987\n",
      "[NOR] Episode: 11620, Length: 506, Avg Reward: 189.601587074, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.347821116447\n",
      "[NOR] Episode: 11630, Length: 430, Avg Reward: 186.026376678, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.51126241684\n",
      "[NOR] Episode: 11640, Length: 317, Avg Reward: 211.582909291, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.782266557217\n",
      "[NOR] Episode: 11650, Length: 445, Avg Reward: 191.968487077, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.33729040623\n",
      "[NOR] Episode: 11660, Length: 374, Avg Reward: 163.237405357, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.102925993502\n",
      "[NOR] Episode: 11670, Length: 486, Avg Reward: 189.824169209, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.416441619396\n",
      "[NOR] Episode: 11680, Length: 415, Avg Reward: 184.161801254, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.844115376472\n",
      "[NOR] Episode: 11690, Length: 202, Avg Reward: 156.722567129, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.189220428467\n",
      "[NOR] Episode: 11700, Length: 392, Avg Reward: 175.231152005, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.558693051338\n",
      "[NOR] Episode: 11710, Length: 219, Avg Reward: 133.460656916, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.644793748856\n",
      "[NOR] Episode: 11720, Length: 303, Avg Reward: 156.474691511, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.841830313206\n",
      "[NOR] Episode: 11730, Length: 266, Avg Reward: 159.119784251, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0170633047819\n",
      "[NOR] Episode: 11740, Length: 373, Avg Reward: 127.011355479, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.258484840393\n",
      "[NOR] Episode: 11750, Length: 204, Avg Reward: 86.2945869348, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.369293689728\n",
      "[NOR] Episode: 11760, Length: 674, Avg Reward: 164.148384852, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.14674629271\n",
      "[NOR] Episode: 11770, Length: 125, Avg Reward: 122.877473667, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.564502716064\n",
      "[NOR] Episode: 11780, Length: 637, Avg Reward: 159.628810721, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.17941451073\n",
      "[NOR] Episode: 11790, Length: 747, Avg Reward: 108.139462428, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.507945358753\n",
      "[NOR] Episode: 11800, Length: 1000, Avg Reward: 44.077942572, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.540393710136\n",
      "[NOR] Episode: 11810, Length: 599, Avg Reward: 159.699223161, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.033848464489\n",
      "[NOR] Episode: 11820, Length: 512, Avg Reward: 138.419991302, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.36555814743\n",
      "[NOR] Episode: 11830, Length: 809, Avg Reward: 134.665491082, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.69445884228\n",
      "[NOR] Episode: 11840, Length: 665, Avg Reward: 126.382027922, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.3229637146\n",
      "[NOR] Episode: 11850, Length: 756, Avg Reward: 134.573077966, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.145376756787\n",
      "[NOR] Episode: 11860, Length: 650, Avg Reward: 121.373607791, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.539716959\n",
      "[NOR] Episode: 11870, Length: 622, Avg Reward: 75.6722039127, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.00549928843975\n",
      "[NOR] Episode: 11880, Length: 157, Avg Reward: 101.109517871, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.373940855265\n",
      "[NOR] Episode: 11890, Length: 152, Avg Reward: 20.4620605033, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.533721446991\n",
      "[NOR] Episode: 11900, Length: 332, Avg Reward: 28.2070160788, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.987458109856\n",
      "[NOR] Episode: 11910, Length: 180, Avg Reward: -42.364133197, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.201811254025\n",
      "[NOR] Episode: 11920, Length: 153, Avg Reward: 21.9289922033, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.16174173355\n",
      "[NOR] Episode: 11930, Length: 484, Avg Reward: 64.1361711725, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.945611476898\n",
      "[NOR] Episode: 11940, Length: 985, Avg Reward: 52.3679025023, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.280271410942\n",
      "[NOR] Episode: 11950, Length: 648, Avg Reward: -8.63292868586, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.941913664341\n",
      "[NOR] Episode: 11960, Length: 153, Avg Reward: 57.6187000886, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.28278040886\n",
      "[NOR] Episode: 11970, Length: 224, Avg Reward: 37.0722178856, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.203353613615\n",
      "[NOR] Episode: 11980, Length: 491, Avg Reward: 66.2910774901, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.52131319046\n",
      "[NOR] Episode: 11990, Length: 120, Avg Reward: 26.2989312074, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.536236524582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 06:48:28,883] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video012000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 12000, Length: 323, Avg Reward: 41.3672979409, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.00386797636747\n",
      "[NOR] Episode: 12010, Length: 692, Avg Reward: -17.8602356071, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.532893836498\n",
      "[NOR] Episode: 12020, Length: 951, Avg Reward: 51.7037693555, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.457917422056\n",
      "[NOR] Episode: 12030, Length: 1000, Avg Reward: -36.5657848162, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.156159073114\n",
      "[NOR] Episode: 12040, Length: 558, Avg Reward: 38.1504539473, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.118020638824\n",
      "[NOR] Episode: 12050, Length: 437, Avg Reward: 46.2315207067, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.432966262102\n",
      "[NOR] Episode: 12060, Length: 172, Avg Reward: -6.57488078849, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.393674612045\n",
      "[NOR] Episode: 12070, Length: 428, Avg Reward: 18.9820318316, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.57759052515\n",
      "[NOR] Episode: 12080, Length: 328, Avg Reward: 86.9864266821, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.456175982952\n",
      "[NOR] Episode: 12090, Length: 200, Avg Reward: 43.164474826, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.7133346796\n",
      "[NOR] Episode: 12100, Length: 381, Avg Reward: 3.31272578848, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.724011540413\n",
      "[NOR] Episode: 12110, Length: 177, Avg Reward: 38.7930467755, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0888030156493\n",
      "[NOR] Episode: 12120, Length: 326, Avg Reward: 39.072117207, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.767390429974\n",
      "[NOR] Episode: 12130, Length: 160, Avg Reward: 4.62890436416, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.863866090775\n",
      "[NOR] Episode: 12140, Length: 507, Avg Reward: 72.6188243757, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.55698943138\n",
      "[NOR] Episode: 12150, Length: 186, Avg Reward: 104.295667485, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.60538983345\n",
      "[NOR] Episode: 12160, Length: 897, Avg Reward: 47.5178705169, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.146900072694\n",
      "[NOR] Episode: 12170, Length: 356, Avg Reward: -53.8207648271, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.21456374228\n",
      "[NOR] Episode: 12180, Length: 360, Avg Reward: 6.41263263401, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.870706439018\n",
      "[NOR] Episode: 12190, Length: 268, Avg Reward: 52.1745215103, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.212311565876\n",
      "[NOR] Episode: 12200, Length: 442, Avg Reward: 62.6247874171, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 3.14448165894\n",
      "[NOR] Episode: 12210, Length: 381, Avg Reward: 28.5291884455, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.458320945501\n",
      "[NOR] Episode: 12220, Length: 1000, Avg Reward: 108.376924609, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0910325050354\n",
      "[NOR] Episode: 12230, Length: 206, Avg Reward: 77.4144936697, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0926773995161\n",
      "[NOR] Episode: 12240, Length: 453, Avg Reward: 107.064250033, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.216325581074\n",
      "[NOR] Episode: 12250, Length: 288, Avg Reward: 123.700800867, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.774351477623\n",
      "[NOR] Episode: 12260, Length: 523, Avg Reward: 55.641435222, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.319042742252\n",
      "[NOR] Episode: 12270, Length: 631, Avg Reward: 148.428785313, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.533125579357\n",
      "[NOR] Episode: 12280, Length: 558, Avg Reward: 139.342132108, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.878339469433\n",
      "[NOR] Episode: 12290, Length: 425, Avg Reward: 146.978632807, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.158346056938\n",
      "[NOR] Episode: 12300, Length: 403, Avg Reward: 157.499132104, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.119982868433\n",
      "[NOR] Episode: 12310, Length: 496, Avg Reward: 152.687176838, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.737673819065\n",
      "[NOR] Episode: 12320, Length: 345, Avg Reward: 172.103159683, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.346558511257\n",
      "[NOR] Episode: 12330, Length: 530, Avg Reward: 155.031679092, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.881008982658\n",
      "[NOR] Episode: 12340, Length: 601, Avg Reward: 171.528711021, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.28534567356\n",
      "[NOR] Episode: 12350, Length: 585, Avg Reward: 201.792159286, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.03941559792\n",
      "[NOR] Episode: 12360, Length: 966, Avg Reward: 182.203409386, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.384312391281\n",
      "[NOR] Episode: 12370, Length: 385, Avg Reward: 188.023624916, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.198352336884\n",
      "[NOR] Episode: 12380, Length: 333, Avg Reward: 168.05940449, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -3.50450921059\n",
      "[NOR] Episode: 12390, Length: 295, Avg Reward: 203.694690259, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 3.21363782883\n",
      "[NOR] Episode: 12400, Length: 310, Avg Reward: 214.249382672, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.684019565582\n",
      "[NOR] Episode: 12410, Length: 405, Avg Reward: 185.105443458, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.451933920383\n",
      "[NOR] Episode: 12420, Length: 274, Avg Reward: 187.826584555, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.426105618477\n",
      "[NOR] Episode: 12430, Length: 397, Avg Reward: 189.625491639, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.408287197351\n",
      "[NOR] Episode: 12440, Length: 451, Avg Reward: 172.427437608, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.13307285309\n",
      "[NOR] Episode: 12450, Length: 356, Avg Reward: 202.028475901, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.551793038845\n",
      "[NOR] Episode: 12460, Length: 469, Avg Reward: 201.984737835, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.422292858362\n",
      "[NOR] Episode: 12470, Length: 401, Avg Reward: 203.791884906, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.190320730209\n",
      "[NOR] Episode: 12480, Length: 414, Avg Reward: 188.905286184, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.658847630024\n",
      "[NOR] Episode: 12490, Length: 376, Avg Reward: 199.241052432, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.819652676582\n",
      "[NOR] Episode: 12500, Length: 487, Avg Reward: 197.691986079, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.02033281326\n",
      "[NOR] Episode: 12510, Length: 420, Avg Reward: 189.032449687, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.15629386902\n",
      "[NOR] Episode: 12520, Length: 503, Avg Reward: 209.898155442, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.23314940929\n",
      "[NOR] Episode: 12530, Length: 435, Avg Reward: 213.588348468, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0656816661358\n",
      "[NOR] Episode: 12540, Length: 388, Avg Reward: 210.479862554, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.81449699402\n",
      "[NOR] Episode: 12550, Length: 498, Avg Reward: 180.732044259, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.157818049192\n",
      "[NOR] Episode: 12560, Length: 450, Avg Reward: 195.618499131, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0671180486679\n",
      "[NOR] Episode: 12570, Length: 468, Avg Reward: 191.920394124, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.80490279198\n",
      "[NOR] Episode: 12580, Length: 318, Avg Reward: 200.398992081, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.63240224123\n",
      "[NOR] Episode: 12590, Length: 341, Avg Reward: 219.206405732, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.600551664829\n",
      "[NOR] Episode: 12600, Length: 862, Avg Reward: 184.954988978, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.112523943186\n",
      "[NOR] Episode: 12610, Length: 1000, Avg Reward: 126.633067018, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -4.20821428299\n",
      "[NOR] Episode: 12620, Length: 291, Avg Reward: 195.929139258, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.602265715599\n",
      "[NOR] Episode: 12630, Length: 547, Avg Reward: 197.991832915, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.750717282295\n",
      "[NOR] Episode: 12640, Length: 432, Avg Reward: 202.823939529, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.38763237\n",
      "[NOR] Episode: 12650, Length: 452, Avg Reward: 193.694643046, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -3.64197921753\n",
      "[NOR] Episode: 12660, Length: 447, Avg Reward: 194.703452978, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.38802146912\n",
      "[NOR] Episode: 12670, Length: 401, Avg Reward: 216.637731537, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.49833792448\n",
      "[NOR] Episode: 12680, Length: 370, Avg Reward: 176.675395576, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.52684533596\n",
      "[NOR] Episode: 12690, Length: 533, Avg Reward: 164.417696118, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.152784317732\n",
      "[NOR] Episode: 12700, Length: 451, Avg Reward: 144.362528243, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0998276770115\n",
      "[NOR] Episode: 12710, Length: 317, Avg Reward: 178.923280633, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.41461670399\n",
      "[NOR] Episode: 12720, Length: 407, Avg Reward: 185.983057139, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.222168445587\n",
      "[NOR] Episode: 12730, Length: 484, Avg Reward: 191.320466828, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.60271584988\n",
      "[NOR] Episode: 12740, Length: 460, Avg Reward: 86.4450185084, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.847192287445\n",
      "[NOR] Episode: 12750, Length: 220, Avg Reward: 114.20412584, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.99865180254\n",
      "[NOR] Episode: 12760, Length: 450, Avg Reward: 124.078192797, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0967008173466\n",
      "[NOR] Episode: 12770, Length: 400, Avg Reward: 159.030248318, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.263204813\n",
      "[NOR] Episode: 12780, Length: 430, Avg Reward: 153.263298669, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.30464744568\n",
      "[NOR] Episode: 12790, Length: 662, Avg Reward: 84.4881093335, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.43777084351\n",
      "[NOR] Episode: 12800, Length: 331, Avg Reward: 72.3256053653, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.186649769545\n",
      "[NOR] Episode: 12810, Length: 312, Avg Reward: 62.3154354856, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0897691249847\n",
      "[NOR] Episode: 12820, Length: 518, Avg Reward: 67.1461447963, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.075841575861\n",
      "[NOR] Episode: 12830, Length: 640, Avg Reward: 24.0510676671, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.27119684219\n",
      "[NOR] Episode: 12840, Length: 752, Avg Reward: 125.794283779, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.32702183723\n",
      "[NOR] Episode: 12850, Length: 188, Avg Reward: 76.0256072796, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.788512587547\n",
      "[NOR] Episode: 12860, Length: 202, Avg Reward: 49.0829566562, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.46872854233\n",
      "[NOR] Episode: 12870, Length: 619, Avg Reward: 99.7418044524, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.22385644913\n",
      "[NOR] Episode: 12880, Length: 739, Avg Reward: 64.1813131191, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.79513072968\n",
      "[NOR] Episode: 12890, Length: 402, Avg Reward: -19.0208042923, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.780633807182\n",
      "[NOR] Episode: 12900, Length: 1000, Avg Reward: 16.0669821561, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.62657523155\n",
      "[NOR] Episode: 12910, Length: 732, Avg Reward: 33.7601405349, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.424632519484\n",
      "[NOR] Episode: 12920, Length: 309, Avg Reward: 114.359130426, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.31644237041\n",
      "[NOR] Episode: 12930, Length: 425, Avg Reward: 102.451151176, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.594886898994\n",
      "[NOR] Episode: 12940, Length: 237, Avg Reward: -48.0210966806, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.77036809921\n",
      "[NOR] Episode: 12950, Length: 193, Avg Reward: 74.9344899052, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.01958274841\n",
      "[NOR] Episode: 12960, Length: 573, Avg Reward: 93.1403173044, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.619615793228\n",
      "[NOR] Episode: 12970, Length: 452, Avg Reward: 103.377576551, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.15736746788\n",
      "[NOR] Episode: 12980, Length: 231, Avg Reward: 91.8138221718, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.26151049137\n",
      "[NOR] Episode: 12990, Length: 294, Avg Reward: 78.806168023, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.835582137108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 07:20:11,074] Starting new video recorder writing to /home/cristian/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/24/openaigym.video.23.10254.video013000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 13000, Length: 913, Avg Reward: 110.293591455, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.683110296726\n",
      "[NOR] Episode: 13010, Length: 634, Avg Reward: 104.562120538, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.193180352449\n",
      "[NOR] Episode: 13020, Length: 735, Avg Reward: 94.0510525536, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.745446681976\n",
      "[NOR] Episode: 13030, Length: 733, Avg Reward: 82.3592709385, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.12540626526\n",
      "[NOR] Episode: 13040, Length: 531, Avg Reward: 103.876386144, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.935826838017\n",
      "[NOR] Episode: 13050, Length: 274, Avg Reward: 82.5586403889, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.36504709721\n",
      "[NOR] Episode: 13060, Length: 121, Avg Reward: -104.283707728, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.30247426033\n",
      "[NOR] Episode: 13070, Length: 163, Avg Reward: -98.4221063992, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.2397646904\n",
      "[NOR] Episode: 13080, Length: 185, Avg Reward: -124.615220882, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.31939029694\n",
      "[NOR] Episode: 13090, Length: 100, Avg Reward: -138.915233784, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.821703135967\n",
      "[NOR] Episode: 13100, Length: 193, Avg Reward: -140.130100379, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.125070273876\n",
      "[NOR] Episode: 13110, Length: 162, Avg Reward: -181.67927163, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.58974981308\n",
      "[NOR] Episode: 13120, Length: 145, Avg Reward: -162.622367106, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.668999314308\n",
      "[NOR] Episode: 13130, Length: 165, Avg Reward: -176.309176098, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.693750739098\n",
      "[NOR] Episode: 13140, Length: 283, Avg Reward: -156.459535317, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.192295074463\n",
      "[NOR] Episode: 13150, Length: 144, Avg Reward: -150.708850651, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.172128930688\n",
      "[NOR] Episode: 13160, Length: 219, Avg Reward: -147.123398359, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.03850865364\n",
      "[NOR] Episode: 13170, Length: 1000, Avg Reward: -186.659398599, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.24109470844\n",
      "[NOR] Episode: 13180, Length: 186, Avg Reward: -138.558944789, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.814137637615\n",
      "[NOR] Episode: 13190, Length: 959, Avg Reward: 108.581400779, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.800348937511\n",
      "[NOR] Episode: 13200, Length: 1000, Avg Reward: 76.305147938, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.609453082085\n",
      "[NOR] Episode: 13210, Length: 720, Avg Reward: 109.74124215, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0479639172554\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-12e3c146c23e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterp1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mupdate_target_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-70-deb27017290c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, keep_prob, e, learning_rate, print_step, update_target_step, episodes, max_episode_length, batch_size)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_e\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mr_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mep_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cristian/data/cristian/tfinterface/tfinterface/reinforcement/expanded_state_env.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhorizontal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/wrappers/monitoring.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/wrappers/time_limit.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyLinearImpulse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpulse_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36mBeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mcontactListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mBeginContact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 40000.\n",
    "model.fit(\n",
    "    env, print_step=10, \n",
    "    episodes=int(1e5), max_episode_length=10000, batch_size=32,\n",
    "    learning_rate = 0.002, # lambda t: 0.05 * k / (k + t)\n",
    "    e = interp1d([0, 300000], [0.4, 0.05], fill_value=0.05, bounds_error=False),\n",
    "    keep_prob = 0.5,\n",
    "    update_target_step = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 18:20:33,289] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "212.63485096\n",
      "222.209029125\n",
      "137.585769854\n",
      "180.875673014\n",
      "206.109792056\n",
      "244.175226865\n",
      "219.223174017\n",
      "226.254220443\n",
      "189.530083255\n",
      "180.200875147\n",
      "226.533856294\n",
      "215.507410864\n",
      "257.758179742\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <type 'exceptions.TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ac2829c8c03c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_polygon\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLunarLanderContinuous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLunarLander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/window/xlib/__init__.pyc\u001b[0m in \u001b[0;36mdispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# Check for the events specific to this window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         while xlib.XCheckWindowEvent(_x_display, _window,\n\u001b[0;32m--> 853\u001b[0;31m                                      0x1ffffff, byref(e)):\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;31m# Key events are filtered by the xlib window event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# handler so they get a shot at the prefiltered event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: <type 'exceptions.TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/{name}\".format(path = os.getcwd(), name = name)\n",
    "logs_path = \"{path}/logs/\".format(path = os.getcwd(), name = name)\n",
    "\n",
    "\n",
    "model_run = LunarLander(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path,\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total = 0.\n",
    "    ep = 0\n",
    "    while not done and ep < 700:\n",
    "        ep += 1\n",
    "        a = model_run.predict(s, 0.0)\n",
    "        s, r, done, info = env.step(a)\n",
    "        total += r\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "    \n",
    "    print(total)\n",
    "    \n",
    "env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
