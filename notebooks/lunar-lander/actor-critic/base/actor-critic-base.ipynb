{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tfinterface.model_base import ModelBase\n",
    "from tfinterface.reinforcement import ExperienceReplay\n",
    "from tfinterface.utils import select_columns, soft_if, get_run, shifted_log_loss\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from tfinterface.reinforcement import ExpandedStateEnv\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "name = \"actor-critic-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Inputs(object):\n",
    "    def __init__(self, n_states, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.episode_length = tf.placeholder(tf.int64, [], name='episode_length')\n",
    "\n",
    "            self.s = tf.placeholder(tf.float32, [None, n_states], name='s')\n",
    "            self.a = tf.placeholder(tf.int32, [None], name='a')\n",
    "            self.r = tf.placeholder(tf.float32, [None], name='r')\n",
    "            self.v1 = tf.placeholder(tf.float32, [None], name='V1')\n",
    "            self.done = tf.placeholder(tf.bool, [None], name='done')\n",
    "            \n",
    "            self.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            self.keep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\n",
    "            self.training = tf.placeholder(tf.bool, [], name='training')\n",
    "            \n",
    "            self.pi = tf.placeholder(tf.float32, [], name='pi')\n",
    "            \n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, base_model, inputs, n_actions, n_states, y, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            \n",
    "            ops = dict(\n",
    "                trainable=True,\n",
    "                kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01)\n",
    "            )\n",
    "\n",
    "            net = inputs.s\n",
    "\n",
    "            net = tf.layers.dense(net, 64, activation=tf.nn.relu, name=\"relu_layer\", **ops)        \n",
    "            \n",
    "            self.V = tf.layers.dense(net, n_actions, name='V', **ops)[:, 0]\n",
    "\n",
    "            self.target = tf.where(inputs.done, inputs.r,  inputs.r + y * inputs.v1)\n",
    "\n",
    "            self.error = self.target - self.V\n",
    "            self.loss = Pipe(self.error, tf.nn.l2_loss, tf.reduce_mean)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "            self.update = tf.train.AdamOptimizer(inputs.learning_rate).minimize(self.loss, var_list=self.variables)\n",
    "\n",
    "            avg_error, std_error = tf.nn.moments(self.error, [0])\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('avg_target', tf.reduce_mean(self.target)),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.scalar('avg_error', avg_error),\n",
    "                tf.summary.scalar('std_error', std_error),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n",
    "class Actor(object):\n",
    "    def __init__(self, base_model, inputs, critic, n_actions, n_states, y, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            ops = dict(\n",
    "                trainable=True,\n",
    "                kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01)\n",
    "            )\n",
    "\n",
    "            net = inputs.s\n",
    "\n",
    "            net = tf.layers.dense(net, 128, activation=tf.nn.relu, name=\"relu_layer\", use_bias=True, **ops)\n",
    "            net = tf.nn.dropout(net, inputs.keep_prob)\n",
    "            net = tf.layers.dense(net, 64, activation=tf.nn.relu, name=\"relu_layer2\", use_bias=True, **ops)\n",
    "            net = tf.nn.dropout(net, inputs.keep_prob)\n",
    "\n",
    "            \n",
    "            self.logits = tf.layers.dense(net, n_actions, name='P', use_bias=False, **ops)\n",
    "            self.P = tf.nn.softmax(self.logits)\n",
    "            \n",
    "            self.Pa = select_columns(self.P, inputs.a)\n",
    "\n",
    "            self.loss = shifted_log_loss(self.Pa) * critic.error\n",
    "            self.loss = tf.reduce_mean(self.loss)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "            self.update = tf.train.AdamOptimizer(inputs.learning_rate).minimize(self.loss, var_list=self.variables)\n",
    "\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LunarLander(ModelBase):\n",
    "    \n",
    "    def define_model(self, n_actions, n_states, y=0.98, buffer_length=50000, pi=0.1):\n",
    "        self.global_max = float('-inf')\n",
    "\n",
    "        self.replay_buffer = ExperienceReplay(5, max_length=buffer_length)\n",
    "\n",
    "\n",
    "        with self.graph.as_default(), tf.device(\"cpu:0\"):\n",
    "\n",
    "            self.inputs = Inputs(n_states, \"inputs\")\n",
    "\n",
    "            self.critic = Critic(self, self.inputs, n_actions, n_states, y, \"critic\")\n",
    "            self.target_critic = Critic(self, self.inputs, n_actions, n_states, y, \"target_critic\")\n",
    "            self.actor = Actor(self, self.inputs, self.target_critic, n_actions, n_states, y, \"actor\")\n",
    "\n",
    "            self.update = tf.group(self.critic.update, self.actor.update)\n",
    "\n",
    "            self.episode_length_summary = tf.summary.scalar('episode_length', self.inputs.episode_length)\n",
    "\n",
    "            self.summaries = tf.summary.merge([self.actor.summaries, self.critic.summaries, self.target_critic.summaries])\n",
    "\n",
    "            self.update_target = tf.group(*[\n",
    "                t.assign_add(pi * (a - t)) for t, a in zip(self.target_critic.variables, self.critic.variables)\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    def predict_feed(self, S):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.keep_prob: 1.0,\n",
    "            self.inputs.training: False\n",
    "        }\n",
    "    \n",
    "    def predict(self, state, e = 0.0):\n",
    "        predict_feed = self.predict_feed([state])\n",
    "        actions = self.sess.run(self.actor.P, feed_dict=predict_feed)\n",
    "        actions = actions[0]\n",
    "        n = len(actions)\n",
    "\n",
    "        if random.random() < e:\n",
    "            return random.randint(0, n-1)\n",
    "        else:\n",
    "            return np.random.choice(n, p=actions)\n",
    "    \n",
    "    def fit_feed(self, S, A, R, V1, Done, learning_rate, keep_prob):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.a: A,\n",
    "            self.inputs.r: R,\n",
    "            self.inputs.v1: V1,\n",
    "            self.inputs.done: Done,\n",
    "            self.inputs.learning_rate: learning_rate,\n",
    "            self.inputs.keep_prob: keep_prob,\n",
    "            self.inputs.training: True\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def fit(self, env, keep_prob=0.5, e=0.01, learning_rate=0.01, print_step=10, \n",
    "            update_target_step = 32, episodes=100000, max_episode_length=float('inf'), batch_size=32):\n",
    "        \n",
    "        r_total = 0.\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            ep_step = 0\n",
    "            s = env.reset()\n",
    "            episode_length = 0\n",
    "            ep_reward = 0.\n",
    "            \n",
    "            while not done and ep_step <= max_episode_length:\n",
    "                self.global_step += 1\n",
    "                episode_length += 1\n",
    "                ep_step += 1\n",
    "                \n",
    "                \n",
    "                _learning_rate = learning_rate(self.global_step) if hasattr(learning_rate, '__call__') else learning_rate\n",
    "                _e = e(self.global_step) if hasattr(e, '__call__') else e\n",
    "                \n",
    "                \n",
    "                a = self.predict(s, e = _e)\n",
    "                s1, r, done, info = env.step(a)\n",
    "                r_total += r\n",
    "                ep_reward += r\n",
    "                \n",
    "                \n",
    "                self.replay_buffer.append(s, a, r, s1, done)\n",
    "                \n",
    "                \n",
    "                S, A, R, S1, Done = self.replay_buffer.random_batch(batch_size).unzip()\n",
    "                predict_feed = self.predict_feed(S1)\n",
    "                V1 = self.sess.run(self.target_critic.V, feed_dict=predict_feed)\n",
    "\n",
    "                \n",
    "                fit_feed = self.fit_feed(S, A, R, V1, Done, _learning_rate, keep_prob)\n",
    "                _, summaries = self.sess.run([self.update, self.summaries], feed_dict=fit_feed)\n",
    "                self.writer.add_summary(summaries, self.global_step)\n",
    "                \n",
    "                \n",
    "                if self.global_step % update_target_step == 0:\n",
    "                    self.sess.run(self.update_target)\n",
    "                \n",
    "                \n",
    "                s = s1\n",
    "                \n",
    "            \n",
    "            episode_length_summary = self.sess.run(self.episode_length_summary,\n",
    "                                                   feed_dict={self.inputs.episode_length: episode_length})\n",
    "            self.writer.add_summary(episode_length_summary, self.global_step)\n",
    "\n",
    "\n",
    "            if ep_reward >= self.global_max:\n",
    "                print(\"[MAX] Episode: {}, Length: {}, Reward: {}, buffer_len: {}\".format(episode, episode_length, ep_reward, len(self.replay_buffer)))\n",
    "                self.save(model_path = self.model_path + \".{score}\".format(score = ep_reward))\n",
    "                self.global_max = ep_reward\n",
    "\n",
    "\n",
    "            if episode % print_step == 0 and episode > 0:\n",
    "                avg_r = r_total / print_step\n",
    "                actor_loss = self.sess.run(self.actor.loss, feed_dict=fit_feed)\n",
    "                print(\"[NOR] Episode: {}, Length: {}, Avg Reward: {}, e: {}, Learning Rate: {}, buffer_len: {}\".format(episode, episode_length, avg_r, _e, _learning_rate, len(self.replay_buffer)))\n",
    "                print(\"Loss: {}\".format(actor_loss))\n",
    "                self.save()\n",
    "                r_total = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 16:11:05,707] Making new env: LunarLander-v2\n",
      "[2017-03-23 16:11:05,710] Creating monitor directory monitor/27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "run = get_run()\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = wrappers.Monitor(env, \"monitor/{run}\".format(run = run))\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/models/{run}\".format(path = os.getcwd(), run = run)\n",
    "logs_path = \"{path}/logs/{run}\".format(path = os.getcwd(), run = run)\n",
    "\n",
    "\n",
    "model = LunarLander(\n",
    "    n_actions, n_states, y=0.9999, \n",
    "    buffer_length=500000,\n",
    "    model_path = model_path,\n",
    "    logs_path = logs_path,\n",
    "    restore = False,\n",
    "    pi = 0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 16:11:06,521] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/27/openaigym.video.2.25564.video000000.mp4\n",
      "[2017-03-23 16:11:14,540] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/27/openaigym.video.2.25564.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 0, Length: 153, Reward: -229.244791235, buffer_len: 153\n",
      "[MAX] Episode: 6, Length: 156, Reward: -199.190314485, buffer_len: 1374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 16:11:20,165] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/27/openaigym.video.2.25564.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 10, Length: 73, Reward: -83.1431761836, buffer_len: 1984\n",
      "[NOR] Episode: 10, Length: 73, Avg Reward: -349.558546986, e: 0.3976865, Learning Rate: 0.002, buffer_len: 1984\n",
      "Loss: -4.00674438477\n",
      "[NOR] Episode: 20, Length: 74, Avg Reward: -520.052531677, e: 0.396374, Learning Rate: 0.002, buffer_len: 3109\n",
      "Loss: -5.40413951874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 16:11:29,330] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/27/openaigym.video.2.25564.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 30, Length: 151, Avg Reward: -549.930262161, e: 0.395048666667, Learning Rate: 0.002, buffer_len: 4245\n",
      "Loss: -4.74908542633\n",
      "[NOR] Episode: 40, Length: 78, Avg Reward: -456.695379397, e: 0.393844666667, Learning Rate: 0.002, buffer_len: 5277\n",
      "Loss: -7.82006978989\n",
      "[MAX] Episode: 42, Length: 1000, Reward: 21.8118941444, buffer_len: 6355\n",
      "[NOR] Episode: 50, Length: 325, Avg Reward: -134.876163275, e: 0.388686833333, Learning Rate: 0.002, buffer_len: 9698\n",
      "Loss: -8.99314880371\n",
      "[NOR] Episode: 60, Length: 135, Avg Reward: -191.0788197, e: 0.3847855, Learning Rate: 0.002, buffer_len: 13042\n",
      "Loss: -9.96517467499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 16:12:06,084] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/27/openaigym.video.2.25564.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 66, Length: 645, Reward: 97.4244092196, buffer_len: 15412\n",
      "[NOR] Episode: 70, Length: 312, Avg Reward: -92.9634802731, e: 0.3801375, Learning Rate: 0.002, buffer_len: 17026\n",
      "Loss: -3.62788248062\n",
      "[NOR] Episode: 80, Length: 181, Avg Reward: -135.806014708, e: 0.375088166667, Learning Rate: 0.002, buffer_len: 21354\n",
      "Loss: -4.69043779373\n",
      "[NOR] Episode: 90, Length: 280, Avg Reward: -119.100028599, e: 0.3700155, Learning Rate: 0.002, buffer_len: 25702\n",
      "Loss: -1.21044552326\n",
      "[NOR] Episode: 100, Length: 121, Avg Reward: -100.119687985, e: 0.3650385, Learning Rate: 0.002, buffer_len: 29968\n",
      "Loss: -1.13451874256\n",
      "[NOR] Episode: 110, Length: 160, Avg Reward: -91.9696142684, e: 0.362153333333, Learning Rate: 0.002, buffer_len: 32441\n",
      "Loss: -2.80442619324\n",
      "[NOR] Episode: 120, Length: 154, Avg Reward: -175.738541751, e: 0.360591166667, Learning Rate: 0.002, buffer_len: 33780\n",
      "Loss: -0.0747852921486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 16:13:31,672] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/27/openaigym.video.2.25564.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 130, Length: 156, Avg Reward: -220.988399114, e: 0.3591305, Learning Rate: 0.002, buffer_len: 35032\n",
      "Loss: -2.04960727692\n",
      "[NOR] Episode: 140, Length: 109, Avg Reward: -127.140908591, e: 0.355793833333, Learning Rate: 0.002, buffer_len: 37892\n",
      "Loss: -3.48042011261\n",
      "[NOR] Episode: 150, Length: 99, Avg Reward: -296.739705315, e: 0.354736833333, Learning Rate: 0.002, buffer_len: 38798\n",
      "Loss: -1.29190063477\n",
      "[NOR] Episode: 160, Length: 176, Avg Reward: -172.053190404, e: 0.353332166667, Learning Rate: 0.002, buffer_len: 40002\n",
      "Loss: -6.97999238968\n",
      "[NOR] Episode: 170, Length: 1000, Avg Reward: -159.037032277, e: 0.3489035, Learning Rate: 0.002, buffer_len: 43798\n",
      "Loss: -4.02130079269\n",
      "[NOR] Episode: 180, Length: 281, Avg Reward: -58.9089999952, e: 0.344793333333, Learning Rate: 0.002, buffer_len: 47321\n",
      "Loss: -2.51121640205\n",
      "[MAX] Episode: 181, Length: 1000, Reward: 99.8685626909, buffer_len: 48321\n",
      "[NOR] Episode: 190, Length: 197, Avg Reward: -84.4453929308, e: 0.3416515, Learning Rate: 0.002, buffer_len: 50014\n",
      "Loss: -0.798361003399\n",
      "[NOR] Episode: 200, Length: 143, Avg Reward: -96.166168672, e: 0.337779333333, Learning Rate: 0.002, buffer_len: 53333\n",
      "Loss: -1.51173043251\n",
      "[NOR] Episode: 210, Length: 188, Avg Reward: -62.6735267404, e: 0.334940833333, Learning Rate: 0.002, buffer_len: 55766\n",
      "Loss: -1.83445048332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 16:14:59,481] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/27/openaigym.video.2.25564.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 220, Length: 174, Avg Reward: -97.018105782, e: 0.332966833333, Learning Rate: 0.002, buffer_len: 57458\n",
      "Loss: 0.977512359619\n",
      "[NOR] Episode: 230, Length: 155, Avg Reward: -114.464612211, e: 0.331258833333, Learning Rate: 0.002, buffer_len: 58922\n",
      "Loss: -3.66474342346\n",
      "[NOR] Episode: 240, Length: 224, Avg Reward: -101.437091019, e: 0.329322166667, Learning Rate: 0.002, buffer_len: 60582\n",
      "Loss: -4.18593883514\n",
      "[NOR] Episode: 250, Length: 128, Avg Reward: -128.302399929, e: 0.327424, Learning Rate: 0.002, buffer_len: 62209\n",
      "Loss: -0.0509030222893\n",
      "[NOR] Episode: 260, Length: 153, Avg Reward: -97.3189406017, e: 0.325270333333, Learning Rate: 0.002, buffer_len: 64055\n",
      "Loss: -2.78596735001\n",
      "[NOR] Episode: 270, Length: 127, Avg Reward: -124.103892438, e: 0.323422333333, Learning Rate: 0.002, buffer_len: 65639\n",
      "Loss: -1.84047353268\n",
      "[NOR] Episode: 280, Length: 167, Avg Reward: -127.16001214, e: 0.321488, Learning Rate: 0.002, buffer_len: 67297\n",
      "Loss: -3.40476894379\n",
      "[NOR] Episode: 290, Length: 129, Avg Reward: -125.625182049, e: 0.319602666667, Learning Rate: 0.002, buffer_len: 68913\n",
      "Loss: -0.633002638817\n",
      "[NOR] Episode: 300, Length: 219, Avg Reward: -114.340370742, e: 0.317656666667, Learning Rate: 0.002, buffer_len: 70581\n",
      "Loss: -2.31373953819\n",
      "[NOR] Episode: 310, Length: 146, Avg Reward: -120.313268258, e: 0.314629166667, Learning Rate: 0.002, buffer_len: 73176\n",
      "Loss: -5.13708400726\n",
      "[NOR] Episode: 320, Length: 1000, Avg Reward: -89.8819071363, e: 0.308921833333, Learning Rate: 0.002, buffer_len: 78068\n",
      "Loss: 0.865307688713\n",
      "[NOR] Episode: 330, Length: 119, Avg Reward: -113.982430555, e: 0.3062665, Learning Rate: 0.002, buffer_len: 80344\n",
      "Loss: -2.21413397789\n",
      "[NOR] Episode: 340, Length: 142, Avg Reward: -57.3705992431, e: 0.300230166667, Learning Rate: 0.002, buffer_len: 85518\n",
      "Loss: -0.966915905476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 16:16:51,967] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/27/openaigym.video.2.25564.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 350, Length: 202, Avg Reward: -35.678343063, e: 0.294987166667, Learning Rate: 0.002, buffer_len: 90012\n",
      "Loss: 1.81864070892\n",
      "[MAX] Episode: 351, Length: 723, Reward: 146.523224822, buffer_len: 90735\n",
      "[NOR] Episode: 360, Length: 244, Avg Reward: -6.86647791673, e: 0.2880595, Learning Rate: 0.002, buffer_len: 95950\n",
      "Loss: -2.13402938843\n",
      "[MAX] Episode: 361, Length: 340, Reward: 164.040888163, buffer_len: 96290\n",
      "[NOR] Episode: 370, Length: 138, Avg Reward: -42.6090627936, e: 0.2823965, Learning Rate: 0.002, buffer_len: 100804\n",
      "Loss: -2.3611676693\n",
      "[NOR] Episode: 380, Length: 1000, Avg Reward: -21.6554577176, e: 0.273995333333, Learning Rate: 0.002, buffer_len: 108005\n",
      "Loss: -5.26689958572\n",
      "[NOR] Episode: 390, Length: 497, Avg Reward: 9.67759390557, e: 0.264995666667, Learning Rate: 0.002, buffer_len: 115719\n",
      "Loss: -8.89630794525\n",
      "[NOR] Episode: 400, Length: 1000, Avg Reward: -7.42759707553, e: 0.256623666667, Learning Rate: 0.002, buffer_len: 122895\n",
      "Loss: -1.92040622234\n",
      "[MAX] Episode: 409, Length: 760, Reward: 196.609876075, buffer_len: 127779\n",
      "[NOR] Episode: 410, Length: 1000, Avg Reward: -4.16184325533, e: 0.249759, Learning Rate: 0.002, buffer_len: 128779\n",
      "Loss: -1.33465528488\n",
      "[NOR] Episode: 420, Length: 214, Avg Reward: -19.4850962545, e: 0.2427555, Learning Rate: 0.002, buffer_len: 134782\n",
      "Loss: -3.68796157837\n",
      "[MAX] Episode: 428, Length: 643, Reward: 200.987318589, buffer_len: 139318\n",
      "[NOR] Episode: 430, Length: 1000, Avg Reward: 15.9454320261, e: 0.235130166667, Learning Rate: 0.002, buffer_len: 141318\n",
      "Loss: -1.86088752747\n",
      "[NOR] Episode: 440, Length: 220, Avg Reward: -13.518898789, e: 0.226474666667, Learning Rate: 0.002, buffer_len: 148737\n",
      "Loss: -2.60722780228\n",
      "[NOR] Episode: 450, Length: 196, Avg Reward: -31.2171020829, e: 0.218467833333, Learning Rate: 0.002, buffer_len: 155600\n",
      "Loss: -6.99015522003\n",
      "[MAX] Episode: 451, Length: 435, Reward: 212.669115993, buffer_len: 156035\n",
      "[MAX] Episode: 458, Length: 703, Reward: 233.578600584, buffer_len: 158938\n",
      "[NOR] Episode: 460, Length: 1000, Avg Reward: 46.7464853756, e: 0.212240166667, Learning Rate: 0.002, buffer_len: 160938\n",
      "Loss: -0.853986024857\n",
      "[NOR] Episode: 470, Length: 207, Avg Reward: 25.4245013438, e: 0.203427166667, Learning Rate: 0.002, buffer_len: 168492\n",
      "Loss: 0.3470505476\n",
      "[NOR] Episode: 480, Length: 202, Avg Reward: -37.3474357292, e: 0.196731666667, Learning Rate: 0.002, buffer_len: 174231\n",
      "Loss: -3.54982447624\n",
      "[NOR] Episode: 490, Length: 374, Avg Reward: -48.2556936661, e: 0.190519166667, Learning Rate: 0.002, buffer_len: 179556\n",
      "Loss: 0.763898611069\n",
      "[NOR] Episode: 500, Length: 200, Avg Reward: -33.9136251072, e: 0.185762666667, Learning Rate: 0.002, buffer_len: 183633\n",
      "Loss: 0.413511306047\n",
      "[NOR] Episode: 510, Length: 396, Avg Reward: -24.7396317315, e: 0.180089166667, Learning Rate: 0.002, buffer_len: 188496\n",
      "Loss: -1.57466888428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 16:23:59,898] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/27/openaigym.video.2.25564.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 520, Length: 398, Avg Reward: -54.3171931464, e: 0.173187166667, Learning Rate: 0.002, buffer_len: 194412\n",
      "Loss: -4.18339443207\n",
      "[NOR] Episode: 530, Length: 209, Avg Reward: -32.0010323107, e: 0.1660075, Learning Rate: 0.002, buffer_len: 200566\n",
      "Loss: -1.37670195103\n",
      "[NOR] Episode: 540, Length: 344, Avg Reward: 4.59464100836, e: 0.158885, Learning Rate: 0.002, buffer_len: 206671\n",
      "Loss: 5.54103183746\n",
      "[NOR] Episode: 550, Length: 1000, Avg Reward: 28.8735830419, e: 0.150909666667, Learning Rate: 0.002, buffer_len: 213507\n",
      "Loss: -2.07209062576\n",
      "[NOR] Episode: 560, Length: 452, Avg Reward: -63.7860949957, e: 0.146623333333, Learning Rate: 0.002, buffer_len: 217181\n",
      "Loss: -0.932694673538\n",
      "[NOR] Episode: 570, Length: 150, Avg Reward: -152.831682143, e: 0.142071, Learning Rate: 0.002, buffer_len: 221083\n",
      "Loss: -3.21540880203\n",
      "[NOR] Episode: 580, Length: 381, Avg Reward: -164.445426455, e: 0.138712166667, Learning Rate: 0.002, buffer_len: 223962\n",
      "Loss: -1.38223290443\n",
      "[NOR] Episode: 590, Length: 270, Avg Reward: -96.2683498841, e: 0.134245, Learning Rate: 0.002, buffer_len: 227791\n",
      "Loss: -2.30593562126\n",
      "[NOR] Episode: 600, Length: 1000, Avg Reward: -78.5188931165, e: 0.127855166667, Learning Rate: 0.002, buffer_len: 233268\n",
      "Loss: -5.96333694458\n",
      "[NOR] Episode: 610, Length: 419, Avg Reward: -187.113686395, e: 0.124831166667, Learning Rate: 0.002, buffer_len: 235860\n",
      "Loss: 0.850197076797\n",
      "[NOR] Episode: 620, Length: 408, Avg Reward: -92.2666310326, e: 0.120805, Learning Rate: 0.002, buffer_len: 239311\n",
      "Loss: 0.13256727159\n",
      "[NOR] Episode: 630, Length: 595, Avg Reward: -45.7383403378, e: 0.116424166667, Learning Rate: 0.002, buffer_len: 243066\n",
      "Loss: 1.36914432049\n",
      "[NOR] Episode: 640, Length: 340, Avg Reward: -40.0508569996, e: 0.1120165, Learning Rate: 0.002, buffer_len: 246844\n",
      "Loss: -0.776442527771\n",
      "[NOR] Episode: 650, Length: 1000, Avg Reward: -59.4122024738, e: 0.104532333333, Learning Rate: 0.002, buffer_len: 253259\n",
      "Loss: -2.20688986778\n",
      "[NOR] Episode: 660, Length: 1000, Avg Reward: -31.3651630539, e: 0.0960075, Learning Rate: 0.002, buffer_len: 260566\n",
      "Loss: -1.69990098476\n",
      "[NOR] Episode: 670, Length: 457, Avg Reward: -61.4667902263, e: 0.0905545, Learning Rate: 0.002, buffer_len: 265240\n",
      "Loss: -1.19979965687\n",
      "[NOR] Episode: 680, Length: 558, Avg Reward: -57.6685328173, e: 0.0838146666667, Learning Rate: 0.002, buffer_len: 271017\n",
      "Loss: -0.96650236845\n",
      "[NOR] Episode: 690, Length: 581, Avg Reward: -70.1062067843, e: 0.0764471666667, Learning Rate: 0.002, buffer_len: 277332\n",
      "Loss: -2.68621110916\n",
      "[NOR] Episode: 700, Length: 863, Avg Reward: -45.2366873516, e: 0.069243, Learning Rate: 0.002, buffer_len: 283507\n",
      "Loss: -1.45125937462\n",
      "[NOR] Episode: 710, Length: 1000, Avg Reward: -86.734513878, e: 0.062047, Learning Rate: 0.002, buffer_len: 289675\n",
      "Loss: 0.0930793881416\n",
      "[NOR] Episode: 720, Length: 703, Avg Reward: -81.3830898432, e: 0.055572, Learning Rate: 0.002, buffer_len: 295225\n",
      "Loss: -3.00781130791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 16:31:22,947] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/27/openaigym.video.2.25564.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 730, Length: 380, Avg Reward: -64.3893151943, e: 0.0500233333333, Learning Rate: 0.002, buffer_len: 299981\n",
      "Loss: -2.26348233223\n",
      "[NOR] Episode: 740, Length: 576, Avg Reward: -45.489821629, e: 0.05, Learning Rate: 0.002, buffer_len: 305275\n",
      "Loss: -0.522294938564\n",
      "[NOR] Episode: 750, Length: 539, Avg Reward: -42.2189712273, e: 0.05, Learning Rate: 0.002, buffer_len: 309823\n",
      "Loss: -0.705880284309\n",
      "[NOR] Episode: 760, Length: 1000, Avg Reward: -11.9431309635, e: 0.05, Learning Rate: 0.002, buffer_len: 314959\n",
      "Loss: -0.301007390022\n",
      "[NOR] Episode: 770, Length: 740, Avg Reward: -15.7184341403, e: 0.05, Learning Rate: 0.002, buffer_len: 320479\n",
      "Loss: 1.84604632854\n",
      "[NOR] Episode: 780, Length: 762, Avg Reward: 12.2524531479, e: 0.05, Learning Rate: 0.002, buffer_len: 326242\n",
      "Loss: -1.81003117561\n",
      "[NOR] Episode: 790, Length: 453, Avg Reward: 28.2625820243, e: 0.05, Learning Rate: 0.002, buffer_len: 331708\n",
      "Loss: -0.355839192867\n",
      "[NOR] Episode: 800, Length: 559, Avg Reward: -45.2608920484, e: 0.05, Learning Rate: 0.002, buffer_len: 336950\n",
      "Loss: -2.52487158775\n",
      "[NOR] Episode: 810, Length: 1000, Avg Reward: -42.728240419, e: 0.05, Learning Rate: 0.002, buffer_len: 344173\n",
      "Loss: -1.88515472412\n",
      "[NOR] Episode: 820, Length: 841, Avg Reward: -86.3009567149, e: 0.05, Learning Rate: 0.002, buffer_len: 349813\n",
      "Loss: 0.776802539825\n",
      "[NOR] Episode: 830, Length: 490, Avg Reward: -59.827860116, e: 0.05, Learning Rate: 0.002, buffer_len: 354906\n",
      "Loss: -2.76584291458\n",
      "[NOR] Episode: 840, Length: 572, Avg Reward: 35.0521606963, e: 0.05, Learning Rate: 0.002, buffer_len: 360118\n",
      "Loss: -0.790336370468\n",
      "[NOR] Episode: 850, Length: 473, Avg Reward: -7.2819032418, e: 0.05, Learning Rate: 0.002, buffer_len: 365253\n",
      "Loss: -0.559098362923\n",
      "[NOR] Episode: 860, Length: 193, Avg Reward: 17.9589542001, e: 0.05, Learning Rate: 0.002, buffer_len: 370477\n",
      "Loss: -0.791870236397\n",
      "[NOR] Episode: 870, Length: 544, Avg Reward: 19.8158768497, e: 0.05, Learning Rate: 0.002, buffer_len: 376248\n",
      "Loss: -1.82211124897\n",
      "[NOR] Episode: 880, Length: 772, Avg Reward: 36.1994445762, e: 0.05, Learning Rate: 0.002, buffer_len: 381612\n",
      "Loss: 0.858674526215\n",
      "[NOR] Episode: 890, Length: 337, Avg Reward: -32.538008246, e: 0.05, Learning Rate: 0.002, buffer_len: 385696\n",
      "Loss: -5.07790708542\n",
      "[NOR] Episode: 900, Length: 434, Avg Reward: 67.6512416338, e: 0.05, Learning Rate: 0.002, buffer_len: 390081\n",
      "Loss: -0.217243254185\n",
      "[NOR] Episode: 910, Length: 1000, Avg Reward: 42.331019527, e: 0.05, Learning Rate: 0.002, buffer_len: 394444\n",
      "Loss: -0.260316729546\n",
      "[NOR] Episode: 920, Length: 550, Avg Reward: -9.26843037828, e: 0.05, Learning Rate: 0.002, buffer_len: 398150\n",
      "Loss: -1.38851952553\n",
      "[NOR] Episode: 930, Length: 492, Avg Reward: 52.8930759785, e: 0.05, Learning Rate: 0.002, buffer_len: 402442\n",
      "Loss: -1.54175281525\n",
      "[NOR] Episode: 940, Length: 514, Avg Reward: 34.8263036711, e: 0.05, Learning Rate: 0.002, buffer_len: 408250\n",
      "Loss: 0.53407907486\n",
      "[NOR] Episode: 950, Length: 396, Avg Reward: -50.9534206241, e: 0.05, Learning Rate: 0.002, buffer_len: 412134\n",
      "Loss: -0.480430424213\n",
      "[NOR] Episode: 960, Length: 534, Avg Reward: -37.7395105648, e: 0.05, Learning Rate: 0.002, buffer_len: 417102\n",
      "Loss: -0.539424300194\n",
      "[NOR] Episode: 970, Length: 159, Avg Reward: -161.118428277, e: 0.05, Learning Rate: 0.002, buffer_len: 422175\n",
      "Loss: -1.81421124935\n",
      "[NOR] Episode: 980, Length: 682, Avg Reward: -56.1594166067, e: 0.05, Learning Rate: 0.002, buffer_len: 426587\n",
      "Loss: 1.57082271576\n",
      "[NOR] Episode: 990, Length: 209, Avg Reward: -126.007145075, e: 0.05, Learning Rate: 0.002, buffer_len: 430693\n",
      "Loss: -1.6665545702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 16:40:47,680] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/27/openaigym.video.2.25564.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 1000, Length: 964, Avg Reward: -43.9108466078, e: 0.05, Learning Rate: 0.002, buffer_len: 435063\n",
      "Loss: -1.95838820934\n",
      "[NOR] Episode: 1010, Length: 944, Avg Reward: -12.4057315411, e: 0.05, Learning Rate: 0.002, buffer_len: 439978\n",
      "Loss: 0.552754819393\n",
      "[NOR] Episode: 1020, Length: 350, Avg Reward: -58.5776861254, e: 0.05, Learning Rate: 0.002, buffer_len: 444907\n",
      "Loss: -3.17436933517\n",
      "[NOR] Episode: 1030, Length: 569, Avg Reward: -48.386242217, e: 0.05, Learning Rate: 0.002, buffer_len: 449774\n",
      "Loss: -2.94911241531\n",
      "[NOR] Episode: 1040, Length: 411, Avg Reward: -22.2013142673, e: 0.05, Learning Rate: 0.002, buffer_len: 453901\n",
      "Loss: -1.40606343746\n",
      "[NOR] Episode: 1050, Length: 269, Avg Reward: 3.42773443422, e: 0.05, Learning Rate: 0.002, buffer_len: 459382\n",
      "Loss: -1.31490254402\n",
      "[NOR] Episode: 1060, Length: 728, Avg Reward: 15.1008029895, e: 0.05, Learning Rate: 0.002, buffer_len: 464356\n",
      "Loss: 0.402203202248\n",
      "[NOR] Episode: 1070, Length: 197, Avg Reward: -83.0089497378, e: 0.05, Learning Rate: 0.002, buffer_len: 469599\n",
      "Loss: -2.49935531616\n",
      "[NOR] Episode: 1080, Length: 492, Avg Reward: -65.9500000539, e: 0.05, Learning Rate: 0.002, buffer_len: 473602\n",
      "Loss: -1.25873744488\n",
      "[NOR] Episode: 1090, Length: 393, Avg Reward: 22.2990580082, e: 0.05, Learning Rate: 0.002, buffer_len: 478135\n",
      "Loss: -1.52819764614\n",
      "[NOR] Episode: 1100, Length: 421, Avg Reward: -50.7662972144, e: 0.05, Learning Rate: 0.002, buffer_len: 483454\n",
      "Loss: -2.07743430138\n",
      "[NOR] Episode: 1110, Length: 848, Avg Reward: 3.28162298735, e: 0.05, Learning Rate: 0.002, buffer_len: 490173\n",
      "Loss: -1.63396346569\n",
      "[NOR] Episode: 1120, Length: 224, Avg Reward: -0.876445135056, e: 0.05, Learning Rate: 0.002, buffer_len: 494651\n",
      "Loss: -3.88889122009\n",
      "[NOR] Episode: 1130, Length: 267, Avg Reward: 64.0631358268, e: 0.05, Learning Rate: 0.002, buffer_len: 499718\n",
      "Loss: -0.425155758858\n",
      "[NOR] Episode: 1140, Length: 587, Avg Reward: 62.4285259338, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.48036086559\n",
      "[NOR] Episode: 1150, Length: 700, Avg Reward: 73.8322512576, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -4.90764474869\n",
      "[NOR] Episode: 1160, Length: 884, Avg Reward: 60.1644715328, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.538139462471\n",
      "[NOR] Episode: 1170, Length: 757, Avg Reward: 103.855536286, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.63177442551\n",
      "[NOR] Episode: 1180, Length: 1000, Avg Reward: -18.4175397064, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.346703469753\n",
      "[NOR] Episode: 1190, Length: 1000, Avg Reward: -7.26091037967, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.706214308739\n",
      "[NOR] Episode: 1200, Length: 1000, Avg Reward: -19.7157258294, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.551894724369\n",
      "[NOR] Episode: 1210, Length: 1000, Avg Reward: -68.0038138706, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.822465658188\n",
      "[NOR] Episode: 1220, Length: 1000, Avg Reward: -91.0917124669, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.412876307964\n",
      "[NOR] Episode: 1230, Length: 827, Avg Reward: 15.2863169343, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.17151165009\n",
      "[NOR] Episode: 1240, Length: 884, Avg Reward: 81.1990042924, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.744905471802\n",
      "[NOR] Episode: 1250, Length: 846, Avg Reward: 65.8277111512, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.24067831039\n",
      "[NOR] Episode: 1260, Length: 218, Avg Reward: 51.5523131014, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.583845019341\n",
      "[NOR] Episode: 1270, Length: 573, Avg Reward: -15.4501369223, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.56681680679\n",
      "[NOR] Episode: 1280, Length: 545, Avg Reward: 69.4061730609, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.694899260998\n",
      "[NOR] Episode: 1290, Length: 130, Avg Reward: -8.55462981842, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.668976306915\n",
      "[NOR] Episode: 1300, Length: 897, Avg Reward: 64.7841624218, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.591967701912\n",
      "[NOR] Episode: 1310, Length: 438, Avg Reward: 97.3932851044, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.16799223423\n",
      "[NOR] Episode: 1320, Length: 496, Avg Reward: 51.5077699783, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.939432144165\n",
      "[NOR] Episode: 1330, Length: 873, Avg Reward: 20.0335662054, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.63353651762\n",
      "[NOR] Episode: 1340, Length: 827, Avg Reward: 14.8678778923, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.674046039581\n",
      "[NOR] Episode: 1350, Length: 219, Avg Reward: 0.8783703274, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.47638869286\n",
      "[NOR] Episode: 1360, Length: 615, Avg Reward: 89.5969327447, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.559225916862\n",
      "[NOR] Episode: 1370, Length: 1000, Avg Reward: -74.117037195, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.99447059631\n",
      "[NOR] Episode: 1380, Length: 816, Avg Reward: -38.7902413127, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.656790196896\n",
      "[NOR] Episode: 1390, Length: 506, Avg Reward: 65.1638695833, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.676990509033\n",
      "[NOR] Episode: 1400, Length: 957, Avg Reward: -13.2908128164, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.5295945406\n",
      "[NOR] Episode: 1410, Length: 539, Avg Reward: 30.612743176, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 1.46830523014\n",
      "[NOR] Episode: 1420, Length: 321, Avg Reward: 41.9501576773, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.54201292992\n",
      "[NOR] Episode: 1430, Length: 703, Avg Reward: -46.1941388307, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.07615172863\n",
      "[NOR] Episode: 1440, Length: 695, Avg Reward: -25.1200747789, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.20681715012\n",
      "[NOR] Episode: 1450, Length: 583, Avg Reward: 41.6029877432, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.21814799309\n",
      "[NOR] Episode: 1460, Length: 337, Avg Reward: 0.725354766024, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.174266576767\n",
      "[NOR] Episode: 1470, Length: 744, Avg Reward: 15.4832252521, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -4.28819608688\n",
      "[NOR] Episode: 1480, Length: 165, Avg Reward: 43.5262122477, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 2.3999941349\n",
      "[NOR] Episode: 1490, Length: 467, Avg Reward: 37.6458287422, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.75845003128\n",
      "[NOR] Episode: 1500, Length: 1000, Avg Reward: 33.399688029, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.45053350925\n",
      "[NOR] Episode: 1510, Length: 1000, Avg Reward: 47.9845640997, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.08796846867\n",
      "[NOR] Episode: 1520, Length: 300, Avg Reward: 99.5166181168, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.17729723454\n",
      "[NOR] Episode: 1530, Length: 550, Avg Reward: 50.8679744728, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.47012400627\n",
      "[NOR] Episode: 1540, Length: 625, Avg Reward: -9.84918533494, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.803943872452\n",
      "[NOR] Episode: 1550, Length: 484, Avg Reward: 47.3880272309, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.785103678703\n",
      "[NOR] Episode: 1560, Length: 394, Avg Reward: 36.5314556012, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.32186388969\n",
      "[NOR] Episode: 1570, Length: 324, Avg Reward: 54.6061132515, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.48774039745\n",
      "[NOR] Episode: 1580, Length: 589, Avg Reward: 40.7137484491, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.911706507206\n",
      "[NOR] Episode: 1590, Length: 538, Avg Reward: 69.5465708834, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.02214741707\n",
      "[MAX] Episode: 1596, Length: 345, Reward: 242.429456983, buffer_len: 500000\n",
      "[NOR] Episode: 1600, Length: 676, Avg Reward: 111.285709963, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.115453794599\n",
      "[NOR] Episode: 1610, Length: 364, Avg Reward: 78.3043353541, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.281644463539\n",
      "[NOR] Episode: 1620, Length: 522, Avg Reward: 54.7990539183, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.20739746094\n",
      "[NOR] Episode: 1630, Length: 416, Avg Reward: 75.9481410674, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.583559930325\n",
      "[NOR] Episode: 1640, Length: 331, Avg Reward: 75.6208448199, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.708145856857\n",
      "[NOR] Episode: 1650, Length: 399, Avg Reward: 71.6830325844, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.30947351456\n",
      "[NOR] Episode: 1660, Length: 626, Avg Reward: 70.6439514748, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.18495464325\n",
      "[NOR] Episode: 1670, Length: 573, Avg Reward: 53.8903030423, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.36688053608\n",
      "[NOR] Episode: 1680, Length: 367, Avg Reward: 77.7671318474, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.222746878862\n",
      "[NOR] Episode: 1690, Length: 1000, Avg Reward: -18.2182930111, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.41663789749\n",
      "[NOR] Episode: 1700, Length: 309, Avg Reward: 53.6727958149, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.11571884155\n",
      "[NOR] Episode: 1710, Length: 330, Avg Reward: 87.6534311282, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.27112388611\n",
      "[NOR] Episode: 1720, Length: 486, Avg Reward: 45.5164984278, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.27476429939\n",
      "[NOR] Episode: 1730, Length: 729, Avg Reward: 16.5711861021, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.325447618961\n",
      "[NOR] Episode: 1740, Length: 372, Avg Reward: 87.4735903044, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.86058807373\n",
      "[NOR] Episode: 1750, Length: 500, Avg Reward: 32.1466737757, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.723507165909\n",
      "[NOR] Episode: 1760, Length: 537, Avg Reward: 115.427712513, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.187017321587\n",
      "[NOR] Episode: 1770, Length: 369, Avg Reward: 24.7860443864, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.567714452744\n",
      "[NOR] Episode: 1780, Length: 301, Avg Reward: 0.18501132149, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.37109446526\n",
      "[NOR] Episode: 1790, Length: 385, Avg Reward: 41.1590839034, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.32266759872\n",
      "[NOR] Episode: 1800, Length: 279, Avg Reward: 20.0969599335, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.44480597973\n",
      "[NOR] Episode: 1810, Length: 553, Avg Reward: 60.6413822648, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.51331472397\n",
      "[NOR] Episode: 1820, Length: 277, Avg Reward: -55.0175840804, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.03871703148\n",
      "[NOR] Episode: 1830, Length: 390, Avg Reward: 80.5718194276, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.6901113987\n",
      "[NOR] Episode: 1840, Length: 226, Avg Reward: 38.9541057044, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.40511512756\n",
      "[MAX] Episode: 1842, Length: 301, Reward: 244.224935631, buffer_len: 500000\n",
      "[NOR] Episode: 1850, Length: 417, Avg Reward: 56.8846336582, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.995583891869\n",
      "[NOR] Episode: 1860, Length: 667, Avg Reward: 78.5095872684, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.818280756474\n",
      "[NOR] Episode: 1870, Length: 718, Avg Reward: 119.039524631, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.433346450329\n",
      "[NOR] Episode: 1880, Length: 315, Avg Reward: -23.7615410236, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.966739952564\n",
      "[NOR] Episode: 1890, Length: 279, Avg Reward: -47.3765904151, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.869089961052\n",
      "[NOR] Episode: 1900, Length: 307, Avg Reward: 39.6694639785, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.07228064537\n",
      "[NOR] Episode: 1910, Length: 256, Avg Reward: 50.6319141259, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.380504816771\n",
      "[NOR] Episode: 1920, Length: 357, Avg Reward: 35.2408830839, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.01847183704\n",
      "[NOR] Episode: 1930, Length: 427, Avg Reward: 94.9240215982, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.08676671982\n",
      "[NOR] Episode: 1940, Length: 254, Avg Reward: 100.306685008, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.44350552559\n",
      "[NOR] Episode: 1950, Length: 389, Avg Reward: 79.0230289791, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.100231051445\n",
      "[NOR] Episode: 1960, Length: 429, Avg Reward: 47.9938379877, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0606906861067\n",
      "[NOR] Episode: 1970, Length: 280, Avg Reward: 45.9491266425, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.424884915352\n",
      "[NOR] Episode: 1980, Length: 281, Avg Reward: 82.7393225789, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.1629254818\n",
      "[NOR] Episode: 1990, Length: 478, Avg Reward: 80.8089534729, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 3.79249191284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-23 17:26:10,158] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/27/openaigym.video.2.25564.video002000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 2000, Length: 288, Avg Reward: 141.942679739, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.165142059326\n",
      "[NOR] Episode: 2010, Length: 457, Avg Reward: 150.630464735, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.13773488998\n",
      "[NOR] Episode: 2020, Length: 530, Avg Reward: 56.651510002, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.57807087898\n",
      "[NOR] Episode: 2030, Length: 637, Avg Reward: 96.1385635797, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.1858522892\n",
      "[NOR] Episode: 2040, Length: 421, Avg Reward: 127.57834515, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.00994777679443\n",
      "[NOR] Episode: 2050, Length: 571, Avg Reward: 125.094223443, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.10862416029\n",
      "[NOR] Episode: 2060, Length: 466, Avg Reward: 153.216891706, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.426585495472\n",
      "[NOR] Episode: 2070, Length: 492, Avg Reward: 178.768384365, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.227450966835\n",
      "[NOR] Episode: 2080, Length: 617, Avg Reward: 146.106488601, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.13456201553\n",
      "[NOR] Episode: 2090, Length: 409, Avg Reward: 186.688638004, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.379651010036\n",
      "[NOR] Episode: 2100, Length: 491, Avg Reward: 171.722451218, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.418731987476\n",
      "[NOR] Episode: 2110, Length: 433, Avg Reward: 161.101635194, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.21424114704\n",
      "[NOR] Episode: 2120, Length: 530, Avg Reward: 182.07841156, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.1153254509\n",
      "[NOR] Episode: 2130, Length: 408, Avg Reward: 144.539753512, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.642335176468\n",
      "[NOR] Episode: 2140, Length: 424, Avg Reward: 159.632986943, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0223872661591\n",
      "[NOR] Episode: 2150, Length: 372, Avg Reward: 133.059993382, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.806836009026\n",
      "[NOR] Episode: 2160, Length: 350, Avg Reward: 108.947608327, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.52514517307\n",
      "[NOR] Episode: 2170, Length: 328, Avg Reward: 187.947686583, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.150920420885\n",
      "[NOR] Episode: 2180, Length: 390, Avg Reward: 204.72798486, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.1341830194\n",
      "[NOR] Episode: 2190, Length: 382, Avg Reward: 204.828288592, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.426369309425\n",
      "[NOR] Episode: 2200, Length: 535, Avg Reward: 189.979433546, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.841019511223\n",
      "[NOR] Episode: 2210, Length: 731, Avg Reward: 208.930726525, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.659647464752\n",
      "[NOR] Episode: 2220, Length: 321, Avg Reward: 148.117950699, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.573336482048\n",
      "[MAX] Episode: 2225, Length: 336, Reward: 252.783971666, buffer_len: 500000\n",
      "[NOR] Episode: 2230, Length: 363, Avg Reward: 146.832385042, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.950258255\n",
      "[NOR] Episode: 2240, Length: 522, Avg Reward: 88.7450036426, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.251278400421\n",
      "[NOR] Episode: 2250, Length: 327, Avg Reward: 174.314276616, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.271948486567\n",
      "[NOR] Episode: 2260, Length: 517, Avg Reward: 169.026146809, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.393056094646\n",
      "[NOR] Episode: 2270, Length: 317, Avg Reward: 194.802191834, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.255712896585\n",
      "[NOR] Episode: 2280, Length: 430, Avg Reward: 165.123388824, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.644138276577\n",
      "[NOR] Episode: 2290, Length: 387, Avg Reward: 191.814698361, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.728381276131\n",
      "[NOR] Episode: 2300, Length: 414, Avg Reward: 112.473064562, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.00677059590816\n",
      "[NOR] Episode: 2310, Length: 342, Avg Reward: 176.527445539, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -3.01463150978\n",
      "[NOR] Episode: 2320, Length: 359, Avg Reward: 144.048167662, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.558572351933\n",
      "[NOR] Episode: 2330, Length: 266, Avg Reward: 143.281681979, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.516032576561\n",
      "[NOR] Episode: 2340, Length: 279, Avg Reward: 175.943048644, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.32197153568\n",
      "[NOR] Episode: 2350, Length: 312, Avg Reward: 209.822662045, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.58120059967\n",
      "[NOR] Episode: 2360, Length: 371, Avg Reward: 205.357447044, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.403262138367\n",
      "[NOR] Episode: 2370, Length: 241, Avg Reward: 180.824468542, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.92624056339\n",
      "[NOR] Episode: 2380, Length: 552, Avg Reward: 60.2252866525, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.577841043472\n",
      "[NOR] Episode: 2390, Length: 381, Avg Reward: 93.7959223256, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.15287196636\n",
      "[NOR] Episode: 2400, Length: 310, Avg Reward: 205.568604519, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.57962846756\n",
      "[NOR] Episode: 2410, Length: 294, Avg Reward: 206.880349525, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.39227247238\n",
      "[NOR] Episode: 2420, Length: 346, Avg Reward: 199.08617955, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.787298679352\n",
      "[NOR] Episode: 2430, Length: 461, Avg Reward: 210.322369811, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.248970001936\n",
      "[NOR] Episode: 2440, Length: 345, Avg Reward: 207.334775742, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.900601327419\n",
      "[NOR] Episode: 2450, Length: 364, Avg Reward: 186.902252375, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.41137957573\n",
      "[NOR] Episode: 2460, Length: 441, Avg Reward: 195.228012257, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.56500482559\n",
      "[NOR] Episode: 2470, Length: 290, Avg Reward: 208.629532888, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0088297277689\n",
      "[NOR] Episode: 2480, Length: 284, Avg Reward: 214.466145031, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0526159107685\n",
      "[NOR] Episode: 2490, Length: 302, Avg Reward: 215.81676999, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.738967716694\n",
      "[NOR] Episode: 2500, Length: 357, Avg Reward: 216.456768763, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -3.09372758865\n",
      "[NOR] Episode: 2510, Length: 414, Avg Reward: 208.019897838, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.50918638706\n",
      "[NOR] Episode: 2520, Length: 246, Avg Reward: 211.764566469, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.624124288559\n",
      "[NOR] Episode: 2530, Length: 307, Avg Reward: 218.435994772, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.53050065041\n",
      "[NOR] Episode: 2540, Length: 360, Avg Reward: 217.944774466, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.739780902863\n",
      "[NOR] Episode: 2550, Length: 276, Avg Reward: 171.896757771, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.116721630096\n",
      "[NOR] Episode: 2560, Length: 380, Avg Reward: 214.122416409, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.35416781902\n",
      "[NOR] Episode: 2570, Length: 337, Avg Reward: 204.961933802, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.0554568469524\n",
      "[NOR] Episode: 2580, Length: 395, Avg Reward: 212.435781116, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.60571241379\n",
      "[NOR] Episode: 2590, Length: 586, Avg Reward: 163.879257059, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.707569479942\n",
      "[NOR] Episode: 2600, Length: 858, Avg Reward: 185.866934924, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.256914794445\n",
      "[NOR] Episode: 2610, Length: 327, Avg Reward: 205.478224654, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.07709610462\n",
      "[NOR] Episode: 2620, Length: 302, Avg Reward: 198.497957816, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.106308266521\n",
      "[NOR] Episode: 2630, Length: 423, Avg Reward: 219.807038485, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.414274275303\n",
      "[NOR] Episode: 2640, Length: 293, Avg Reward: 210.57848882, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.68819773197\n",
      "[NOR] Episode: 2650, Length: 297, Avg Reward: 197.208308305, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.600650370121\n",
      "[NOR] Episode: 2660, Length: 323, Avg Reward: 217.013144055, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.898938059807\n",
      "[NOR] Episode: 2670, Length: 357, Avg Reward: 220.440434105, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.11243784428\n",
      "[NOR] Episode: 2680, Length: 405, Avg Reward: 207.031996213, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.118065416813\n",
      "[NOR] Episode: 2690, Length: 280, Avg Reward: 184.589703467, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -2.52291750908\n",
      "[MAX] Episode: 2691, Length: 250, Reward: 254.139746997, buffer_len: 500000\n",
      "[NOR] Episode: 2700, Length: 336, Avg Reward: 184.399203305, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.0288538336754\n",
      "[NOR] Episode: 2710, Length: 326, Avg Reward: 176.081007784, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.867857337\n",
      "[NOR] Episode: 2720, Length: 371, Avg Reward: 200.038179583, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.800558924675\n",
      "[NOR] Episode: 2730, Length: 658, Avg Reward: 219.279125402, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.255650609732\n",
      "[MAX] Episode: 2733, Length: 331, Reward: 254.568055111, buffer_len: 500000\n",
      "[NOR] Episode: 2740, Length: 405, Avg Reward: 200.038438902, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.63741850853\n",
      "[NOR] Episode: 2750, Length: 474, Avg Reward: 184.908396127, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.064290702343\n",
      "[NOR] Episode: 2760, Length: 252, Avg Reward: 214.398808505, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.22667050362\n",
      "[NOR] Episode: 2770, Length: 332, Avg Reward: 205.492125615, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.442782580853\n",
      "[NOR] Episode: 2780, Length: 388, Avg Reward: 216.276701314, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.796944856644\n",
      "[NOR] Episode: 2790, Length: 237, Avg Reward: 209.779781841, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.015111900866\n",
      "[NOR] Episode: 2800, Length: 286, Avg Reward: 216.020804996, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -7.58609008789\n",
      "[NOR] Episode: 2810, Length: 255, Avg Reward: 222.022271749, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.32669603825\n",
      "[NOR] Episode: 2820, Length: 642, Avg Reward: 211.590654163, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -4.11338567734\n",
      "[NOR] Episode: 2830, Length: 285, Avg Reward: 216.163182838, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -0.189367055893\n",
      "[NOR] Episode: 2840, Length: 289, Avg Reward: 222.34894775, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: 0.904056072235\n",
      "[NOR] Episode: 2850, Length: 304, Avg Reward: 208.000132227, e: 0.05, Learning Rate: 0.002, buffer_len: 500000\n",
      "Loss: -1.66645765305\n"
     ]
    }
   ],
   "source": [
    "k = 40000.\n",
    "model.fit(\n",
    "    env, print_step=10, \n",
    "    episodes=int(1e5), max_episode_length=10000, batch_size=32,\n",
    "    learning_rate = 0.002, # lambda t: 0.05 * k / (k + t)\n",
    "    e = interp1d([0, 300000], [0.4, 0.05], fill_value=0.05, bounds_error=False),\n",
    "    keep_prob = 0.5,\n",
    "    update_target_step = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 18:20:33,289] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "212.63485096\n",
      "222.209029125\n",
      "137.585769854\n",
      "180.875673014\n",
      "206.109792056\n",
      "244.175226865\n",
      "219.223174017\n",
      "226.254220443\n",
      "189.530083255\n",
      "180.200875147\n",
      "226.533856294\n",
      "215.507410864\n",
      "257.758179742\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <type 'exceptions.TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ac2829c8c03c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_polygon\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLunarLanderContinuous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLunarLander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/window/xlib/__init__.pyc\u001b[0m in \u001b[0;36mdispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# Check for the events specific to this window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         while xlib.XCheckWindowEvent(_x_display, _window,\n\u001b[0;32m--> 853\u001b[0;31m                                      0x1ffffff, byref(e)):\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;31m# Key events are filtered by the xlib window event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# handler so they get a shot at the prefiltered event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: <type 'exceptions.TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/{name}\".format(path = os.getcwd(), name = name)\n",
    "logs_path = \"{path}/logs/\".format(path = os.getcwd(), name = name)\n",
    "\n",
    "\n",
    "model_run = LunarLander(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path,\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total = 0.\n",
    "    ep = 0\n",
    "    while not done and ep < 700:\n",
    "        ep += 1\n",
    "        a = model_run.predict(s, 0.0)\n",
    "        s, r, done, info = env.step(a)\n",
    "        total += r\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "    \n",
    "    print(total)\n",
    "    \n",
    "env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
