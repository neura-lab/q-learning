{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tfinterface.model_base import ModelBase\n",
    "from tfinterface.reinforcement import ExperienceReplay\n",
    "from tfinterface.utils import select_columns, soft_if, get_run, map_gradients\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from tfinterface.reinforcement import ExpandedStateEnv\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "name = \"actor-critic-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Inputs(object):\n",
    "    def __init__(self, n_states, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.episode_length = tf.placeholder(tf.int64, [], name='episode_length')\n",
    "\n",
    "            self.s = tf.placeholder(tf.float32, [None, n_states], name='s')\n",
    "            self.a = tf.placeholder(tf.int32, [None], name='a')\n",
    "            self.r = tf.placeholder(tf.float32, [None], name='r')\n",
    "            self.v1 = tf.placeholder(tf.float32, [None], name='V1')\n",
    "            self.done = tf.placeholder(tf.float32, [None], name='done')\n",
    "            \n",
    "            self.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            self.keep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\n",
    "            self.training = tf.placeholder(tf.bool, [], name='training')\n",
    "            \n",
    "            self.pi = tf.placeholder(tf.float32, [], name='pi')\n",
    "            \n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, base_model, inputs, n_actions, n_states, y, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            \n",
    "            ops = dict(\n",
    "                trainable=True,\n",
    "                kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                use_bias=True\n",
    "            )\n",
    "\n",
    "            net = inputs.s\n",
    "\n",
    "            net = tf.layers.dense(net, 128, activation=tf.nn.relu, name=\"relu_layer\", **ops)\n",
    "#             net = tf.nn.dropout(net, inputs.keep_prob)\n",
    "            \n",
    "            net = tf.layers.dense(net, 128, activation=tf.nn.relu, name=\"relu_layer2\", **ops)\n",
    "#             net = tf.nn.dropout(net, inputs.keep_prob)\n",
    "            \n",
    "            self.V = tf.layers.dense(net, 1, name='V', **ops)[:, 0]\n",
    "\n",
    "            self.target = soft_if(inputs.done, inputs.r,  inputs.r + y * inputs.v1)\n",
    "\n",
    "            self.error = self.target - self.V\n",
    "            self.loss = Pipe(self.error, tf.nn.l2_loss, tf.reduce_mean)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "#             self.update = tf.train.AdamOptimizer(inputs.learning_rate).minimize(self.loss, var_list=self.variables)\n",
    "            trainer = tf.train.AdamOptimizer(inputs.learning_rate)\n",
    "            gradients = trainer.compute_gradients(self.loss, var_list=self.variables)\n",
    "            gradients = map_gradients(lambda g: tf.clip_by_norm(g, 1000), gradients)\n",
    "            self.update = trainer.apply_gradients(gradients)\n",
    "            \n",
    "            avg_error, std_error = tf.nn.moments(self.error, [0])\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('avg_target', tf.reduce_mean(self.target)),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.scalar('avg_error', avg_error),\n",
    "                tf.summary.scalar('std_error', std_error),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n",
    "class Actor(object):\n",
    "    def __init__(self, base_model, inputs, target_critic, n_actions, n_states, y, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            ops = dict(\n",
    "                trainable=True,\n",
    "                kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                use_bias=False\n",
    "            )\n",
    "\n",
    "            net = inputs.s\n",
    "\n",
    "            net = tf.layers.dense(net, 128, activation=tf.nn.relu, name=\"relu_layer\", **ops)\n",
    "            net = tf.nn.dropout(net, inputs.keep_prob)\n",
    "            \n",
    "            net = tf.layers.dense(net, 128, activation=tf.nn.relu, name=\"relu_layer2\", **ops)\n",
    "            net = tf.nn.dropout(net, inputs.keep_prob)\n",
    "\n",
    "            self.P = tf.layers.dense(net, n_actions, activation=tf.nn.softmax, name='P', **ops)\n",
    "            self.Pa = select_columns(self.P, inputs.a)\n",
    "            \n",
    "            r_pred = net\n",
    "#             r_pred = tf.concat([r_pred, tf.one_hot(inputs.a, n_actions)], 1)\n",
    "#             r_pred = tf.layers.dense(net, 32, activation=tf.nn.softmax, name='r_pred_relu', **ops)\n",
    "#             r_pred = tf.layers.dense(r_pred, 1, activation=tf.nn.softmax, name='r_pred', **ops)[:,0]\n",
    "#             r_loss = 0.01 * Pipe(r_pred - inputs.r, tf.nn.l2_loss, tf.reduce_mean)\n",
    "\n",
    "            self.loss = - tf.log(tf.clip_by_value(self.Pa, 1e-3, 1.0)) * target_critic.error\n",
    "            self.loss = tf.reduce_mean(self.loss)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "#             self.update = tf.train.AdamOptimizer(inputs.learning_rate).minimize(self.loss, var_list=self.variables)\n",
    "            trainer = tf.train.AdamOptimizer(inputs.learning_rate)\n",
    "            gradients = trainer.compute_gradients(self.loss, var_list=self.variables)\n",
    "            gradients = map_gradients(lambda g: tf.clip_by_norm(g, 1000), gradients)\n",
    "            self.update = trainer.apply_gradients(gradients)\n",
    "            \n",
    "            \n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LunarLander(ModelBase):\n",
    "    \n",
    "    def define_model(self, n_actions, n_states, y=0.98, buffer_length=50000, pi=0.1):\n",
    "        self.global_max = float('-inf')\n",
    "\n",
    "        self.positive_buffer = ExperienceReplay(max_length=buffer_length/2)\n",
    "        self.negative_buffer = ExperienceReplay(max_length=buffer_length/2)\n",
    "\n",
    "\n",
    "        with self.graph.as_default(), tf.device(\"cpu:0\"):\n",
    "\n",
    "            self.inputs = Inputs(n_states, \"inputs\")\n",
    "\n",
    "            self.critic = Critic(self, self.inputs, n_actions, n_states, y, \"critic\")\n",
    "            self.target_critic = Critic(self, self.inputs, n_actions, n_states, y, \"target_critic\")\n",
    "            self.actor = Actor(self, self.inputs, self.target_critic, n_actions, n_states, y, \"actor\")\n",
    "\n",
    "            self.update = tf.group(self.critic.update, self.actor.update)\n",
    "\n",
    "            self.episode_length_summary = tf.summary.scalar('episode_length', self.inputs.episode_length)\n",
    "\n",
    "            self.summaries = tf.summary.merge([self.actor.summaries, self.critic.summaries, self.target_critic.summaries])\n",
    "\n",
    "            self.update_target = tf.group(*[\n",
    "                t.assign_add(pi * (a - t)) for t, a in zip(self.target_critic.variables, self.critic.variables)\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    def predict_feed(self, S):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.keep_prob: 1.0,\n",
    "            self.inputs.training: False\n",
    "        }\n",
    "    \n",
    "    def predict(self, state, e = 0.0):\n",
    "        predict_feed = self.predict_feed([state])\n",
    "        actions = self.sess.run(self.actor.P, feed_dict=predict_feed)\n",
    "        actions = actions[0]\n",
    "        n = len(actions)\n",
    "\n",
    "        if random.random() < e:\n",
    "            return random.randint(0, n-1)\n",
    "        else:\n",
    "            return np.random.choice(n, p=actions)\n",
    "    \n",
    "    def fit_feed(self, S, A, R, V1, Done, learning_rate, keep_prob):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.a: A,\n",
    "            self.inputs.r: R,\n",
    "            self.inputs.v1: V1,\n",
    "            self.inputs.done: Done,\n",
    "            self.inputs.learning_rate: learning_rate,\n",
    "            self.inputs.keep_prob: keep_prob,\n",
    "            self.inputs.training: True\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def fit(self, env, keep_prob=0.5, e=0.01, learning_rate=0.01, print_step=10, \n",
    "            update_target_step = 32, episodes=100000, max_episode_length=float('inf'), batch_size=32,\n",
    "            min_buffer_size=10000):\n",
    "        \n",
    "        r_total = 0.\n",
    "        batch_size /= 2\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            ep_step = 0\n",
    "            s = env.reset()\n",
    "            episode_length = 0\n",
    "            ep_reward = 0.\n",
    "            \n",
    "            while not done and ep_step <= max_episode_length:\n",
    "                self.global_step += 1\n",
    "                episode_length += 1\n",
    "                ep_step += 1\n",
    "                \n",
    "                \n",
    "                _learning_rate = learning_rate(self.global_step) if hasattr(learning_rate, '__call__') else learning_rate\n",
    "                _e = e(self.global_step) if hasattr(e, '__call__') else e\n",
    "                \n",
    "                \n",
    "                a = self.predict(s, e = _e)\n",
    "                s1, r, done, info = env.step(a)\n",
    "                r = min(max(-10., r), 10.)\n",
    "                r_total += r\n",
    "                ep_reward += r\n",
    "                \n",
    "                \n",
    "                if r > 0:\n",
    "                    self.positive_buffer.append((s, a, r, s1, float(done)))\n",
    "                else:\n",
    "                    self.negative_buffer.append((s, a, r, s1, float(done)))\n",
    "                    \n",
    "                s = s1\n",
    "                \n",
    "                \n",
    "                \n",
    "                Sp, Ap, Rp, S1p, Donep = self.positive_buffer.random_batch(batch_size).unzip() if len(self.positive_buffer) > 0 else ([], [], [], [], [])\n",
    "                Sn, An, Rn, S1n, Donen = self.negative_buffer.random_batch(batch_size).unzip() if len(self.negative_buffer) > 0 else ([], [], [], [], [])\n",
    "                \n",
    "                S, A, R, S1, Done = Sp + Sn, Ap + An, Rp + Rn, S1p + S1n, Donep + Donen\n",
    "                \n",
    "                predict_feed = self.predict_feed(S1)\n",
    "                V1 = self.sess.run(self.target_critic.V, feed_dict=predict_feed)\n",
    "\n",
    "                \n",
    "                fit_feed = self.fit_feed(S, A, R, V1, Done, _learning_rate, keep_prob)\n",
    "                \n",
    "                if self.global_step < min_buffer_size:\n",
    "                    continue\n",
    "                \n",
    "                _, summaries = self.sess.run([self.update, self.summaries], feed_dict=fit_feed)\n",
    "                self.writer.add_summary(summaries, self.global_step)\n",
    "                \n",
    "                \n",
    "                if self.global_step % update_target_step == 0:\n",
    "                    self.sess.run(self.update_target)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            episode_length_summary = self.sess.run(self.episode_length_summary,\n",
    "                                                   feed_dict={self.inputs.episode_length: episode_length})\n",
    "            self.writer.add_summary(episode_length_summary, self.global_step)\n",
    "\n",
    "\n",
    "            if ep_reward >= self.global_max:\n",
    "                print(\"[MAX] Episode: {}, Length: {}, Reward: {}, global_step: {}\".format(episode, episode_length, ep_reward, self.global_step))\n",
    "                self.save(model_path = self.model_path + \".{score}\".format(score = ep_reward))\n",
    "                self.global_max = ep_reward\n",
    "\n",
    "\n",
    "            if episode % print_step == 0 and episode > 0:\n",
    "                avg_r = r_total / print_step\n",
    "                actor_loss = self.sess.run(self.actor.loss, feed_dict=fit_feed)\n",
    "                print(\"[NOR] Episode: {}, Length: {}, Avg Reward: {}, e: {}, Learning Rate: {}, global_step: {}\".format(episode, episode_length, avg_r, _e, _learning_rate, self.global_step))\n",
    "                print(\"Loss: {}\".format(actor_loss))\n",
    "                self.save()\n",
    "                r_total = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 20:19:09,274] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-70.9735026994\n",
      "-74.0987000802\n",
      "-54.3388840129\n",
      "-8.19298049388\n",
      "-29.1446292429\n",
      "-35.2294793741\n",
      "-54.9421207642\n",
      "-88.8666497516\n",
      "-85.8520058051\n",
      "-47.1022662128\n",
      "-14.0613811689\n",
      "-22.3488412596\n",
      "-99.5969495626\n",
      "7.95981789916\n",
      "-88.8510790949\n",
      "-71.2578354585\n",
      "-75.9225128488\n",
      "-97.2410543979\n",
      "-68.108061797\n",
      "-79.3475218677\n",
      "-55.5064504223\n",
      "-10.9989818182\n",
      "-92.6378808474\n",
      "-41.9122755541\n",
      "-91.4358301104\n",
      "-28.1542538685\n",
      "-71.0653980829\n",
      "-75.0239781528\n",
      "-63.997706834\n",
      "-7.81119253799\n",
      "-59.1140177305\n",
      "-87.3226045044\n",
      "-75.9597416635\n",
      "-53.2034266146\n",
      "-54.1755067129\n",
      "-11.9173622045\n",
      "-66.5861605856\n",
      "-45.9250337228\n",
      "-15.5142492584\n",
      "-25.913664303\n",
      "-44.7269481426\n",
      "-72.0487683056\n",
      "-55.2767374953\n",
      "-61.0235000794\n",
      "-23.3901501755\n",
      "-58.6979741191\n",
      "-70.5756089482\n",
      "-87.4173823443\n",
      "-56.5940433796\n",
      "-59.0612326571\n",
      "-89.314294611\n",
      "-46.8570076758\n",
      "-42.9373541291\n",
      "-8.24847127384\n",
      "-40.7610453435\n",
      "-25.2036793355\n",
      "-94.6075358144\n",
      "-69.8322152382\n",
      "-27.7779189039\n",
      "-65.2888103261\n",
      "-36.082711457\n",
      "-62.0687482812\n",
      "-82.9730800588\n",
      "-68.0168367261\n",
      "-76.5100144591\n",
      "-61.1240075065\n",
      "-1.36376967739\n",
      "-50.7125241122\n",
      "-50.878829937\n",
      "-87.8769471718\n",
      "-90.8768280832\n",
      "-21.8719109924\n",
      "-68.3125623856\n",
      "-58.6404640994\n",
      "-62.1095604301\n",
      "-73.7660502628\n",
      "-51.2680279249\n",
      "-57.7322084817\n",
      "10.1775162443\n",
      "-54.9694834915\n",
      "-63.6787583167\n",
      "-86.0421981269\n",
      "-91.2524688519\n",
      "-45.8584146222\n",
      "-85.4380895181\n",
      "-64.1484920358\n",
      "-38.5580870451\n",
      "-52.6271275245\n",
      "-87.0576729333\n",
      "-17.70543874\n",
      "-64.2501245987\n",
      "-59.7232192019\n",
      "-65.1123460838\n",
      "-81.9037213443\n",
      "-77.56153064\n",
      "-66.2521430965\n",
      "-57.7928404888\n",
      "-75.9638568109\n",
      "-57.5094849965\n",
      "-56.4730420188\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/models/{run}\".format(path = os.getcwd(), run = 75)\n",
    "logs_path = \"{path}/logs/\".format(path = os.getcwd(), name = name)\n",
    "\n",
    "\n",
    "model_run = LunarLander(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path,\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total = 0.\n",
    "    ep = 0\n",
    "    while not done and ep < 700:\n",
    "        ep += 1\n",
    "        a = model_run.predict(s, 0.)\n",
    "        s, r, done, info = env.step(a)\n",
    "        total += r\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "    \n",
    "    print(total)\n",
    "    \n",
    "env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
