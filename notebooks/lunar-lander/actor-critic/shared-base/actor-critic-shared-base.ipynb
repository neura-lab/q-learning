{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'module' object has no attribute '__module__'\n"
     ]
    }
   ],
   "source": [
    "from tfinterface.model_base import ModelBase\n",
    "from tfinterface.reinforcement import ExperienceReplay\n",
    "from tfinterface.utils import select_columns, soft_if, get_run, map_gradients\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from tfinterface.reinforcement import ExpandedStateEnv\n",
    "import os\n",
    "import time\n",
    "from itertools import groupby\n",
    "\n",
    "name = \"actor-critic-shared-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 0), (3, 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_dict(d, key, default, f):\n",
    "    if key in d:\n",
    "        d[key] = f(d[key])\n",
    "    else:\n",
    "        d[key] = default\n",
    "\n",
    "def combine_gradients(grads1, grads2):\n",
    "    d = {}\n",
    "    \n",
    "    for g, v in grads1 + grads2:\n",
    "        update_dict(d, v, g, (g1) -> g1 + g)\n",
    "    \n",
    "    return [ (g, v) for v, g in d.items() ]\n",
    "\n",
    "\n",
    "grads1 = [(5, 0), (3, 1)]\n",
    "grads2 = [(2, 0)]\n",
    "combine_gradients(grads1, grads2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/coconut/icoconut/root.py\", line 96, in cache\n",
      "    compiled = memoized_parse_sys(code)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/coconut/icoconut/root.py\", line 73, in memoized_parse_sys\n",
      "    return COMPILER.header_proc(memoized_parse_block(code), header=\"sys\", initial=\"none\")\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/coconut/icoconut/root.py\", line 66, in memoized_parse_block\n",
      "    raise result\n",
      "CoconutParseError: parsing failed (line 34)\n",
      "  self.V = (                 base.net                 |> tf.layers.dense(?, 32, name='relu_layer', activation=tf.nn.relu, **ops)                 |> tf.nn.dropout(?, inputs.keep_prob)                 |> tf.layers.dense(?, n_actions, name='V', **ops)                 |> (lambda net: net[:, 0])             )\n",
      "                                                                        ^\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "parsing failed\n  self.V = (                 base.net                 |> tf.layers.dense(?, 32, name='relu_layer', activation=tf.nn.relu, **ops)                 |> tf.nn.dropout(?, inputs.keep_prob)                 |> tf.layers.dense(?, n_actions, name='V', **ops)                 |> (lambda net: net[:, 0])             )",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m parsing failed\n  self.V = (                 base.net                 |> tf.layers.dense(?, 32, name='relu_layer', activation=tf.nn.relu, **ops)                 |> tf.nn.dropout(?, inputs.keep_prob)                 |> tf.layers.dense(?, n_actions, name='V', **ops)                 |> (lambda net: net[:, 0])             )\n"
     ]
    }
   ],
   "source": [
    "class Inputs(object):\n",
    "    def __init__(self, n_states, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.episode_length = tf.placeholder(tf.int64, [], name='episode_length')\n",
    "            self.episode_reward = tf.placeholder(tf.float32, [], name='episode_reward')\n",
    "\n",
    "            self.s = tf.placeholder(tf.float32, [None, n_states], name='s')\n",
    "            self.a = tf.placeholder(tf.int32, [None], name='a')\n",
    "            self.r = tf.placeholder(tf.float32, [None], name='r')\n",
    "            self.v1 = tf.placeholder(tf.float32, [None], name='V1')\n",
    "            self.done = tf.placeholder(tf.float32, [None], name='done')\n",
    "            \n",
    "            self.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            self.keep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\n",
    "            self.training = tf.placeholder(tf.bool, [], name='training')\n",
    "            \n",
    "            self.pi = tf.placeholder(tf.float32, [], name='pi')\n",
    "            \n",
    "\n",
    "class Base(object):\n",
    "    def __init__(self, inputs, n_states, scope, ops):\n",
    "        with tf.variable_scope(scope):\n",
    "            net = inputs.s\n",
    "\n",
    "            net = tf.layers.dense(net, 64, activation=tf.nn.relu, name=\"relu_layer\", use_bias=True, **ops)\n",
    "            self.net = tf.nn.dropout(net, inputs.keep_prob)\n",
    "            \n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, base, inputs, n_actions, n_states, y, scope, ops):\n",
    "        with tf.variable_scope(scope):\n",
    "            \n",
    "            self.V = (\n",
    "                base.net\n",
    "                |> tf.layers.dense(?, 32, name='relu_layer', activation=tf.nn.relu, **ops)\n",
    "                |> tf.nn.dropout(?, inputs.keep_prob)\n",
    "                |> tf.layers.dense(?, n_actions, name='V', **ops)\n",
    "                |> (lambda net: net[:, 0])\n",
    "            )\n",
    "\n",
    "            self.target = soft_if(inputs.done, inputs.r,  inputs.r + y * inputs.v1)\n",
    "\n",
    "            self.error = self.target - self.V\n",
    "            self.loss = Pipe(self.error, tf.nn.l2_loss, tf.reduce_mean)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope) + base.variables\n",
    "\n",
    "            self.gradients = tf.train.AdamOptimizer(inputs.learning_rate).compute_gradients(self.loss, var_list=self.variables)\n",
    "\n",
    "            avg_error, std_error = tf.nn.moments(self.error, [0])\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('avg_target', tf.reduce_mean(self.target)),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.scalar('avg_error', avg_error),\n",
    "                tf.summary.scalar('std_error', std_error),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n",
    "class Actor(object):\n",
    "    def __init__(self, base, inputs, target_critic, n_actions, n_states, y, scope, ops):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.P = (\n",
    "                base.net\n",
    "                |> tf.layers.dense(?, 32, name='relu_layer', activation=tf.nn.relu, **ops)\n",
    "                |> tf.nn.dropout(?, inputs.keep_prob)\n",
    "                |> tf.layers.dense(base.net, n_actions, activation=tf.nn.softmax, name='P', use_bias=False, **ops)\n",
    "            )\n",
    "            \n",
    "            \n",
    "            self.Pa = select_columns(self.P, inputs.a)\n",
    "\n",
    "            self.loss = - tf.log(tf.clip_by_value(self.Pa, 1e-3, 1.0)) * target_critic.error\n",
    "            self.loss = tf.reduce_mean(self.loss)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope) + base.variables\n",
    "\n",
    "            self.gradients = tf.train.AdamOptimizer(inputs.learning_rate).compute_gradients(self.loss, var_list=self.variables)\n",
    "\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, n_actions),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LunarLander(ModelBase):\n",
    "    \n",
    "    def define_model(self, n_actions, n_states, y=0.98, buffer_length=50000, pi=0.1, clip=10):\n",
    "        self.global_max = float('-inf')\n",
    "        self.replay_buffer = ExperienceReplay(max_length=buffer_length)\n",
    "        ops = dict(\n",
    "            trainable=True,\n",
    "            kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "            bias_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01)\n",
    "        )\n",
    "\n",
    "\n",
    "        with self.graph.as_default(), tf.device(\"cpu:0\"):\n",
    "\n",
    "            self.inputs = Inputs(n_states, \"inputs\")\n",
    "            \n",
    "            self.base = Base(self.inputs, n_states, \"base\", ops)\n",
    "            self.target_base = Base(self.inputs, n_states, \"target_base\", ops)\n",
    "            \n",
    "            self.critic = Critic(self.base, self.inputs, n_actions, n_states, y, \"critic\", ops)\n",
    "            self.target_critic = Critic(self.target_base, self.inputs, n_actions, n_states, y, \"target_critic\", ops)\n",
    "            \n",
    "            self.actor = Actor(self.base, self.inputs, self.target_critic, n_actions, n_states, y, \"actor\", ops)\n",
    "            \n",
    "            with tf.name_scope(\"combine_gradients\"):\n",
    "                self.gradients = (\n",
    "                    combine_gradients(self.actor.gradients, self.critic.gradients)\n",
    "                    |> map_gradients$(tf.clip_by_norm$(?, clip))\n",
    "                )\n",
    "            \n",
    "            self.update = tf.train.AdamOptimizer(self.inputs.learning_rate).apply_gradients(self.gradients)\n",
    "\n",
    "            self.episode_summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('episode_length', self.inputs.episode_length),\n",
    "                tf.summary.scalar('episode_reward', self.inputs.episode_reward)\n",
    "            ])\n",
    "\n",
    "            self.summaries = tf.summary.merge([self.actor.summaries, self.critic.summaries, self.target_critic.summaries])\n",
    "            \n",
    "            with tf.name_scope(\"update_targets\"):\n",
    "                self.update_target = tf.group(*[\n",
    "                    t.assign_add(pi * (a - t)) for t, a in zip(self.target_critic.variables, self.critic.variables)\n",
    "                ])\n",
    "    \n",
    "    \n",
    "    def predict_feed(self, S):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.keep_prob: 1.0,\n",
    "            self.inputs.training: False\n",
    "        }\n",
    "    \n",
    "    def predict(self, state, e = 0.0):\n",
    "        predict_feed = self.predict_feed([state])\n",
    "        actions = self.sess.run(self.actor.P, feed_dict=predict_feed)\n",
    "        actions = actions[0]\n",
    "        n = len(actions)\n",
    "\n",
    "        if random.random() < e:\n",
    "            return random.randint(0, n-1)\n",
    "        else:\n",
    "            return np.random.choice(n, p=actions)\n",
    "    \n",
    "    def fit_feed(self, S, A, R, V1, Done, learning_rate, keep_prob):\n",
    "        return {\n",
    "            self.inputs.s: S,\n",
    "            self.inputs.a: A,\n",
    "            self.inputs.r: R,\n",
    "            self.inputs.v1: V1,\n",
    "            self.inputs.done: Done,\n",
    "            self.inputs.learning_rate: learning_rate,\n",
    "            self.inputs.keep_prob: keep_prob,\n",
    "            self.inputs.training: True\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def fit(self, env, keep_prob=0.5, e=0.01, learning_rate=0.01, print_step=10, \n",
    "            update_target_step = 32, episodes=100000, max_episode_length=float('inf'), batch_size=32):\n",
    "        \n",
    "        r_total = 0.\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            ep_step = 0\n",
    "            s = env.reset()\n",
    "            episode_length = 0\n",
    "            ep_reward = 0.\n",
    "            \n",
    "            while not done and ep_step <= max_episode_length:\n",
    "                self.global_step += 1\n",
    "                episode_length += 1\n",
    "                ep_step += 1\n",
    "                \n",
    "                \n",
    "                _learning_rate = learning_rate(self.global_step) if hasattr(learning_rate, '__call__') else learning_rate\n",
    "                _e = e(self.global_step) if hasattr(e, '__call__') else e\n",
    "                \n",
    "                \n",
    "                a = self.predict(s, e = _e)\n",
    "                s1, r, done, info = env.step(a)\n",
    "                r_total += r\n",
    "                ep_reward += r\n",
    "                \n",
    "                \n",
    "                self.replay_buffer.append((s, a, r, s1, float(done)))\n",
    "                \n",
    "                \n",
    "                S, A, R, S1, Done = self.replay_buffer.random_batch(batch_size).unzip()\n",
    "                predict_feed = self.predict_feed(S1)\n",
    "                V1 = self.sess.run(self.target_critic.V, feed_dict=predict_feed)\n",
    "\n",
    "                \n",
    "                fit_feed = self.fit_feed(S, A, R, V1, Done, _learning_rate, keep_prob)\n",
    "                _, summaries = self.sess.run([self.update, self.summaries], feed_dict=fit_feed)\n",
    "                self.writer.add_summary(summaries, self.global_step)\n",
    "                \n",
    "                \n",
    "                if self.global_step % update_target_step == 0:\n",
    "                    self.sess.run(self.update_target)\n",
    "                \n",
    "                \n",
    "                s = s1\n",
    "                \n",
    "            \n",
    "            episode_summaries = self.sess.run(self.episode_summaries,feed_dict={\n",
    "                self.inputs.episode_length: episode_length,\n",
    "                self.inputs.episode_reward: ep_reward\n",
    "            })\n",
    "            self.writer.add_summary(episode_summaries, self.global_step)\n",
    "\n",
    "\n",
    "            if ep_reward >= self.global_max:\n",
    "                print(\"[MAX] Episode: {}, Length: {}, Reward: {}, buffer_len: {}\".format(episode, episode_length, ep_reward, len(self.replay_buffer)))\n",
    "                self.save(model_path = self.model_path + \".{score}\".format(score = ep_reward))\n",
    "                self.global_max = ep_reward\n",
    "\n",
    "\n",
    "            if episode % print_step == 0 and episode > 0:\n",
    "                avg_r = r_total / print_step\n",
    "                actor_loss = self.sess.run(self.actor.loss, feed_dict=fit_feed)\n",
    "                print(\"[NOR] Episode: {}, Length: {}, Avg Reward: {}, e: {}, Learning Rate: {}, buffer_len: {}\".format(episode, episode_length, avg_r, _e, _learning_rate, len(self.replay_buffer)))\n",
    "                print(\"Loss: {}\".format(actor_loss))\n",
    "                self.save()\n",
    "                r_total = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 16:42:23,485] Making new env: LunarLander-v2\n",
      "[2017-03-17 16:42:23,571] Creating monitor directory monitor/18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "run = get_run()\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = wrappers.Monitor(env, \"monitor/{run}\".format(run = run))\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/models/{name}\".format(path = os.getcwd(), name = name)\n",
    "logs_path = \"{path}/logs/{run}\".format(path = os.getcwd(), name = name, run = run)\n",
    "\n",
    "\n",
    "model = LunarLander(\n",
    "    n_actions, n_states, y=0.9999, \n",
    "    buffer_length=500000,\n",
    "    model_path = model_path,\n",
    "    logs_path = logs_path,\n",
    "    restore = False,\n",
    "    pi = 0.001,\n",
    "    clip = 1.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 16:42:24,272] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/shared-base/monitor/18/openaigym.video.13.4296.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 0, Length: 107, Reward: -203.012263263, buffer_len: 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 16:42:26,694] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/shared-base/monitor/18/openaigym.video.13.4296.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 1, Length: 103, Reward: -111.979307251, buffer_len: 210\n",
      "[MAX] Episode: 2, Length: 1000, Reward: 77.1124244404, buffer_len: 1210\n",
      "[MAX] Episode: 6, Length: 1000, Reward: 85.0371613394, buffer_len: 3095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 16:42:38,609] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/shared-base/monitor/18/openaigym.video.13.4296.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 10, Length: 229, Avg Reward: -183.836558465, e: 0.394752333333, Learning Rate: 0.005, buffer_len: 4499\n",
      "Loss: -2.69007825851\n",
      "[NOR] Episode: 20, Length: 96, Avg Reward: -166.329593138, e: 0.3888035, Learning Rate: 0.005, buffer_len: 9598\n",
      "Loss: -2.90823554993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 16:43:16,801] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/shared-base/monitor/18/openaigym.video.13.4296.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 30, Length: 1000, Avg Reward: -154.126588885, e: 0.383674833333, Learning Rate: 0.005, buffer_len: 13994\n",
      "Loss: -3.56683635712\n",
      "[NOR] Episode: 40, Length: 585, Avg Reward: -139.990539321, e: 0.3757415, Learning Rate: 0.005, buffer_len: 20794\n",
      "Loss: -5.59932327271\n",
      "[NOR] Episode: 50, Length: 1000, Avg Reward: -166.049754661, e: 0.370069166667, Learning Rate: 0.005, buffer_len: 25656\n",
      "Loss: -2.71342277527\n",
      "[NOR] Episode: 60, Length: 1000, Avg Reward: -146.766133836, e: 0.361911833333, Learning Rate: 0.005, buffer_len: 32648\n",
      "Loss: -2.85318017006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 16:44:48,240] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/shared-base/monitor/18/openaigym.video.13.4296.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 70, Length: 1000, Avg Reward: -124.471849263, e: 0.353283166667, Learning Rate: 0.005, buffer_len: 40044\n",
      "Loss: 0.575408935547\n",
      "[NOR] Episode: 80, Length: 239, Avg Reward: -212.367024655, e: 0.347483666667, Learning Rate: 0.005, buffer_len: 45015\n",
      "Loss: -2.27229118347\n",
      "[NOR] Episode: 90, Length: 1000, Avg Reward: -141.810697014, e: 0.337814333333, Learning Rate: 0.005, buffer_len: 53303\n",
      "Loss: -2.23347210884\n",
      "[NOR] Episode: 100, Length: 551, Avg Reward: -173.095287581, e: 0.330523833333, Learning Rate: 0.005, buffer_len: 59552\n",
      "Loss: -6.47022914886\n",
      "[NOR] Episode: 110, Length: 351, Avg Reward: -108.55942773, e: 0.319938666667, Learning Rate: 0.005, buffer_len: 68625\n",
      "Loss: -6.21079349518\n",
      "[NOR] Episode: 120, Length: 506, Avg Reward: -143.906085886, e: 0.311202666667, Learning Rate: 0.005, buffer_len: 76113\n",
      "Loss: -3.97128748894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 16:47:31,036] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/shared-base/monitor/18/openaigym.video.13.4296.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 130, Length: 1000, Avg Reward: -198.704841922, e: 0.301809833333, Learning Rate: 0.005, buffer_len: 84164\n",
      "Loss: -6.07128620148\n",
      "[NOR] Episode: 140, Length: 1000, Avg Reward: -154.708287606, e: 0.291416, Learning Rate: 0.005, buffer_len: 93073\n",
      "Loss: -4.73452615738\n",
      "[NOR] Episode: 150, Length: 1000, Avg Reward: -162.787895553, e: 0.281310333333, Learning Rate: 0.005, buffer_len: 101735\n",
      "Loss: -3.45110487938\n",
      "[NOR] Episode: 160, Length: 1000, Avg Reward: -132.949199284, e: 0.2706715, Learning Rate: 0.005, buffer_len: 110854\n",
      "Loss: -0.300063252449\n",
      "[NOR] Episode: 170, Length: 1000, Avg Reward: -114.672772384, e: 0.260135333333, Learning Rate: 0.005, buffer_len: 119885\n",
      "Loss: 0.494818031788\n",
      "[NOR] Episode: 180, Length: 1000, Avg Reward: -133.22803155, e: 0.249337833333, Learning Rate: 0.005, buffer_len: 129140\n",
      "Loss: -3.48935317993\n",
      "[NOR] Episode: 190, Length: 1000, Avg Reward: -131.574531183, e: 0.237671166667, Learning Rate: 0.005, buffer_len: 139140\n",
      "Loss: -4.56330966949\n",
      "[NOR] Episode: 200, Length: 1000, Avg Reward: -128.108713227, e: 0.2260045, Learning Rate: 0.005, buffer_len: 149140\n",
      "Loss: -2.68105435371\n",
      "[NOR] Episode: 210, Length: 1000, Avg Reward: -123.389357843, e: 0.214337833333, Learning Rate: 0.005, buffer_len: 159140\n",
      "Loss: -1.64010453224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 16:53:05,722] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/shared-base/monitor/18/openaigym.video.13.4296.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 220, Length: 1000, Avg Reward: -149.257113033, e: 0.203727, Learning Rate: 0.005, buffer_len: 168235\n",
      "Loss: -1.82857179642\n",
      "[NOR] Episode: 230, Length: 1000, Avg Reward: -136.781224701, e: 0.192060333333, Learning Rate: 0.005, buffer_len: 178235\n",
      "Loss: 0.729239583015\n",
      "[NOR] Episode: 240, Length: 1000, Avg Reward: -128.985386691, e: 0.181420333333, Learning Rate: 0.005, buffer_len: 187355\n",
      "Loss: -1.62600064278\n",
      "[NOR] Episode: 250, Length: 1000, Avg Reward: -133.786873503, e: 0.170719666667, Learning Rate: 0.005, buffer_len: 196527\n",
      "Loss: -4.27626991272\n",
      "[NOR] Episode: 260, Length: 1000, Avg Reward: -124.278950418, e: 0.159053, Learning Rate: 0.005, buffer_len: 206527\n",
      "Loss: -1.99479913712\n",
      "[NOR] Episode: 270, Length: 1000, Avg Reward: -152.247156899, e: 0.147386333333, Learning Rate: 0.005, buffer_len: 216527\n",
      "Loss: -3.87882232666\n",
      "[NOR] Episode: 280, Length: 1000, Avg Reward: -169.70437647, e: 0.1371115, Learning Rate: 0.005, buffer_len: 225334\n",
      "Loss: -3.2510368824\n",
      "[NOR] Episode: 290, Length: 1000, Avg Reward: -190.077280328, e: 0.125833333333, Learning Rate: 0.005, buffer_len: 235001\n",
      "Loss: -1.59224557877\n",
      "[NOR] Episode: 300, Length: 1000, Avg Reward: -183.22472229, e: 0.114745333333, Learning Rate: 0.005, buffer_len: 244505\n",
      "Loss: -1.7650783062\n",
      "[NOR] Episode: 310, Length: 1000, Avg Reward: -183.727819667, e: 0.103078666667, Learning Rate: 0.005, buffer_len: 254505\n",
      "Loss: -3.09960412979\n",
      "[NOR] Episode: 320, Length: 1000, Avg Reward: -166.212506086, e: 0.091412, Learning Rate: 0.005, buffer_len: 264505\n",
      "Loss: 0.423187166452\n",
      "[NOR] Episode: 330, Length: 836, Avg Reward: -186.888586919, e: 0.0805585, Learning Rate: 0.005, buffer_len: 273808\n",
      "Loss: -0.787787973881\n",
      "[NOR] Episode: 340, Length: 1000, Avg Reward: -199.237352024, e: 0.069698, Learning Rate: 0.005, buffer_len: 283117\n",
      "Loss: -1.78914320469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 17:00:50,273] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/shared-base/monitor/18/openaigym.video.13.4296.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 350, Length: 1000, Avg Reward: -167.306676351, e: 0.0580313333333, Learning Rate: 0.005, buffer_len: 293117\n",
      "Loss: -1.08064293861\n",
      "[NOR] Episode: 360, Length: 1000, Avg Reward: -161.174088126, e: 0.05, Learning Rate: 0.005, buffer_len: 303009\n",
      "Loss: -2.50993943214\n",
      "[NOR] Episode: 370, Length: 1000, Avg Reward: -163.284908394, e: 0.05, Learning Rate: 0.005, buffer_len: 312850\n",
      "Loss: -3.94488048553\n",
      "[NOR] Episode: 380, Length: 1000, Avg Reward: -148.452704446, e: 0.05, Learning Rate: 0.005, buffer_len: 322344\n",
      "Loss: -1.68650698662\n",
      "[NOR] Episode: 390, Length: 368, Avg Reward: -152.5871093, e: 0.05, Learning Rate: 0.005, buffer_len: 331712\n",
      "Loss: -1.82364737988\n",
      "[NOR] Episode: 400, Length: 1000, Avg Reward: -147.729386431, e: 0.05, Learning Rate: 0.005, buffer_len: 341712\n",
      "Loss: -2.0093536377\n",
      "[NOR] Episode: 410, Length: 1000, Avg Reward: -141.439928737, e: 0.05, Learning Rate: 0.005, buffer_len: 351712\n",
      "Loss: -2.58661818504\n",
      "[NOR] Episode: 420, Length: 1000, Avg Reward: -146.861370317, e: 0.05, Learning Rate: 0.005, buffer_len: 361712\n",
      "Loss: -1.15782415867\n",
      "[NOR] Episode: 430, Length: 1000, Avg Reward: -153.335480305, e: 0.05, Learning Rate: 0.005, buffer_len: 371712\n",
      "Loss: -0.95895087719\n",
      "[NOR] Episode: 440, Length: 1000, Avg Reward: -156.275770964, e: 0.05, Learning Rate: 0.005, buffer_len: 381712\n",
      "Loss: -0.674327254295\n",
      "[NOR] Episode: 450, Length: 1000, Avg Reward: -155.343469396, e: 0.05, Learning Rate: 0.005, buffer_len: 391712\n",
      "Loss: -2.72256016731\n",
      "[NOR] Episode: 460, Length: 1000, Avg Reward: -160.498641907, e: 0.05, Learning Rate: 0.005, buffer_len: 401712\n",
      "Loss: -1.41882324219\n",
      "[NOR] Episode: 470, Length: 1000, Avg Reward: -156.064171257, e: 0.05, Learning Rate: 0.005, buffer_len: 410732\n",
      "Loss: -1.06544077396\n",
      "[NOR] Episode: 480, Length: 1000, Avg Reward: -178.962271115, e: 0.05, Learning Rate: 0.005, buffer_len: 420092\n",
      "Loss: -2.43136835098\n",
      "[NOR] Episode: 490, Length: 1000, Avg Reward: -162.851912187, e: 0.05, Learning Rate: 0.005, buffer_len: 429779\n",
      "Loss: -3.25618219376\n",
      "[NOR] Episode: 500, Length: 1000, Avg Reward: -141.277002705, e: 0.05, Learning Rate: 0.005, buffer_len: 438603\n",
      "Loss: -1.92593371868\n",
      "[NOR] Episode: 510, Length: 1000, Avg Reward: -147.63129223, e: 0.05, Learning Rate: 0.005, buffer_len: 447622\n",
      "Loss: -1.29632270336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 17:11:49,041] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/shared-base/monitor/18/openaigym.video.13.4296.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 520, Length: 1000, Avg Reward: -157.997559895, e: 0.05, Learning Rate: 0.005, buffer_len: 457316\n",
      "Loss: -2.69373369217\n",
      "[NOR] Episode: 530, Length: 1000, Avg Reward: -135.72006186, e: 0.05, Learning Rate: 0.005, buffer_len: 467316\n",
      "Loss: -1.22300744057\n",
      "[NOR] Episode: 540, Length: 1000, Avg Reward: -136.443225475, e: 0.05, Learning Rate: 0.005, buffer_len: 477316\n",
      "Loss: -0.171847641468\n",
      "[NOR] Episode: 550, Length: 1000, Avg Reward: -167.210920983, e: 0.05, Learning Rate: 0.005, buffer_len: 486952\n",
      "Loss: -3.0469751358\n",
      "[NOR] Episode: 560, Length: 1000, Avg Reward: -140.870162724, e: 0.05, Learning Rate: 0.005, buffer_len: 496322\n",
      "Loss: -0.0920520424843\n",
      "[NOR] Episode: 570, Length: 1000, Avg Reward: -159.263061959, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -0.00500243902206\n",
      "[NOR] Episode: 580, Length: 198, Avg Reward: -159.439990167, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.48093414307\n",
      "[NOR] Episode: 590, Length: 571, Avg Reward: -181.326126216, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -2.46624326706\n",
      "[NOR] Episode: 600, Length: 1000, Avg Reward: -156.615536559, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -3.57840442657\n",
      "[NOR] Episode: 610, Length: 267, Avg Reward: -169.271676036, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.36286139488\n",
      "[NOR] Episode: 620, Length: 1000, Avg Reward: -135.561150245, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.87593412399\n",
      "[NOR] Episode: 630, Length: 1000, Avg Reward: -146.931608747, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -2.41592717171\n",
      "[NOR] Episode: 640, Length: 150, Avg Reward: -149.606765181, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.87699103355\n",
      "[NOR] Episode: 650, Length: 844, Avg Reward: -169.935191612, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.27334237099\n",
      "[NOR] Episode: 660, Length: 679, Avg Reward: -166.42578329, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -2.17625117302\n",
      "[NOR] Episode: 670, Length: 253, Avg Reward: -191.208482994, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -0.896328091621\n",
      "[NOR] Episode: 680, Length: 180, Avg Reward: -193.115416334, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -3.22278881073\n",
      "[NOR] Episode: 690, Length: 738, Avg Reward: -220.161965935, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -2.38467383385\n",
      "[NOR] Episode: 700, Length: 771, Avg Reward: -302.274313108, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: 0.389115869999\n",
      "[NOR] Episode: 710, Length: 312, Avg Reward: -429.537471505, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: 0.187065005302\n",
      "[NOR] Episode: 720, Length: 426, Avg Reward: -422.383747633, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.49136400223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 17:24:02,740] Starting new video recorder writing to /data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/shared-base/monitor/18/openaigym.video.13.4296.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 730, Length: 301, Avg Reward: -279.703557127, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -0.644678711891\n",
      "[NOR] Episode: 740, Length: 219, Avg Reward: -355.41284555, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: 3.04566311836\n",
      "[NOR] Episode: 750, Length: 225, Avg Reward: -384.149518644, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: 6.12514543533\n",
      "[NOR] Episode: 760, Length: 209, Avg Reward: -370.535625788, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -2.7898235321\n",
      "[NOR] Episode: 770, Length: 129, Avg Reward: -343.310018219, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -3.31606960297\n",
      "[NOR] Episode: 780, Length: 169, Avg Reward: -356.807460591, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -2.72251462936\n",
      "[NOR] Episode: 790, Length: 225, Avg Reward: -381.701390722, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.02618277073\n",
      "[NOR] Episode: 800, Length: 227, Avg Reward: -278.949455841, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -2.10021495819\n",
      "[NOR] Episode: 810, Length: 247, Avg Reward: -259.499245896, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -3.44589781761\n",
      "[NOR] Episode: 820, Length: 303, Avg Reward: -261.679465017, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -0.876663327217\n",
      "[MAX] Episode: 827, Length: 662, Reward: 85.2379653881, buffer_len: 500000\n",
      "[NOR] Episode: 830, Length: 545, Avg Reward: -82.3104243009, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -2.44255828857\n",
      "[MAX] Episode: 836, Length: 577, Reward: 104.927395167, buffer_len: 500000\n",
      "[MAX] Episode: 839, Length: 604, Reward: 118.917315691, buffer_len: 500000\n",
      "[NOR] Episode: 840, Length: 199, Avg Reward: -138.14509688, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -4.28940868378\n",
      "[NOR] Episode: 850, Length: 103, Avg Reward: -223.083438015, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -0.430790960789\n",
      "[NOR] Episode: 860, Length: 134, Avg Reward: -138.630013945, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: 17.315448761\n",
      "[NOR] Episode: 870, Length: 137, Avg Reward: -167.857188524, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.21096801758\n",
      "[NOR] Episode: 880, Length: 175, Avg Reward: -215.738616543, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -2.67561578751\n",
      "[NOR] Episode: 890, Length: 107, Avg Reward: -149.15013604, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -3.29157614708\n",
      "[NOR] Episode: 900, Length: 155, Avg Reward: -148.342408151, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -2.51112031937\n",
      "[NOR] Episode: 910, Length: 147, Avg Reward: -171.573798544, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -3.48142838478\n",
      "[NOR] Episode: 920, Length: 204, Avg Reward: -147.655767846, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -4.22072410583\n",
      "[NOR] Episode: 930, Length: 149, Avg Reward: -167.458613223, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.98278594017\n",
      "[NOR] Episode: 940, Length: 87, Avg Reward: -144.586636199, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -0.0681032389402\n",
      "[NOR] Episode: 950, Length: 73, Avg Reward: -147.614823933, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -2.68802309036\n",
      "[NOR] Episode: 960, Length: 145, Avg Reward: -165.299778022, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.46071350574\n",
      "[NOR] Episode: 970, Length: 1000, Avg Reward: -128.635922494, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.24457144737\n",
      "[MAX] Episode: 976, Length: 748, Reward: 129.852336359, buffer_len: 500000\n",
      "[NOR] Episode: 980, Length: 858, Avg Reward: -137.4527174, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.6225361824\n",
      "[NOR] Episode: 990, Length: 1000, Avg Reward: -143.169274568, e: 0.05, Learning Rate: 0.005, buffer_len: 500000\n",
      "Loss: -1.52473819256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 17:29:10,285] Finished writing results. You can upload them to the scoreboard via gym.upload(u'/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/shared-base/monitor/17')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e94a91f1b8a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcoconut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__coconut__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40000.\u001b[0m  \u001b[0;31m# line 1: k = 40000.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episode_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterp1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_target_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# line 2: model.fit(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-885cd006dc01>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, keep_prob, e, learning_rate, print_step, update_target_step, episodes, max_episode_length, batch_size)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# line 99:                 a = self.predict(s, e = _e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# line 100:                 s1, r, done, info = env.step(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mr_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m  \u001b[0;31m# line 101:                 r_total += r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mep_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m  \u001b[0;31m# line 102:                 ep_reward += r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/cristian/tfinterface/tfinterface/reinforcement/expanded_state_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/wrappers/monitoring.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/wrappers/time_limit.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyLinearImpulse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpulse_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/box2d/lunar_lander.pyc\u001b[0m in \u001b[0;36mBeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mcontactListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mBeginContact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 40000.\n",
    "model.fit(\n",
    "    env, print_step=10, \n",
    "    episodes=int(1e5), max_episode_length=10000, batch_size=32,\n",
    "    learning_rate = 0.005, # lambda t: 0.05 * k / (k + t)\n",
    "    e = interp1d([0, 300000], [0.4, 0.05], fill_value=0.05, bounds_error=False),\n",
    "    keep_prob = 0.5,\n",
    "    update_target_step = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-17 12:07:02,890] Making new env: LunarLander-v2\n",
      "[2017-03-17 12:07:02,892] Finished writing results. You can upload them to the scoreboard via gym.upload('/data/neura-lab/q-learning/notebooks/lunar-lander/actor-critic/base/monitor/actor-critic-base')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "196.104081847\n",
      "149.867198645\n",
      "215.673060563\n",
      "217.593832845\n",
      "201.192692875\n",
      "198.152117614\n",
      "-35.4402700038\n",
      "218.394082887\n",
      "193.044127714\n",
      "120.793062792\n",
      "234.628837747\n",
      "213.755804367\n",
      "0.398475525016\n",
      "219.955313615\n",
      "219.817810851\n",
      "-31.113964878\n",
      "204.808844432\n",
      "218.3261461\n",
      "223.759720668\n",
      "230.32005815\n",
      "192.594561856\n",
      "192.463367622\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ac2829c8c03c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = ExpandedStateEnv(env, 3)\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path =  \"{path}/{name}\".format(path = os.getcwd(), name = name)\n",
    "logs_path = \"{path}/logs/\".format(path = os.getcwd(), name = name)\n",
    "\n",
    "\n",
    "model_run = LunarLander(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path,\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total = 0.\n",
    "    ep = 0\n",
    "    while not done and ep < 700:\n",
    "        ep += 1\n",
    "        a = model_run.predict(s, 0.0)\n",
    "        s, r, done, info = env.step(a)\n",
    "        total += r\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "    \n",
    "    print(total)\n",
    "    \n",
    "env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Coconut",
   "language": "coconut",
   "name": "coconut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3.6
   },
   "file_extension": ".coco",
   "mimetype": "text/x-python3",
   "name": "coconut",
   "pygments_lexer": "coconut"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
