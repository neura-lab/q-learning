{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "from tfinterface.reinforcement import DQN, ExpandedStateEnv\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "import numbers\n",
    "\n",
    "\n",
    "\n",
    "def get_run():\n",
    "    try:\n",
    "        with open(\"run.txt\") as f:\n",
    "            run = int(f.read().split(\"/n\")[0])\n",
    "    except:\n",
    "        run = -1\n",
    "    \n",
    "    with open(\"run.txt\", 'w+') as f:\n",
    "        run += 1\n",
    "        \n",
    "        f.seek(0)\n",
    "        f.write(str(run))\n",
    "        f.truncate()\n",
    "        \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-03 14:10:54,022] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "run = get_run()\n",
    "env_logs = '/tmp/cartpole-{}'.format(run)\n",
    "expansion = 3\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "# env = Monitor(env, env_logs)\n",
    "env = ExpandedStateEnv(env, expansion)\n",
    "                \n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * expansion\n",
    "model_path = os.getcwd() + \"/Q-network-full.model\"\n",
    "logs_path = \"logs/run{}\".format(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LunarDQN(DQN):\n",
    "    \n",
    "    def define_Qs(self, inputs, n_actions, n_states):\n",
    "        ops = dict(\n",
    "            trainable=True,\n",
    "            kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "            use_bias=False,\n",
    "            bias_initializer=None\n",
    "        )\n",
    "\n",
    "        net = tf.layers.dense(inputs.s, 64, activation=tf.nn.relu, name='relu_layer', **ops)\n",
    "        net = tf.layers.dense(inputs.s, 32, activation=tf.nn.relu, name='relu_layer2', **ops)\n",
    "        return tf.layers.dense(inputs.s, n_actions, name='linear_layer', **ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 70,\n",
      " s: Tensor(\"inputs/s:0\", shape=(?, 24), dtype=float32, device=/device:CPU:0),\n",
      " a: Tensor(\"inputs/a:0\", shape=(?,), dtype=int32, device=/device:CPU:0),\n",
      " r: Tensor(\"inputs/r:0\", shape=(?,), dtype=float32, device=/device:CPU:0),\n",
      " Qs: Tensor(\"network/linear_layer/MatMul:0\", shape=(?, 4), dtype=float32, device=/device:CPU:0),\n",
      " update: name: \"network/Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^network/Adam/update_network/linear_layer/kernel/ApplyAdam\"\n",
      "input: \"^network/Adam/Assign\"\n",
      "input: \"^network/Adam/Assign_1\"\n",
      "device: \"/device:CPU:0\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LunarDQN(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path,\n",
    "    logs_path = logs_path,\n",
    "    flush_secs = 3.0,\n",
    "    y = 0.98,\n",
    "    buffer_length=500000,\n",
    "    restore = False\n",
    ")\n",
    "\n",
    "print(\"run: {},\\n s: {},\\n a: {},\\n r: {},\\n Qs: {},\\n update: {}\".format(\n",
    "    run, model.inputs.s, model.inputs.a, model.inputs.r, model.network.Qs, model.update\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX] Episode: 0, Reward: -427.44619018, e: 0.1, learning_rate: 0.01, buffer_len: 90, episode_length: 90\n",
      "[NOR] Episode: 10, avg reward: -286.982311246, e: 0.1, learning_rate: 0.01, buffer_len: 740, episode_length: 60\n",
      "[NOR] Episode: 20, avg reward: -181.130319211, e: 0.1, learning_rate: 0.01, buffer_len: 1388, episode_length: 56\n",
      "[NOR] Episode: 30, avg reward: -242.727730193, e: 0.1, learning_rate: 0.01, buffer_len: 2354, episode_length: 65\n",
      "[NOR] Episode: 40, avg reward: -238.480080357, e: 0.1, learning_rate: 0.01, buffer_len: 3070, episode_length: 64\n",
      "[NOR] Episode: 50, avg reward: -243.644812841, e: 0.1, learning_rate: 0.01, buffer_len: 3738, episode_length: 52\n",
      "[NOR] Episode: 60, avg reward: -365.74857608, e: 0.1, learning_rate: 0.01, buffer_len: 4469, episode_length: 68\n",
      "[NOR] Episode: 70, avg reward: -314.768971952, e: 0.1, learning_rate: 0.01, buffer_len: 5167, episode_length: 55\n",
      "[NOR] Episode: 80, avg reward: -292.654297829, e: 0.1, learning_rate: 0.01, buffer_len: 5857, episode_length: 63\n",
      "[NOR] Episode: 90, avg reward: -274.794022053, e: 0.1, learning_rate: 0.01, buffer_len: 6532, episode_length: 59\n",
      "[NOR] Episode: 100, avg reward: -337.29878289, e: 0.1, learning_rate: 0.01, buffer_len: 7339, episode_length: 132\n",
      "[NOR] Episode: 110, avg reward: -298.128354506, e: 0.1, learning_rate: 0.01, buffer_len: 8153, episode_length: 57\n",
      "[NOR] Episode: 120, avg reward: -298.369397818, e: 0.1, learning_rate: 0.01, buffer_len: 8900, episode_length: 66\n",
      "[NOR] Episode: 130, avg reward: -238.667810887, e: 0.1, learning_rate: 0.01, buffer_len: 9828, episode_length: 145\n",
      "[NOR] Episode: 140, avg reward: -272.146438488, e: 0.1, learning_rate: 0.01, buffer_len: 10544, episode_length: 68\n",
      "[NOR] Episode: 150, avg reward: -392.395855865, e: 0.1, learning_rate: 0.01, buffer_len: 11531, episode_length: 112\n",
      "[NOR] Episode: 160, avg reward: -276.477981779, e: 0.1, learning_rate: 0.01, buffer_len: 12465, episode_length: 53\n",
      "[NOR] Episode: 170, avg reward: -257.280124225, e: 0.1, learning_rate: 0.01, buffer_len: 13349, episode_length: 72\n",
      "[NOR] Episode: 180, avg reward: -345.544169009, e: 0.1, learning_rate: 0.01, buffer_len: 14279, episode_length: 120\n",
      "[NOR] Episode: 190, avg reward: -295.484659797, e: 0.1, learning_rate: 0.01, buffer_len: 15240, episode_length: 88\n",
      "[NOR] Episode: 200, avg reward: -310.934632821, e: 0.1, learning_rate: 0.01, buffer_len: 16351, episode_length: 142\n",
      "[NOR] Episode: 210, avg reward: -463.156417479, e: 0.1, learning_rate: 0.01, buffer_len: 17476, episode_length: 126\n",
      "[NOR] Episode: 220, avg reward: -298.616447343, e: 0.1, learning_rate: 0.01, buffer_len: 18912, episode_length: 211\n",
      "[NOR] Episode: 230, avg reward: -358.381124428, e: 0.1, learning_rate: 0.01, buffer_len: 20369, episode_length: 81\n",
      "[NOR] Episode: 240, avg reward: -383.372909775, e: 0.1, learning_rate: 0.01, buffer_len: 21892, episode_length: 137\n",
      "[NOR] Episode: 250, avg reward: -230.869030934, e: 0.1, learning_rate: 0.01, buffer_len: 23224, episode_length: 263\n",
      "[NOR] Episode: 260, avg reward: -272.371121979, e: 0.1, learning_rate: 0.01, buffer_len: 24991, episode_length: 242\n",
      "[NOR] Episode: 270, avg reward: -293.969860889, e: 0.1, learning_rate: 0.01, buffer_len: 26738, episode_length: 145\n",
      "[NOR] Episode: 280, avg reward: -312.104953421, e: 0.1, learning_rate: 0.01, buffer_len: 28488, episode_length: 130\n",
      "[NOR] Episode: 290, avg reward: -353.676364073, e: 0.1, learning_rate: 0.01, buffer_len: 30081, episode_length: 147\n",
      "[NOR] Episode: 300, avg reward: -354.790421099, e: 0.1, learning_rate: 0.01, buffer_len: 31808, episode_length: 111\n",
      "[NOR] Episode: 310, avg reward: -403.316230425, e: 0.1, learning_rate: 0.01, buffer_len: 33429, episode_length: 193\n",
      "[NOR] Episode: 320, avg reward: -249.666692909, e: 0.1, learning_rate: 0.01, buffer_len: 34890, episode_length: 168\n",
      "[NOR] Episode: 330, avg reward: -367.090849686, e: 0.1, learning_rate: 0.01, buffer_len: 36331, episode_length: 125\n",
      "[NOR] Episode: 340, avg reward: -257.914400105, e: 0.1, learning_rate: 0.01, buffer_len: 37695, episode_length: 117\n",
      "[NOR] Episode: 350, avg reward: -439.470661099, e: 0.1, learning_rate: 0.01, buffer_len: 38883, episode_length: 111\n",
      "[NOR] Episode: 360, avg reward: -295.292733234, e: 0.1, learning_rate: 0.01, buffer_len: 40258, episode_length: 107\n",
      "[NOR] Episode: 370, avg reward: -349.144180937, e: 0.1, learning_rate: 0.01, buffer_len: 41420, episode_length: 108\n",
      "[NOR] Episode: 380, avg reward: -313.519682263, e: 0.1, learning_rate: 0.01, buffer_len: 42620, episode_length: 151\n",
      "[NOR] Episode: 390, avg reward: -395.125081298, e: 0.1, learning_rate: 0.01, buffer_len: 43873, episode_length: 184\n",
      "[NOR] Episode: 400, avg reward: -374.227018892, e: 0.1, learning_rate: 0.01, buffer_len: 45178, episode_length: 163\n",
      "[NOR] Episode: 410, avg reward: -276.008692778, e: 0.1, learning_rate: 0.01, buffer_len: 46519, episode_length: 146\n",
      "[NOR] Episode: 420, avg reward: -300.412767327, e: 0.1, learning_rate: 0.01, buffer_len: 47685, episode_length: 82\n",
      "[NOR] Episode: 430, avg reward: -383.767176121, e: 0.1, learning_rate: 0.01, buffer_len: 49122, episode_length: 138\n",
      "[NOR] Episode: 440, avg reward: -417.275954842, e: 0.1, learning_rate: 0.01, buffer_len: 50477, episode_length: 112\n",
      "[NOR] Episode: 450, avg reward: -284.02163945, e: 0.1, learning_rate: 0.01, buffer_len: 51774, episode_length: 102\n",
      "[NOR] Episode: 460, avg reward: -383.50471301, e: 0.1, learning_rate: 0.01, buffer_len: 52982, episode_length: 100\n",
      "[NOR] Episode: 470, avg reward: -447.64329634, e: 0.1, learning_rate: 0.01, buffer_len: 54115, episode_length: 120\n",
      "[NOR] Episode: 480, avg reward: -341.121506809, e: 0.1, learning_rate: 0.01, buffer_len: 55269, episode_length: 91\n",
      "[NOR] Episode: 490, avg reward: -326.551124974, e: 0.1, learning_rate: 0.01, buffer_len: 56540, episode_length: 133\n",
      "[NOR] Episode: 500, avg reward: -356.344951674, e: 0.1, learning_rate: 0.01, buffer_len: 57691, episode_length: 119\n",
      "[NOR] Episode: 510, avg reward: -360.21340113, e: 0.1, learning_rate: 0.01, buffer_len: 58873, episode_length: 148\n",
      "[NOR] Episode: 520, avg reward: -433.307712834, e: 0.1, learning_rate: 0.01, buffer_len: 60011, episode_length: 143\n",
      "[NOR] Episode: 530, avg reward: -328.46542743, e: 0.1, learning_rate: 0.01, buffer_len: 61123, episode_length: 101\n",
      "[NOR] Episode: 540, avg reward: -272.512573615, e: 0.1, learning_rate: 0.01, buffer_len: 62273, episode_length: 153\n",
      "[NOR] Episode: 550, avg reward: -444.977233715, e: 0.1, learning_rate: 0.01, buffer_len: 63545, episode_length: 97\n",
      "[NOR] Episode: 560, avg reward: -485.960343913, e: 0.1, learning_rate: 0.01, buffer_len: 64683, episode_length: 106\n",
      "[NOR] Episode: 570, avg reward: -475.831637953, e: 0.1, learning_rate: 0.01, buffer_len: 65849, episode_length: 125\n",
      "[NOR] Episode: 580, avg reward: -457.706761594, e: 0.1, learning_rate: 0.01, buffer_len: 67033, episode_length: 164\n",
      "[NOR] Episode: 590, avg reward: -313.358844289, e: 0.1, learning_rate: 0.01, buffer_len: 68081, episode_length: 98\n",
      "[NOR] Episode: 600, avg reward: -445.308916834, e: 0.1, learning_rate: 0.01, buffer_len: 68962, episode_length: 86\n",
      "[NOR] Episode: 610, avg reward: -391.38912075, e: 0.1, learning_rate: 0.01, buffer_len: 70155, episode_length: 119\n",
      "[NOR] Episode: 620, avg reward: -390.268204894, e: 0.1, learning_rate: 0.01, buffer_len: 71095, episode_length: 93\n",
      "[NOR] Episode: 630, avg reward: -504.913015153, e: 0.1, learning_rate: 0.01, buffer_len: 72073, episode_length: 96\n",
      "[NOR] Episode: 640, avg reward: -434.352767638, e: 0.1, learning_rate: 0.01, buffer_len: 73146, episode_length: 114\n",
      "[NOR] Episode: 650, avg reward: -486.303457151, e: 0.1, learning_rate: 0.01, buffer_len: 74339, episode_length: 201\n",
      "[NOR] Episode: 660, avg reward: -478.736288867, e: 0.1, learning_rate: 0.01, buffer_len: 75562, episode_length: 108\n",
      "[NOR] Episode: 670, avg reward: -456.321772751, e: 0.1, learning_rate: 0.01, buffer_len: 76697, episode_length: 92\n",
      "[NOR] Episode: 680, avg reward: -372.415885918, e: 0.1, learning_rate: 0.01, buffer_len: 77874, episode_length: 117\n",
      "[NOR] Episode: 690, avg reward: -325.614941266, e: 0.1, learning_rate: 0.01, buffer_len: 79040, episode_length: 103\n",
      "[NOR] Episode: 700, avg reward: -371.396563223, e: 0.1, learning_rate: 0.01, buffer_len: 80231, episode_length: 137\n",
      "[NOR] Episode: 710, avg reward: -404.693886108, e: 0.1, learning_rate: 0.01, buffer_len: 81467, episode_length: 115\n",
      "[NOR] Episode: 720, avg reward: -333.213309455, e: 0.1, learning_rate: 0.01, buffer_len: 82864, episode_length: 138\n",
      "[NOR] Episode: 730, avg reward: -360.921827804, e: 0.1, learning_rate: 0.01, buffer_len: 84145, episode_length: 132\n",
      "[NOR] Episode: 740, avg reward: -362.457130995, e: 0.1, learning_rate: 0.01, buffer_len: 85381, episode_length: 120\n",
      "[NOR] Episode: 750, avg reward: -400.407745427, e: 0.1, learning_rate: 0.01, buffer_len: 86700, episode_length: 141\n",
      "[NOR] Episode: 760, avg reward: -326.558854641, e: 0.1, learning_rate: 0.01, buffer_len: 87889, episode_length: 96\n",
      "[NOR] Episode: 770, avg reward: -458.951679234, e: 0.1, learning_rate: 0.01, buffer_len: 89099, episode_length: 89\n",
      "[NOR] Episode: 780, avg reward: -388.015868978, e: 0.1, learning_rate: 0.01, buffer_len: 90437, episode_length: 142\n",
      "[NOR] Episode: 790, avg reward: -376.963521115, e: 0.1, learning_rate: 0.01, buffer_len: 91849, episode_length: 140\n",
      "[NOR] Episode: 800, avg reward: -351.366265408, e: 0.1, learning_rate: 0.01, buffer_len: 93043, episode_length: 159\n",
      "[NOR] Episode: 810, avg reward: -371.152401056, e: 0.1, learning_rate: 0.01, buffer_len: 94263, episode_length: 93\n",
      "[NOR] Episode: 820, avg reward: -371.038859161, e: 0.1, learning_rate: 0.01, buffer_len: 95543, episode_length: 123\n",
      "[NOR] Episode: 830, avg reward: -394.108070027, e: 0.1, learning_rate: 0.01, buffer_len: 96881, episode_length: 161\n",
      "[NOR] Episode: 840, avg reward: -337.308101188, e: 0.1, learning_rate: 0.01, buffer_len: 98071, episode_length: 73\n",
      "[NOR] Episode: 850, avg reward: -410.478011782, e: 0.1, learning_rate: 0.01, buffer_len: 99114, episode_length: 83\n",
      "[NOR] Episode: 860, avg reward: -400.738796184, e: 0.1, learning_rate: 0.01, buffer_len: 100395, episode_length: 124\n",
      "[NOR] Episode: 870, avg reward: -359.491005763, e: 0.1, learning_rate: 0.01, buffer_len: 101469, episode_length: 109\n",
      "[NOR] Episode: 880, avg reward: -430.46278473, e: 0.1, learning_rate: 0.01, buffer_len: 102779, episode_length: 107\n",
      "[NOR] Episode: 890, avg reward: -331.56321129, e: 0.1, learning_rate: 0.01, buffer_len: 103882, episode_length: 105\n",
      "[NOR] Episode: 900, avg reward: -349.542762717, e: 0.1, learning_rate: 0.01, buffer_len: 104878, episode_length: 97\n",
      "[NOR] Episode: 910, avg reward: -379.174235146, e: 0.1, learning_rate: 0.01, buffer_len: 105845, episode_length: 76\n",
      "[NOR] Episode: 920, avg reward: -460.269071114, e: 0.1, learning_rate: 0.01, buffer_len: 107098, episode_length: 91\n",
      "[NOR] Episode: 930, avg reward: -486.978642861, e: 0.1, learning_rate: 0.01, buffer_len: 108361, episode_length: 84\n",
      "[NOR] Episode: 940, avg reward: -436.92901612, e: 0.1, learning_rate: 0.01, buffer_len: 109536, episode_length: 84\n",
      "[NOR] Episode: 950, avg reward: -377.592918855, e: 0.1, learning_rate: 0.01, buffer_len: 110961, episode_length: 198\n",
      "[NOR] Episode: 960, avg reward: -442.299033272, e: 0.1, learning_rate: 0.01, buffer_len: 112132, episode_length: 137\n",
      "[NOR] Episode: 970, avg reward: -442.026512399, e: 0.1, learning_rate: 0.01, buffer_len: 113489, episode_length: 83\n",
      "[NOR] Episode: 980, avg reward: -409.516295262, e: 0.1, learning_rate: 0.01, buffer_len: 114773, episode_length: 175\n",
      "[NOR] Episode: 990, avg reward: -428.829660688, e: 0.1, learning_rate: 0.01, buffer_len: 116211, episode_length: 102\n",
      "[NOR] Episode: 1000, avg reward: -365.484269654, e: 0.1, learning_rate: 0.01, buffer_len: 117403, episode_length: 93\n",
      "[NOR] Episode: 1010, avg reward: -498.174404376, e: 0.1, learning_rate: 0.01, buffer_len: 118545, episode_length: 71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-06d27bfb4fbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_episode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#lambda t: max(0.001, k / (k + t)),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;31m#interp1d([0, 4000], [1, 0.05], fill_value=0.05, bounds_error=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/home/cristian/data/cristian/tfinterface/tfinterface/reinforcement/dnq.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, learning_rate, e, print_step, episodes, max_episode_length, discount, batch_size)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                 \u001b[0mMaxQs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_Qs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mS1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxQs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_learning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 5000.\n",
    "model.fit(\n",
    "    env, \n",
    "    episodes=50000,\n",
    "    max_episode_length = 2000,\n",
    "    learning_rate = 0.01, #lambda t: max(0.001, k / (k + t)),\n",
    "    e = 0.1 #interp1d([0, 4000], [1, 0.05], fill_value=0.05, bounds_error=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model_run = DQN(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path + \".max\",\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "s = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    a = model_run.choose_action(s, e=0.2)\n",
    "    s, r, done, info = env.step(a)\n",
    "    env.render()\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
