{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from phi.api import *\n",
    "import tensorflow as tf\n",
    "from tfinterface.reinforcement import OnBatchModel, OnExperienceTrainer, ExperienceReplay\n",
    "from tfinterface.interfaces import EnvironmentInterface\n",
    "from tfinterface.model_base import ModelBase\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "import numbers\n",
    "\n",
    "\n",
    "\n",
    "def get_run():\n",
    "    try:\n",
    "        with open(\"run.txt\") as f:\n",
    "            run = int(f.read().split(\"/n\")[0])\n",
    "    except:\n",
    "        run = -1\n",
    "    \n",
    "    with open(\"run.txt\", 'w+') as f:\n",
    "        run += 1\n",
    "        \n",
    "        f.seek(0)\n",
    "        f.write(str(run))\n",
    "        f.truncate()\n",
    "        \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-02 23:08:28,037] Making new env: CartPole-v1\n"
     ]
    }
   ],
   "source": [
    "def select_columns(tensor, indexes):\n",
    "    idx = tf.stack((tf.range(tf.shape(indexes)[0]), indexes), 1)\n",
    "    return tf.gather_nd(tensor, idx)\n",
    "\n",
    "\n",
    "def soft_if(cond, then, else_):\n",
    "    return (cond * then) + (1.0 - cond) * else_\n",
    "    \n",
    "\n",
    "def clip(gradients, clip_fun):\n",
    "    return [ (clip_fun(g), v) for g, v in gradients ]\n",
    "\n",
    "\n",
    "class ExpandedStateEnv(EnvironmentInterface):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = gym.make(env) if type(env) is str else env\n",
    "        \n",
    "    def reset(self):\n",
    "        s = self.env.reset()\n",
    "        self.s = np.hstack((s,s,s))\n",
    "        return self.s\n",
    "    \n",
    "    def step(self, a):\n",
    "        s, r, done, info = self.env.step(a)\n",
    "        n = len(s)\n",
    "        self.s = np.hstack((self.s[n:], s))\n",
    "        \n",
    "        return self.s, r, done, info\n",
    "    \n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self.env, attr)\n",
    "    \n",
    "\n",
    "\n",
    "class Inputs(object):\n",
    "    def __init__(self, n_actions, n_states, y, buffer_length, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.episode_length = tf.placeholder(tf.int64, [], name='episode_length')\n",
    "\n",
    "            self.s = tf.placeholder(tf.float32, [None, n_states], name='s')\n",
    "            self.a = tf.placeholder(tf.int32, [None], name='a')\n",
    "            self.r = tf.placeholder(tf.float32, [None], name='r')\n",
    "\n",
    "            self.done = tf.placeholder(tf.float32, [None], name='done')\n",
    "\n",
    "            self.max_Qs1 = tf.placeholder(tf.float32, [None], name='max_Qs1')\n",
    "            self.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "            \n",
    "class Network(object):\n",
    "    def __init__(self, inputs, n_actions, n_states, y, buffer_length, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            ops = dict(\n",
    "                trainable=True, \n",
    "                kernel_initializer=tf.random_uniform_initializer(minval=0.0, maxval=0.01),\n",
    "                use_bias=False,\n",
    "                bias_initializer=None\n",
    "            )\n",
    "\n",
    "\n",
    "            net = tf.layers.dense(inputs.s, 32, activation=tf.nn.relu, name='relu_layer', **ops)\n",
    "            self.Qs = tf.layers.dense(net, n_actions, name='linear_layer', **ops)\n",
    "\n",
    "            self.Qsa = select_columns(self.Qs, inputs.a)\n",
    "\n",
    "            self.max_Qs = tf.reduce_max(self.Qs, 1)\n",
    "\n",
    "            self.target = soft_if(inputs.done, inputs.r,  inputs.r + y * inputs.max_Qs1)\n",
    "\n",
    "            self.error = self.target - self.Qsa\n",
    "            self.loss = Pipe(self.error, tf.nn.l2_loss, tf.reduce_mean)\n",
    "\n",
    "            self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope)\n",
    "\n",
    "            self.update = tf.train.AdamOptimizer(inputs.learning_rate).minimize(self.loss)\n",
    "\n",
    "\n",
    "            self.episode_length_summary = tf.summary.scalar('episode_length', inputs.episode_length)\n",
    "\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar('loss', self.loss),\n",
    "                tf.summary.scalar('avg_target', tf.reduce_mean(self.target)),\n",
    "                tf.summary.scalar('variables_sum', sum([ tf.reduce_sum(v) for v in self.variables ])),\n",
    "                tf.summary.histogram(\n",
    "                    'avg_action', Pipe(\n",
    "                    inputs.a,\n",
    "                    Then(tf.one_hot, 2),\n",
    "                    Then(tf.reduce_mean, axis=0)\n",
    "                ))\n",
    "            ]+[\n",
    "                tf.summary.histogram('var{}'.format(i), self.variables[i]) for i in range(len(self.variables))\n",
    "            ])\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "class DQN(ModelBase):\n",
    "    def define_model(self, n_actions, n_states, y=0.98, buffer_length=500000):\n",
    "        \n",
    "        self.replay_buffer = ExperienceReplay(max_length=buffer_length)\n",
    "        self.global_max = 0.0\n",
    "        \n",
    "        with self.graph.as_default(), tf.device(\"cpu:0\"):\n",
    "            self.inputs = Inputs(n_actions, n_states, y, buffer_length, \"inputs\")\n",
    "            self.network = Network(self.inputs, n_actions, n_states, y, buffer_length, \"network\")\n",
    "            self.target_network = Network(self.inputs, n_actions, n_states, y, buffer_length, \"target_network\")\n",
    "            \n",
    "            self.update = self.network.update\n",
    "            self.update_target = tf.group(*[\n",
    "                tf.assign(t, a) for t, a in zip(self.target_network.variables, self.network.variables)\n",
    "            ])\n",
    "            \n",
    "            self.summaries = tf.summary.merge([self.network.summaries, self.target_network.summaries])\n",
    "            \n",
    "        \n",
    "            \n",
    "                              \n",
    "    def fit_feed(self, S, A, R, Max_Qs1, Done, learning_rate):\n",
    "        return {\n",
    "            self.inputs.s: S, self.inputs.a: A, self.inputs.r: R, \n",
    "            self.inputs.max_Qs1: Max_Qs1, self.inputs.done: Done, \n",
    "            self.inputs.learning_rate: learning_rate\n",
    "        }\n",
    "                \n",
    "    def choose_action(self, state, e=0.1):\n",
    "        actions = self.sess.run(self.network.Qs, feed_dict={self.inputs.s: [state]})[0]\n",
    "        n = len(actions)\n",
    "        \n",
    "        if random.random() < e:\n",
    "            return random.randint(0, n-1)\n",
    "        else:\n",
    "            return np.argmax(actions)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, env, k=100., learning_rate=0.05, print_step=10, episodes=100000, max_episode_length=float('inf'), discount=0.9, batch_size=32):\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            done = False\n",
    "            ep_num = 0\n",
    "            s = env.reset()\n",
    "            episode_length = 0\n",
    "            \n",
    "            \n",
    "            if episode % 20 == 0:\n",
    "                self.sess.run(self.update_target)\n",
    "            \n",
    "            while not done and ep_num <= max_episode_length:\n",
    "                self.global_step += 1\n",
    "                episode_length += 1\n",
    "                \n",
    "                learning_rate = max(0.0001, 1. / (1. + (self.global_step / k)))\n",
    "                e = max(0.01, 1. / (1. + (self.global_step / k)))\n",
    "                \n",
    "                a = self.choose_action(s, e)\n",
    "                s1, r, done, info = env.step(a)\n",
    "                \n",
    "                if done:\n",
    "                    r = -100.0\n",
    "                \n",
    "                self.replay_buffer.append((s, a, r, s1, float(done)))\n",
    "                \n",
    "                S, A, R, S1, Done = self.replay_buffer.random_batch(batch_size).unzip()\n",
    "                MaxQs1 = self.sess.run(self.target_network.max_Qs, feed_dict={self.inputs.s: S1})\n",
    "                \n",
    "                feed_dict = self.fit_feed(S, A, R, MaxQs1, Done, learning_rate)\n",
    "                _, summaries = self.sess.run([self.update, self.summaries], feed_dict=feed_dict)\n",
    "                self.writer.add_summary(summaries)\n",
    "                \n",
    "                s = s1\n",
    "                \n",
    "            \n",
    "            \n",
    "            episode_length_summary = self.sess.run(self.network.episode_length_summary, feed_dict={self.inputs.episode_length: episode_length})\n",
    "            self.writer.add_summary(episode_length_summary)\n",
    "            \n",
    "            if episode_length > self.global_max:\n",
    "                print(\"[MAX] Episode: {}, Length: {}, e: {}, learning_rate: {}, buffer_len: {}\".format(episode, episode_length, e, learning_rate, len(self.replay_buffer)))\n",
    "                self.save(model_path = self.model_path + \".max\")\n",
    "                self.save(model_path = self.logs_path + \"/Q-network-full.max\")\n",
    "                self.global_max = episode_length\n",
    "            \n",
    "            \n",
    "            if episode % print_step == 0:\n",
    "                print(\"[NOR] Episode: {}, Length: {}, e: {}, learning_rate: {}, buffer_len: {}\".format(episode, episode_length, e, learning_rate, len(self.replay_buffer)))\n",
    "                self.save()\n",
    "                self.save(model_path = self.logs_path + \"/Q-network-full.model\")\n",
    "\n",
    "run = get_run()\n",
    "env_logs = '/tmp/cartpole-{}'.format(run)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = Monitor(env, env_logs)\n",
    "env = ExpandedStateEnv(env)\n",
    "                \n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.shape[0] * 3\n",
    "model_path = os.getcwd() + \"/Q-network-full.model\"\n",
    "logs_path = \"logs/run{}\".format(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 32,\n",
      " s: Tensor(\"inputs/s:0\", shape=(?, 12), dtype=float32, device=/device:CPU:0),\n",
      " a: Tensor(\"inputs/a:0\", shape=(?,), dtype=int32, device=/device:CPU:0),\n",
      " r: Tensor(\"inputs/r:0\", shape=(?,), dtype=float32, device=/device:CPU:0),\n",
      " Qs: Tensor(\"network/linear_layer/MatMul:0\", shape=(?, 2), dtype=float32, device=/device:CPU:0),\n",
      " update: name: \"network/Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^network/Adam/update_network/relu_layer/kernel/ApplyAdam\"\n",
      "input: \"^network/Adam/update_network/linear_layer/kernel/ApplyAdam\"\n",
      "input: \"^network/Adam/Assign\"\n",
      "input: \"^network/Adam/Assign_1\"\n",
      "device: \"/device:CPU:0\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = DQN(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path,\n",
    "    logs_path = logs_path,\n",
    "    flush_secs = 3.0,\n",
    "    y = 0.9999,\n",
    "    buffer_length=500000\n",
    ")\n",
    "\n",
    "print(\"run: {},\\n s: {},\\n a: {},\\n r: {},\\n Qs: {},\\n update: {}\".format(\n",
    "    run, model.inputs.s, model.inputs.a, model.inputs.r, model.network.Qs, model.update\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOR] Episode: 0, Length: 43, e: 0.137023842149, learning_rate: 0.137023842149, buffer_len: 6299\n",
      "[MAX] Episode: 4, Length: 172, e: 0.129382843835, learning_rate: 0.129382843835, buffer_len: 6730\n",
      "[NOR] Episode: 10, Length: 37, e: 0.126008064516, learning_rate: 0.126008064516, buffer_len: 6937\n",
      "[NOR] Episode: 20, Length: 10, e: 0.117453605826, learning_rate: 0.117453605826, buffer_len: 7515\n",
      "[NOR] Episode: 30, Length: 13, e: 0.112511251125, learning_rate: 0.112511251125, buffer_len: 7889\n",
      "[NOR] Episode: 40, Length: 161, e: 0.104460461715, learning_rate: 0.104460461715, buffer_len: 8574\n",
      "[MAX] Episode: 41, Length: 179, e: 0.102543068089, learning_rate: 0.102543068089, buffer_len: 8753\n",
      "[MAX] Episode: 45, Length: 194, e: 0.100050025013, learning_rate: 0.100050025013, buffer_len: 8996\n",
      "[NOR] Episode: 50, Length: 167, e: 0.0946700747894, learning_rate: 0.0946700747894, buffer_len: 9564\n",
      "[MAX] Episode: 55, Length: 213, e: 0.09055510278, learning_rate: 0.09055510278, buffer_len: 10044\n",
      "[MAX] Episode: 58, Length: 218, e: 0.0882534639485, learning_rate: 0.0882534639485, buffer_len: 10332\n",
      "[NOR] Episode: 60, Length: 10, e: 0.088097964937, learning_rate: 0.088097964937, buffer_len: 10352\n",
      "[NOR] Episode: 70, Length: 21, e: 0.0825832025766, learning_rate: 0.0825832025766, buffer_len: 11110\n",
      "[NOR] Episode: 80, Length: 100, e: 0.0756944970101, learning_rate: 0.0756944970101, buffer_len: 12212\n",
      "[NOR] Episode: 90, Length: 139, e: 0.07101264025, learning_rate: 0.07101264025, buffer_len: 13083\n",
      "[NOR] Episode: 100, Length: 27, e: 0.0662558802094, learning_rate: 0.0662558802094, buffer_len: 14094\n",
      "[NOR] Episode: 110, Length: 60, e: 0.0625547353935, learning_rate: 0.0625547353935, buffer_len: 14987\n",
      "[NOR] Episode: 120, Length: 146, e: 0.0595876534382, learning_rate: 0.0595876534382, buffer_len: 15783\n",
      "[NOR] Episode: 130, Length: 47, e: 0.0570613409415, learning_rate: 0.0570613409415, buffer_len: 16526\n",
      "[NOR] Episode: 140, Length: 137, e: 0.0533447135389, learning_rate: 0.0533447135389, buffer_len: 17747\n",
      "[NOR] Episode: 150, Length: 27, e: 0.0519588485919, learning_rate: 0.0519588485919, buffer_len: 18247\n",
      "[NOR] Episode: 160, Length: 19, e: 0.0509139045873, learning_rate: 0.0509139045873, buffer_len: 18642\n",
      "[MAX] Episode: 162, Length: 330, e: 0.0496499677275, learning_rate: 0.0496499677275, buffer_len: 19142\n",
      "[NOR] Episode: 170, Length: 123, e: 0.0473417601666, learning_rate: 0.0473417601666, buffer_len: 20124\n",
      "[NOR] Episode: 180, Length: 105, e: 0.044167660439, learning_rate: 0.044167660439, buffer_len: 21642\n",
      "[NOR] Episode: 190, Length: 91, e: 0.0432675666321, learning_rate: 0.0432675666321, buffer_len: 22113\n",
      "[NOR] Episode: 200, Length: 68, e: 0.04167882299, learning_rate: 0.04167882299, buffer_len: 22994\n",
      "[NOR] Episode: 210, Length: 16, e: 0.0414456233422, learning_rate: 0.0414456233422, buffer_len: 23129\n",
      "[NOR] Episode: 220, Length: 12, e: 0.0410525883657, learning_rate: 0.0410525883657, buffer_len: 23360\n",
      "[NOR] Episode: 230, Length: 116, e: 0.0402998307407, learning_rate: 0.0402998307407, buffer_len: 23815\n",
      "[NOR] Episode: 240, Length: 217, e: 0.03936233025, learning_rate: 0.03936233025, buffer_len: 24406\n",
      "[NOR] Episode: 250, Length: 10, e: 0.0387942739652, learning_rate: 0.0387942739652, buffer_len: 24778\n",
      "[NOR] Episode: 260, Length: 60, e: 0.037065866044, learning_rate: 0.037065866044, buffer_len: 25980\n",
      "[NOR] Episode: 270, Length: 46, e: 0.0362621024767, learning_rate: 0.0362621024767, buffer_len: 26578\n",
      "[NOR] Episode: 280, Length: 79, e: 0.0355656720134, learning_rate: 0.0355656720134, buffer_len: 27118\n",
      "[NOR] Episode: 290, Length: 101, e: 0.0347680967944, learning_rate: 0.0347680967944, buffer_len: 27763\n",
      "[NOR] Episode: 300, Length: 85, e: 0.0341320226637, learning_rate: 0.0341320226637, buffer_len: 28299\n",
      "[NOR] Episode: 310, Length: 134, e: 0.0329608754409, learning_rate: 0.0329608754409, buffer_len: 29340\n",
      "[NOR] Episode: 320, Length: 137, e: 0.0315646602064, learning_rate: 0.0315646602064, buffer_len: 30682\n",
      "[MAX] Episode: 323, Length: 390, e: 0.031001984127, learning_rate: 0.031001984127, buffer_len: 31257\n",
      "[NOR] Episode: 330, Length: 327, e: 0.0295884250081, learning_rate: 0.0295884250081, buffer_len: 32798\n",
      "[NOR] Episode: 340, Length: 122, e: 0.0279306203391, learning_rate: 0.0279306203391, buffer_len: 34804\n",
      "[NOR] Episode: 350, Length: 124, e: 0.0268103702512, learning_rate: 0.0268103702512, buffer_len: 36300\n",
      "[NOR] Episode: 360, Length: 62, e: 0.0259403372244, learning_rate: 0.0259403372244, buffer_len: 37551\n",
      "[NOR] Episode: 370, Length: 136, e: 0.0251205787781, learning_rate: 0.0251205787781, buffer_len: 38809\n",
      "[NOR] Episode: 380, Length: 352, e: 0.0240459759059, learning_rate: 0.0240459759059, buffer_len: 40588\n",
      "[NOR] Episode: 390, Length: 248, e: 0.0229415678267, learning_rate: 0.0229415678267, buffer_len: 42590\n",
      "[NOR] Episode: 400, Length: 370, e: 0.0217046860417, learning_rate: 0.0217046860417, buffer_len: 45074\n",
      "[MAX] Episode: 410, Length: 396, e: 0.0202216290544, learning_rate: 0.0202216290544, buffer_len: 48453\n",
      "[NOR] Episode: 410, Length: 396, e: 0.0202216290544, learning_rate: 0.0202216290544, buffer_len: 48453\n",
      "[NOR] Episode: 420, Length: 329, e: 0.0191076717302, learning_rate: 0.0191076717302, buffer_len: 51336\n",
      "[MAX] Episode: 428, Length: 499, e: 0.0184959124034, learning_rate: 0.0184959124034, buffer_len: 53067\n",
      "[NOR] Episode: 430, Length: 198, e: 0.0183637866128, learning_rate: 0.0183637866128, buffer_len: 53456\n",
      "[NOR] Episode: 440, Length: 135, e: 0.0174489617868, learning_rate: 0.0174489617868, buffer_len: 56311\n",
      "[MAX] Episode: 447, Length: 520, e: 0.0168375679817, learning_rate: 0.0168375679817, buffer_len: 58392\n",
      "[NOR] Episode: 450, Length: 305, e: 0.016485328058, learning_rate: 0.016485328058, buffer_len: 59661\n",
      "[MAX] Episode: 458, Length: 556, e: 0.0158017824411, learning_rate: 0.0158017824411, buffer_len: 62285\n",
      "[MAX] Episode: 459, Length: 635, e: 0.0156448004506, learning_rate: 0.0156448004506, buffer_len: 62920\n",
      "[NOR] Episode: 460, Length: 460, e: 0.0155330154243, learning_rate: 0.0155330154243, buffer_len: 63380\n",
      "[NOR] Episode: 470, Length: 322, e: 0.0147891802357, learning_rate: 0.0147891802357, buffer_len: 66618\n",
      "[NOR] Episode: 480, Length: 612, e: 0.0140439575872, learning_rate: 0.0140439575872, buffer_len: 70206\n",
      "[MAX] Episode: 482, Length: 665, e: 0.0137999558401, learning_rate: 0.0137999558401, buffer_len: 71465\n",
      "[MAX] Episode: 483, Length: 710, e: 0.0136660562495, learning_rate: 0.0136660562495, buffer_len: 72175\n",
      "[MAX] Episode: 484, Length: 889, e: 0.0135020185518, learning_rate: 0.0135020185518, buffer_len: 73064\n",
      "[MAX] Episode: 487, Length: 917, e: 0.0131185391195, learning_rate: 0.0131185391195, buffer_len: 75229\n",
      "[NOR] Episode: 490, Length: 782, e: 0.0127606359901, learning_rate: 0.0127606359901, buffer_len: 77367\n",
      "[NOR] Episode: 500, Length: 379, e: 0.0117598635856, learning_rate: 0.0117598635856, buffer_len: 84036\n",
      "[NOR] Episode: 510, Length: 100, e: 0.0113792828776, learning_rate: 0.0113792828776, buffer_len: 86880\n",
      "[NOR] Episode: 520, Length: 225, e: 0.0110768958107, learning_rate: 0.0110768958107, buffer_len: 89279\n",
      "[NOR] Episode: 530, Length: 262, e: 0.0105547580849, learning_rate: 0.0105547580849, buffer_len: 93745\n",
      "[NOR] Episode: 540, Length: 388, e: 0.0101790494804, learning_rate: 0.0101790494804, buffer_len: 97242\n",
      "[NOR] Episode: 550, Length: 281, e: 0.01, learning_rate: 0.00991581473292, buffer_len: 99850\n",
      "[NOR] Episode: 560, Length: 215, e: 0.01, learning_rate: 0.0096425506475, buffer_len: 102708\n",
      "[NOR] Episode: 570, Length: 247, e: 0.01, learning_rate: 0.00941486607353, buffer_len: 105216\n",
      "[NOR] Episode: 580, Length: 256, e: 0.01, learning_rate: 0.00918315808807, buffer_len: 107896\n",
      "[NOR] Episode: 590, Length: 207, e: 0.01, learning_rate: 0.0089475850468, buffer_len: 110763\n",
      "[NOR] Episode: 600, Length: 178, e: 0.01, learning_rate: 0.00873026959072, buffer_len: 113545\n",
      "[NOR] Episode: 610, Length: 275, e: 0.01, learning_rate: 0.00853402515831, buffer_len: 116179\n",
      "[NOR] Episode: 620, Length: 670, e: 0.01, learning_rate: 0.0083176683912, buffer_len: 119227\n",
      "[MAX] Episode: 625, Length: 991, e: 0.01, learning_rate: 0.00811082633099, buffer_len: 122293\n",
      "[NOR] Episode: 630, Length: 506, e: 0.01, learning_rate: 0.00794344268806, buffer_len: 124891\n",
      "[NOR] Episode: 640, Length: 858, e: 0.01, learning_rate: 0.00761516026105, buffer_len: 130318\n",
      "[MAX] Episode: 647, Length: 999, e: 0.01, learning_rate: 0.00741476725046, buffer_len: 133867\n",
      "[MAX] Episode: 650, Length: 1113, e: 0.01, learning_rate: 0.00729171223987, buffer_len: 136143\n",
      "[NOR] Episode: 650, Length: 1113, e: 0.01, learning_rate: 0.00729171223987, buffer_len: 136143\n",
      "[MAX] Episode: 656, Length: 1413, e: 0.01, learning_rate: 0.00709134359687, buffer_len: 140018\n",
      "[NOR] Episode: 660, Length: 245, e: 0.01, learning_rate: 0.0070038801496, buffer_len: 141779\n",
      "[NOR] Episode: 670, Length: 238, e: 0.01, learning_rate: 0.00687890377789, buffer_len: 144373\n",
      "[NOR] Episode: 680, Length: 354, e: 0.01, learning_rate: 0.00674622717245, buffer_len: 147232\n",
      "[NOR] Episode: 690, Length: 792, e: 0.01, learning_rate: 0.00649346432815, buffer_len: 153002\n",
      "[NOR] Episode: 700, Length: 508, e: 0.01, learning_rate: 0.00612343622747, buffer_len: 162308\n",
      "[NOR] Episode: 710, Length: 1031, e: 0.01, learning_rate: 0.0059195415907, buffer_len: 167933\n",
      "[NOR] Episode: 720, Length: 898, e: 0.01, learning_rate: 0.00573217008495, buffer_len: 173455\n",
      "[NOR] Episode: 730, Length: 453, e: 0.01, learning_rate: 0.00560048388181, buffer_len: 177557\n",
      "[NOR] Episode: 740, Length: 169, e: 0.01, learning_rate: 0.00550557714965, buffer_len: 180635\n",
      "[NOR] Episode: 750, Length: 224, e: 0.01, learning_rate: 0.00541037710328, buffer_len: 183831\n",
      "[NOR] Episode: 760, Length: 354, e: 0.01, learning_rate: 0.00531708432364, buffer_len: 187074\n",
      "[NOR] Episode: 770, Length: 236, e: 0.01, learning_rate: 0.00522659920869, buffer_len: 190330\n",
      "[NOR] Episode: 780, Length: 408, e: 0.01, learning_rate: 0.00513318036456, buffer_len: 193812\n",
      "[MAX] Episode: 786, Length: 1538, e: 0.01, learning_rate: 0.00501225496339, buffer_len: 198512\n",
      "[NOR] Episode: 790, Length: 515, e: 0.01, learning_rate: 0.00493924725872, buffer_len: 201461\n",
      "[NOR] Episode: 800, Length: 336, e: 0.01, learning_rate: 0.00476878543804, buffer_len: 208698\n",
      "[NOR] Episode: 810, Length: 277, e: 0.01, learning_rate: 0.00470945045423, buffer_len: 211340\n",
      "[NOR] Episode: 820, Length: 264, e: 0.01, learning_rate: 0.00465135749869, buffer_len: 213992\n",
      "[NOR] Episode: 830, Length: 310, e: 0.01, learning_rate: 0.00459514477004, buffer_len: 216622\n",
      "[NOR] Episode: 840, Length: 283, e: 0.01, learning_rate: 0.00454048065528, buffer_len: 219242\n",
      "[NOR] Episode: 850, Length: 317, e: 0.01, learning_rate: 0.00447483353619, buffer_len: 222473\n",
      "[NOR] Episode: 860, Length: 283, e: 0.01, learning_rate: 0.00440518931301, buffer_len: 226006\n",
      "[NOR] Episode: 870, Length: 379, e: 0.01, learning_rate: 0.00433673911912, buffer_len: 229589\n",
      "[NOR] Episode: 880, Length: 993, e: 0.01, learning_rate: 0.00425425106037, buffer_len: 234060\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    env, \n",
    "    episodes=50000,\n",
    "    max_episode_length = 60000,\n",
    "    k = 1000.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-02 18:00:32,674] Making new env: CartPole-v1\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <type 'exceptions.TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e1c35c4694f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/cartpole.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/window/xlib/__init__.pyc\u001b[0m in \u001b[0;36mdispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# Check for the events specific to this window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         while xlib.XCheckWindowEvent(_x_display, _window,\n\u001b[0;32m--> 853\u001b[0;31m                                      0x1ffffff, byref(e)):\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;31m# Key events are filtered by the xlib window event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# handler so they get a shot at the prefiltered event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: <type 'exceptions.TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model_run = DQN(\n",
    "    n_actions, n_states,\n",
    "    model_path = model_path + \".max\",\n",
    "    flush_secs = 3.0,\n",
    "    restore = True\n",
    ")\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = Monitor(env, env_logs)\n",
    "env = ExpandedStateEnv(env)\n",
    "\n",
    "\n",
    "s = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    a = model_run.choose_action(s, e=0)\n",
    "    s, r, done, info = env.step(a)\n",
    "    env.render()\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
